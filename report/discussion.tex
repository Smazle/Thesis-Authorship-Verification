\section{Discussion} \label{sec:discussion}

\begin{itemize}

    \item

        What could be the reason that qian got such good results on authorship
        verification using the LSTM when we got so shit results?

    \item

        Discuss whether or not we can handle group turn ins.

    \item

        Look at different filters and discuss what they might look at and
        whether or not that seems like good features to be looking at.

\end{itemize}


\subsection{Results}

Looking at the results presented in Section \ref{sec:results} it is apparent
that we have not achieved less the than 10\% accusation error on the test
set that MaCom required We believe however that some of the methods show
promising results. While all the networks we applied to the the test set beat
our baselines the best performing network was the \gls{conv-char-NN}. This
network had an accusation error closest to to the one required in addition to
having the highest overall accuracy in all the categories we tested it on.
While this network was in fact the best, it was closely followed by network
\gls{conv-char-word-NN}, while the \gls{rec-sent-NN} we also tested had sub-par
performance, lending credence to the applicability of \glspl{CNN} to \gls{NLP}
problems. It is however worth noting that this could very well be due to the
infeasibility of running any of our \glspl{RNN} on the character or word level
of the texts supplied to it. We hypothesize that the accuracy of a RNN would be
greatly increased had it been run on a lower linguistic level. As experiments
showed in \ref{subsubsec:conv_char_word_nn}, even \gls{CNN}s performed badly on the
sentence level. Ideally we would have run an RNN on the word level. We suspect
it would be able to learn the words and their relation to on another, which
makes sense with words, but not with sentences. However in order to do so, we
would need computational power than was available to train the model. Looking at
the performance of the two \glspl{CNN}, it becomes obvious that the character
level is of great importance when it comes to determining the author of a text.
The sentence level appears to on a too high linguistic level. At least in the
way we applied it. We hypothesize that applying it to a lower level, such as
the word level would greatly increase its performance, as \glspl{RNN} would
be able to learn based on the words relation to one another, and therefore
use a more grammar based learning approach. Looking at \gls{conv-char-NN},
and \gls{conv-char-word-NN}, it would seem that focusing on the word level of
the texts seems to actually hurt the model a bit. We are a bit uncertain as
to the reason for this, we do however have some theories. The reason might be
similar to the problem \gls{rec-sent-NN} had. The level might actually be a bit
to high for our convolutional networks, the image-equivalent of having a too
large convolutional window, thus ignoring all the small details that are needed
in order to properly determine the author. This also highlight a potential
reason behind the differing performance of networks \gls{conv-char-NN} and
\gls{conv-char-word-NN}. \gls{conv-char-NN} had many filters on the character
level of the text, whereas \gls{conv-char-word-NN} had more filters on the
word level. This might very well have led to \gls{conv-char-word-NN} missing
some relevant person-specific traits on the character level such as typos, dot
position, comma positions and the like. It would not even be able to find such
traits on the word level, as the usage of a pre-trained embedding layer means
that only correctly spelled word were even considered.

Compared to the results produced during validation, which can be seen in Table
\ref{tab:experi-results}, we can clearly see a difference. The results from the
validation is of course under the required accusation error threshold as that
was the dataset on which we actually found the theta and the weight function to
get under that threshold. Because of that, it stands to reason that the models
would have a somewhat lower performance when applied to a fresh untouched test
dataset. This lowered performance takes the form of an increased accusation
error, and a decreased accuracy. This generally is the case in machine learning
tasks.

A more general description of a models performance can be computed using the
\gls{AUC} of the \gls{ROC}-curve of a model, better known as \gls{AUROC}.
A \gls{ROC} curve is the \gls{FPR} plotted against the \gls{TPR}, both of
which are defined in Section \ref{sec:method}. \gls{AUROC} is a measure of
discrimination, or a measure of the amount of certainty our model has when it
makes a decisions.

The ROC curves and AUROC results for the different networks we produced, can
be seen in Figure \ref{fig:AUROC}. These figures reinforce the relationship
between the networks already expressed. \gls{conv-char-NN} still performs
best. It is however very closely followed by \gls{conv-char-word-NN}, to the
point of certain overlaps in the ROC-Curve in the 96/04 split constrained
configuration. While \gls{rec-sent-NN} does have the worst AUROC score, it is
higher than we anticipated when looking back at the results presented in Section
\ref{sec:results}. This might be attributed to the model being working good in
general but not adapting very well to thresholding. At least not as well as the
other two networks.

\begin{figure}
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \textbf{Constrained ROC-Curve and AUROC for each network}
        \includegraphics[width=1\linewidth]{./pictures/discussion/AUROC_Constrained}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \textbf{Unconstrained ROC-Curve and AUROC for each network}
        \includegraphics[width=1\linewidth]{./pictures/discussion/AUROC_Unconstrained}
    \end{minipage}
    \caption{The ROC-Curve for the three networks produced, plotted for both of
        the possible data sample splits. The curves shown based on the weight
        functions that produced the best possible values produced with that
        weight on the validation set \gls{C}.}
    \label{fig:AUROC}
\end{figure}

Another \gls{NLP} advantage \glspl{CNN} has in this context, is the fact that we
are able to more accurately determine what part of the text contributed to final
classification as can be seen in the following section.


\subsection{Teacher Feedback}

As mentioned we wanted to look at what kind of feedback we could give to
teachers in conjunction with the bare predictions. As explained earlier the
system is not meant to be the final judge of which students are cheating but are
rather meant as a support system for teachers that are already suspicious. We
have focused on the \gls{conv-char-NN} network since it performed best on the
test dataset.

\subsection{Extracting Important Features}

We have previously looked at the output of the feature extraction layer to
obtain information on what a specific network were looking at. We wanted to do
something similar for teacher feedback. Recall that \gls{conv-char-NN} started
with a convolutional layer followed by a max pool layer. We therefore know
that the larger the output of the convolutional layer the more important that
particular character sequence is. The output of the feature extraction can be
thought of as in Figure \ref{fig:feature_extraction_output_example}. Each filter
gives a single output that is the maximum output of any filter position. The
combining function for the third network was the absolute difference. That means
that when we are comparing texts $t$ and $t'$ the output of the combination
will be high for a particular filter iff the maximum output of that filter is
significantly different for $t$ and $t'$.

\begin{figure}
    \centering
    \textbf{Teacher Feedback Example}\par\medskip
    \includegraphics[width=\textwidth]{./pictures/discussion/teacher_feedback_example}
    \caption{Illustrates our feedback to teachers. The
        particular network used in this example only has three filters. The
        three filters maximum activations are shown in three different colors
        for the two texts they are comparing. The first filter looks for
        negative qualifiers. Therefore it reacts strongly to both the Danish
        word "ikke" (not) and the Danish word "ngen" (noone). The second filter
        looks for city names so it reacts strongly to the string "Rom " (Rome)
        but less strongly to "Der " (not a city name) even though it looks like
        a city name. The third filter reacts to phrases that contains the word
        "p\aa " (on) and therefore reacts about the same to both texts.}
    \label{fig:feature_extraction_output_example}
\end{figure}

It is hard to know exactly what the following layers does with the absolute
difference. But we feel that it is a fair assumption that the largest filter
differences translates to the most important differences. The feedback system
we implemented for teachers takes an author $\alpha$, text $t$ and $n \in
\mathbb{N}^+$ and outputs the $n$ largest differences between each $t' \in
T_\alpha$ and $t$. The idea is that the when our system reports a negative the
teacher can ask for feedback from the system. The teacher will then get a list
of the $n$ greatest differences between each of the texts and can use that
information to argue against the student.

As an example we ran our system on a random author and a text that
that author did not write. The whole output can be seen in Appendix
\ref{subsec:teacher_feedback_text_comparisons}. We have shown a truncated
output in Table \ref{tab:teacher_feedback_output}. The output contains only
the comparison between one of the candidate authors texts and the unknown
text. Text 1 is from the candidate author and Text 2 is the unknown text. We
have illustrated what each of the filters is generally looking at by the three
substrings from the \gls{C} dataset that generated the highest values.

\begin{table}
    \begin{tabular}{lll|lll}
        \textbf{Max 1}    & \textbf{Max 2}    & \textbf{Max 3}       &
        \textbf{Text 1}   & \textbf{Text 2}   & \textbf{Difference}  \\
        \hline
        \verb[nemlig 1[   & \verb[nemlig 1[   & \verb[nemlig –[      &
        \verb'pere.\n\nH' & \verb'nemlig b'   & |3.06 - 4.70| = 1.64 \\

        \verb[, F.eks.[   & \verb[, F.eks.[   & \verb[, F.eks.[      &
        \verb'. F.eks.'   & \verb'del. Und'   & |4.73 - 3.18| = 1.55 \\

        \verb[ke …''. [   & \verb[a...''\n\n[ & \verb[n''. '' [      &
        \verb'v. Men d'   & \verb'n. Den v'   & |4.28 - 2.91| = 1.37 \\

        \verb[forsøger[   & \verb[forsøger[   & \verb[forsøger[      &
        \verb'for sætt'   & \verb'forsøgte'   & |3.40 - 4.77| = 1.37 \\

        \verb[, Hvorda[   & \verb[, Hvorda[   & \verb[,08 – 6,[      &
        \verb', som ti'   & \verb', Hvorda'   & |3.70 - 5.07| = 1.37 \\

        \verb[der; ’’M[   & \verb[der; ”Ha[   & \verb[der; ”Ma[      &
        \verb'dem; Nia'   & \verb'der omha'   & |4.64 - 3.28| = 1.36 \\

        \verb[. Her ef[   & \verb[. Her ef[   & \verb[' Her br[      &
        \verb'. Jeg vi'   & \verb'. Her fo'   & |2.61 - 3.92| = 1.31 \\

        \verb[r dog kr[   & \verb[r dog kr[   & \verb[r dog ’d[      &
        \verb'r og lud'   & \verb'r dog i '   & |2.83 - 4.13| = 1.30 \\

        \verb[11], da [   & \verb[:1], da [   & \verb[:1], da [      &
        \verb'ys”, der'   & \verb'for, da '   & |3.78 - 5.04| = 1.26 \\

        \verb[, så Car[   & \verb[, så Car[   & \verb[, så Car[      &
        \verb', så er '   & \verb', som En'   & |5.19 - 3.94| = 1.25 \\
        \hline
        \verb[; ’S[       & \verb[; ’S[       & \verb[; ’E[          &
        \verb'; Ni'       & \verb'r He'       & |3.12 - 1.78| = 1.34 \\

        \verb[; ”t[       & \verb[; ”t[       & \verb[; ”t[          &
        \verb'; ”H'       & \verb', ”j'       & |3.44 - 2.13| = 1.31 \\

        \verb[d.’ [       & \verb[d.’ [       & \verb[d.’ [          &
        \verb'ne-V'       & \verb'20’e'       & |1.75 - 2.77| = 1.02 \\

        \verb[1\n’’[      & \verb[1]’’[       & \verb[1]’’[          &
        \verb' l2-'       & \verb'720’'       & |1.75 - 2.71| = 0.96 \\

        \verb['Det[       & \verb['Det[       & \verb['Det[          &
        \verb'ndet'       & \verb' Det'       & |2.37 - 3.25| = 0.88 \\

        \verb[f 1[        &, \verb[f 1[       &, \verb[f 1[          &
        \verb'f 2\n'      & \verb'v og'       & |3.05 - 2.20| = 0.85 \\

        \verb[æk''[       & \verb[’’ é[       & \verb[ud;'[          &
        \verb'lv; '       & \verb',tro'       & |2.60 - 1.77| = 0.83 \\

        \verb[\n\nx\n[    & \verb[\n\nx\n[    & \verb[\n\nx\n[       &
        \verb'\n\n\n\n'   & \verb'\n\n5\n'    & |1.81 - 2.61| = 0.80 \\
        \verb[ “… [       & \verb[ “… [       & \verb[?“! [          &
        \verb'nd” '       & \verb'r,” '       & |1.75 - 2.53| = 0.78 \\

        \verb[S\n, [      & \verb[S\n, [      & \verb[O\n, [         &
        \verb'e\n, '      & \verb'ad, '       & |2.62 - 1.92| = 0.70 \\
    \end{tabular}
    \caption{Shows the 10 most different activations of convolutional filters on
        two different texts. Both the 10 most different activations for the
        filters of size 8 and size 4 are shown. What each filter is looking at is
        represented by the three greatest activations on any text in the \gls{C}
        dataset. The strings the filter reacted most strongly to in the texts we
        are comparing is shown in column \textbf{Text 1} and column \textbf{Text
        2}. The actual activation values are shown in column \textbf{Difference}
        where the left number corresponds to text 1 and the right to text 2. The
        filters are sorted such that the greatest difference activation is at
        the top and the differences fall as we move down the list.}
    \label{tab:teacher_feedback_output}
\end{table}

Some of the filters are very clear about what they are looking at while others
require some parsing. As an example of one of the easier filters consider
the filter activating to the string "F. eks". That phrase is Danish for "for
example" or "for instance" and some people write that as "for eksempel" while
others use the shorthand above. This could indicate that you are not the author
of an assignment if that suddenly changes. That cannot be the only reasson
given but if thousands of those reasons are present you can say with some
confidence that someone else has written the assignment. As an exampe of one of
the harder to parse filters consider the filter looking at \verb["ke …''."[,
\verb["a...''\n\n"[ and \verb["n''. '' "[. That filter seems to react to non
alphanumeric characters but none of the reactions on the two texts we are
comparing looks anything like what gives the highest numbers. Instead the filter
seem to also pick up on the start of sentences.


\subsubsection{Locating Ghost Written Areas}

We also looked at whether or not we could say something about which parts of an
assignment are the most likely to be ghost written. That would allow a teacher
that is suspicious of a student to identify the part of the assignment that
were least likely to be written by him and look closely at that. To find the
parts of a text $t$ most likely to be ghost written we split $t$ into a list of
paragraphs. Each paragraph is separated by at least 2 newline characters. We
throw away paragraphs of less than 100 characters since it does not make sense
to predict on so little text. After that we use one of our networks to predict
each individual paragraph against all of an authors other texts. We take the
average of the predictions and can then report which paragraphs in the file is
least similar to the authors other texts.

To see how well that system worked we looped through each $\alpha \in
\mathcal{T}$ where $\mathcal{T}$ is the \gls{C} dataset. For each $\alpha$ we
chose a random text $t$ with more than 5 paragraphs. We then split that text up
into paragraphs and removed all paragraphs except the first 5. We then chose
a text $t' \not\in T_\alpha$ and chose a random paragraph from that text. We
then used \gls{conv-char-NN} to predict the 5 paragraphs from $t$ and single
paragraph from $t'$ against each $t'' \in T_\alpha \setminus \{t\}$. We computed
the average prediction on each of the 5 first paragraphs in $t$ and the average
prediction on the paragraph from $t'$. We expect that the average prediction on
paragraphs from $t$ is higher than the paragraph from $t'$.

The predictions on the 5 paragraphs from $t$ was 0.42216, 0.43975, 0.44447,
0.44543 and 0.44425 while the prediction on the paragraph from $t'$ was 0.39524.
As expected we see that the paragraph from text $t'$ has a lower average score
than the 5 paragraphs from $t$. The difference is not as large as we would
have expected however. It seems as if the difference is small enough that
we cannot rely on the results when used on a single assignment. It is also
interesting that no predictions have above 50\% chance of being written by
$\alpha$. It seems like our network does not work well on texts as short as
a single paragraph. That makes sense since the network works by extracting
information from 1200 global max pools and comparing the output from two texts.
Short texts will not be able to achieve an activation of a enough of the filters
for it to be similar to a longer text. The character limit we chose in this
experiment was 100 characters while it was 400 when we trained the networks
which might also be part of the reason.

It is also interesting that the first two paragraphs from $t$ received a lower
average score than the last 3. It seems like the network is better at predicting
the middle of a text than the start of a text.


\subsubsection{Applying Other Machine Learning Methods}

We have also considered using other machine learning methods to give feedback
to teachers. Simpler machine learning models has the advantage that it is much
easier to interpret their results compared to neural networks. Consider for
example logistic regression as described by \citet{Abu-Mostafa:2012:LD:2207825}.
The input to the logistic regression is a vector of features and the output is a
probability,

\begin{equation}
    h(\mathbf{x}) = \theta(\mathbf{w}^Tx)
\end{equation}

where $\mathbf{w}$ is the weights of the model and $\theta$ is the sigmoid
function. Here the importance of each individual feature is easily
found as it can be read directly from the weights vector. The downside
to logistic regression models is that they are "only" linear models
\citet{Abu-Mostafa:2012:LD:2207825}. They are therefore not able to model
arbitrary functions. But we could still apply logistic regression to the output
of the feature extraction layer. If we did that we could get an idea of which
features are the most important extracted by the network. We could then use
that information to report to teachers which of the filters they should be most
attentive of.

In a previous project \citep{US} we used the random forest method in an
authorship verification task. The random forest has a build in notion of feature
importance as it relates to the number of times trees split on a particular
feature. Random Forests are not normally applied to raw data as we do with our
networks but to a set of features like logistic regression or our baseline
methods. Again it is much easier to get information from a random forest about
which features are the most important and furthermore the forest is able to
model non-linear data. We could therefore also try applying a random forest to
the features extracted by the neural network. Alternately the random forest
model could be applied using a separate set of features. A \gls{NN} would
perform the prediction, and in the negative cases a random forest model would
be trained with the purpose of pointing towards the cause. From that we would
be able to conclude which features are the most important for this authorship
verification task.

\subsection{Applicability of Method}\label{sec:app_of_method}
% TODO: Discuss scaleability.

In this section we will discuss how applicable our approach will be to real
world situations. We have created our solutions in a lab setting which
simplifies the problem quite a bit. We discuss here how our results can be
applied to find real ghost writers.


\subsubsection{Data Deficiencies}

The biggest problem with our approach is that we had no actual ghost written
assignments available. We were given a dataset of authors where each author
had written a set of assignments. To create ghost written assignments we used
assignments turned in by other authors. It would of course have been better if
we had had a dataset of authors where each author had a set of assignments known
to be written by him/her and a set of assignments known to have been written by
a ghost writer hired by that author. That was not possible however since MaCom
did not have that data. In a real ghost writer setting the ghost writer might
try to mimic the writing style of a student and in that way might trick our
algorithm into classifying a \gls{FP}.

We have discussed another problem with our dataset during the experiments we
performed. We observed multiple times that the network were reacting strongly
to strings looking like Danish names and school class identifiers. On our
artificially constructed dataset all of an authors texts will have the correct
name and most generated ghost written texts will have a different name.
Therefore names are an excellent feature to look at due to the way our dataset
was constructed. In a real ghost writer setting however the names attached to an
assignment is a useless feature as a ghost writer will always put the students
name on the assignment. We have tried to work against these deficiencies of
our dataset by removing as many names as possible and as much other metadata
information as possible.

We know that we removed a significant amount of information from the texts since
we observed the training and validation accuracy falling after the change. We
also saw less of the filters looking at text metadata suggesting that it is not
as important to the networks.

It is hard to predict exactly how well the network will perform on ghost written
assignments when we had no such assignments available during testing. However we
believe we have handled the deficiencies in the dataset as well as possible.


\subsubsection{Applicability to non Danish Class Texts}

Another thing to be discussed is our methods applicability to other school
subjects. The assignments provided to us for the development of this paper was
texts from the Danish course. Therefore the models almost certainly fitted
features that are specific to texts from that course. Thus a certain bias
towards the specific course is to be expected. It would stand to reason, that
assignments made in the Danish course are more focused on the higher linguistic
levels such as \gls{POS}-tags and words. Meanwhile courses such as Maths would
focus more on the individual characters used such as "=", "?" and numbers. One
could take this even further and apply similar methods to code, which suddenly
introduces a lot of formatting specific quirks. Thus the applicability of our
current methods has a very large bias towards the texts from the Danish course
and maybe even the specific skill level of a secondary school student. Students
at that age might very well have a general tendency towards some specific quirks
that students on a higher level does not. That could result in specific network
design decision working better on one group than another. In order to verify how
generalizing our methods are further experimentation into these areas would be
needed.

We believe however that if we trained the same networks on data from other
courses we could obtain a model that had comparable performance to the models
trained in this thesis. The model would probably find other n-grams to extract
from the texts but the same architecture would probably still give about the
same results.


\subsubsection{Intentional Cheating of System}

Assuming that a student had obtained information about the inner working of
the system would he/she be able to cheat the system? Cheating would be if the
student did not write an assignment herself but still had the text classified
as written by her. We have in a previous section described how we can figure
out what the network convolutional networks is probably looking at by looking
at the output of the filter extraction layer. If a student could obtain that
information she could make sure that any ghost written assignments contained
the same phrases that the network reacted to each time. It would however be
very hard to make sure that all filters received the same values in a ghost
written assignment. The student would have to go through the ghost written
assignment and making sure that each filter is represented by approximately the
same phrase as he/she usually use in an assignment. And even if the student
obtains the correct values for most filters the few filters that obtained a
different value might be very important in the dense layers that followed the
feature extraction layers. It is very hard to predict what the dense layers are
doing with the features it is given so it is in our opinion impractical to cheat
the convolutional networks by engineering the features in a ghost written
assignment.

The problem is even harder in our \gls{RNN} networks. Here the student has even
less information available since it is not possible to tie the reaction of the
\gls{RNN} to particular places in a text. Again we do not believe it is possible
to cheat the networks.

What the student could do is exploit our prediction system. Some of the
prediction systems we have discussed is especially easy to cheat. Lets for
example consider the $P_{max}$ prediction system. In this prediction system a
text would have to be similar to only a single other text to be considered not
ghost written. A student could therefore seed his ``library'' of texts by buying
a ghost written assignment for a not very important assignment. That would
probably not result in a problem since teachers will not be as on guard when
it is only a small assignment. Then when they turn in their \gls{SRP} they can
use the same ghost writer with no problem since they have seeded one of his/her
texts into their library.


% TODO: It would be nice to have some data here on how long it takes to predict
% a text against an author with n texts here for each of our networks. We could
% then discuss system load in periods where n students turn in assignments at
% the same time.
%
% I also want a discussion of other scalability issues here. I tried rewriting
% the section but could not come up with a good introduction. So I'm saving it
% for later.
\subsubsection{Scalability}

The runtime our methods is also a factor. As mentioned in the beginning of this
papers, running time was a focus during development. If we had chosen an author
specific approach, we would have to train each of our methods on the texts of
each individual author. This would introduce several problems. The methods
would have to be trained for each author individually, before performing any
kind of prediction. This would also lead to both varying quality of model, as
the quantity of samples would very. This would also mean that we would not be
able to apply them to new student, as they have no assignments associated with
them. The generalizing approach was used instead. This mean that we only have
to train the \glspl{NN} once, and then it can be applied text even written by
new student. Most of our networks reached their max potential with the first
10 epochs, meaning that max training time would be around 1 day or continuous
training. This is the most time consuming part, as the actual prediction time is
negligible. Of course the networks might need retraining some-time, to keep up
with an evolving education system.


\subsubsection{Citations}

Depending on which course the assignments we are using are associated with, the
amount of citations will differ. A course with a hefty focus on third party
sources such as history, will most likely have an increase in citation usage.
This is not something our models compensate for. Citations introduces a break in
the linguistic patterns exhibited by the student. For this reason each citation
serves as nothing but a source of noise for our models. The solution for is
however pretty simple and will be addressed in Section \ref{sec:future_work}.
It should be noted that there would be cases where citation would add to the
accuracy of the model. This would be cases where a student for some reason uses
the same sources consistently for their assignments. In this scenario we suspect
our models would perceive the contents of the citations as part of the student
vocabulary.


\subsection{Prediction Systems}

As described in Section \ref{subsec:prediction_system} we use several weight
functions to weight which of an authors texts are most important. Looking
at the graphs in Section \ref{subsubsec:prediction_system_conv-char-NN},
\ref{subsubsec:prediction_system_rec-sent-NN} and
\ref{subsubsec:prediction_system_conv-char-word-NN} we can get an idea of the
relative strengths of the weight methods. Using those graphs we want to discuss
a couple of the prediction systems.

\begin{description}

    \item[$P_\mathrm{U}$]

        The uniform prediction system $P_\mathcal{U}$ generally has a lower
        accuracy and higher accusation error than the other prediction systems.
        The only prediction system that performs worse than the uniform weight
        is $P_{min}$. We had expected that the uniform weighing would perform
        worse than the other since it does not use any metadata about the texts
        to make its predictions. It only use the raw predictions on the texts of
        the networks and simply takes an average of that.

    \item[$P_{exp_\lambda}$]

        The exponential dropoff prediction system used the time of an
        assignment to determine the most important text. Recall that
        as $\lambda \rightarrow \infty$ more weight is placed on the
        newest assignment. We assumed that an authors writing style
        would change over time and the newest text would therefore be
        a better predictor of current writing style than the oldest
        text. In Figure \ref{fig:conv_char_prediction_zoom_50} and
        \ref{fig:conv_char_prediction_zoom_04} we have shown a plot of the
        $P_{exp_\lambda}$ and $P_\mathcal{U}$ prediction system accuracies and
        accusation error with important intervals highlighted. The important
        intervals is when $\theta \approx 0$, when $\theta \approx 1$ and when
        the accuracy is maximized around $\theta \approx 0.5$. In both Figures
        we observe that the accuracy is lower and accusation error higher in
        $P_\mathcal{U}$ than all $P_{exp_\lambda}$. We can therefore conclude
        that as expected the newest text is more indicative of the writing style
        of a student than the older texts.

        \begin{figure}
            \centering
            \textbf{$P_{exp_\lambda}$ 0.5 Split}\par\medskip
            \includegraphics[width=0.7\textwidth]{./pictures/discussion/conv_char_nn_prediction_zoom_50_time}
            \caption{Illustrate important intervals for our different
                $P_{exp_\lambda}$ prediction systems for the \gls{conv-char-NN}
                network on the 0.5 split dataset. On the left we have shown the
                beginning of the curves, in the middle we have shown the top
                accuracy and to the right we have shown the end of the curves.}
            \label{fig:conv_char_prediction_zoom_50}
        \end{figure}

        \begin{figure}
            \centering
            \textbf{$P_{exp_\lambda}$ 0.04 Split}\par\medskip
            \includegraphics[width=0.7\textwidth]{./pictures/discussion/conv_char_nn_prediction_zoom_04_time}
            \caption{Illustrate important intervals for our different
                $P_{exp_\lambda}$ prediction systems for the \gls{conv-char-NN}
                network on the 0.04 split dataset. On the left we have shown the
                beginning of the curves, in the middle we have shown the top
                accuracy and to the right we have shown the end of the curves.}
            \label{fig:conv_char_prediction_zoom_04}
        \end{figure}

        It is hard to say which $\lambda$ produce the best results when looking
        at the Figures. Let us focus on the 0.5 case as that gives a clearer
        picture. For low $\theta$s we have that $\lambda = 1.0$ gives the
        highest accuracy but with highest accusation error while $\lambda =
        0.25$ gives the lowest accuracy with the lowest accusation error. We see
        a similar picture for high $\theta$s. There we get the highest accuracy
        but lowest accusation error when $\lambda = 1.0$ and the lowest accuracy
        and highest accusation error when $\lambda = 0.25$. At both extremes we
        have that the highest accuracy is obtained when $\lambda = 1.0$. In the
        middle of the graph however we observe that the highest accuracy AND
        lowest is accusation error is obtained when $\lambda = 0.25$. It seems
        as if the best $\lambda$ value changes depending on what threshold is
        chosen. We will now discuss the three threshold intervals.

        When $\theta \in (0, 0.2)$ higher accuracy is obtained as $\lambda
        \rightarrow 1$ however it is clear that there is diminishing returns
        for increasing $\lambda$. There is a large difference in accuracy
        between $\lambda = 0.25$ and $\lambda = 0.5$, a smaller difference
        between $\lambda = 0.5$ and $\lambda = 0.75$ and an even smaller
        difference between $\lambda = 0.75$ and $\lambda = 1.0$. At the same
        time the accusation error increases as $\lambda \rightarrow 1$ but it
        does not seem that it increases with diminishing returns. The best
        prediction system in this interval is therefore a tradeoff between the
        accusation error and accuracy obtained. You can increase your accuracy
        by increasing $\lambda$ but at the same time the accusation error will
        also rise. We solved that problem by maximizing the accuracy while
        keeping the accusation error under a threshold which seems to have been
        the right thing to do. When looking at the Figure it seems to us as if
        the best weighing for time is either $\lambda = 0.25$ or $\lambda = 0.5$
        as those have low accusation error coupled with fine accuracy. Let us
        consider the reason that the accuracy rises when we give more weight
        to the first assignment. In this interval $\theta$ is low. That means
        that we will get almost all positive cases correct. The accuracy above
        0.5 must consist of those negative cases where the assignment is most
        clearly written by someone else than a candidate author. When we predict
        that negative text against all of a candidate authors texts it has to
        be extremely different for it to get very low scores for all of that
        authors texts. However there is a good chance that it will be different
        from one of the texts i.e. the newest text. Therefore higher $\lambda$
        values has a higher chance of reporting a negative meaning that they
        will obtain a higher accuracy on this part of the graph but also a
        higher accusation error. Similarly lower $\lambda$ values will have a
        lower chance of reporting a negative giving a lower accuracy but also a
        lower accusation error.

        When $\theta \in (0.4, 0.6)$ higher accuracy is obtained as $\lambda
        \rightarrow 0.25$. At the same time the lowest accusation error is
        also obtained when $\lambda$ is low. It is therefore clear that the
        best configuration in this interval is low $\lambda$ values. But it is
        also clear that $\lambda$ should be greater than 0 (not uniform) as
        the uniform weights is clearly worse than all the other. Lets consider
        why that might be the case. Here in the middle of the graph we do
        not require extreme values to produce either negatives or positives.
        Therefore a weigh that is closer to uniform is likely to produce better
        results since it better use information from all texts available.

        When $\theta \in (0.8, 1.0)$ higher accuracy is obtained as $\lambda
        \rightarrow 1.0$. As in the first interval the accuracy is obtained with
        diminishing returns. Unlike the first interval the accusation error is
        now lowest for higher $\lambda$. It is therefore clear that the best
        $\lambda$ value in this interval is $\lambda = 1.0$. Again we have that
        the reason for obtaining greater accuracy when $\lambda$ increases in
        is that at this extreme we will get almost all negative cases correct
        but only a small subset of the positives will be correct. Therefore the
        further away from uniform weights we get we will report more positives
        giving us higher accuracy. In this case however that does not lead
        to higher accusation error since almost all problems is reported as
        negative and the few we move to positive will reduce the accusation
        error. The best $\lambda$ in this case is irrelevant since we would
        never choose a $\theta$ in this range. Such a $\theta$ would lead to way
        to many false accusation.

        This whole discussion has been based on the picture presented
        for the dataset of 50\% positives and 50\% negatives. In Figure
        \ref{fig:conv_char_prediction_zoom_04} we see a slightly different
        picture. When there is less negatives available $\lambda = 1.0$ does
        not lead to the best results in the interval $\theta \in (0, 0.2)$.
        Instead $\lambda = 0.25$ or $\lambda = 0.5$ seem to give the best
        results. Since it is now more risky to report a negative it makes sense
        that weights closer to uniform performs better since more is required
        to make them report a negative in this interval. It is still clear that
        $P_\mathcal{U}$ is the worst prediction system so we can still conclude
        that the newest assignment is more indicative of writing style than the
        oldest.

    \item[$P_l$]

        The idea behind the length based prediction system was that
        longer texts would better reflect the writing style of an
        author. Some of the texts in the dataset are only a few hundred
        characters long which means that only few of the n-grams the
        networks are looking for will be present in those texts. In Figure
        \ref{fig:conv_char_prediction_zoom_50_text_length} we have shown a
        comparison between $P_\mathcal{U}$ and $P_l$. It can be seen there that
        it is always better to weight based on text length than it is to just
        use uniform weights. The curves follow each other closely but $P_l$
        have slightly higher accuracy and slightly lower accusation error than
        $P_\mathcal{U}$.

        \begin{figure}
            \centering
            \textbf{$P_l$ 0.5 Split}\par\medskip
            \includegraphics[width=0.7\textwidth]{./pictures/discussion/conv_char_nn_prediction_zoom_50_text_length}
            \caption{Illustrate important intervals for our $P_l$ prediction
                systems for the \gls{conv-char-NN} network on the 0.5 split
                dataset. On the left we have shown the beginning of the curves,
                in the middle we have shown the top accuracy and to the right we
                have shown the end of the curves.}
            \label{fig:conv_char_prediction_zoom_50_text_length}
        \end{figure}

        That is exactly the result we expected and shows that it was a good idea
        to look at text length as part of our prediction systems. We have not
        shown the results of the 0.04 split as the same is the case there.

    \item[$P_{lepx_{0.25}}$]

        This prediction system were a combination of the time based and text
        length based prediction systems. Multiple different networks had this
        configuration as the configuration that maximized accuracy. We have
        already concluded that both $P_l$ and $P_{exp_\lambda}$ was better
        weightings than $P_\mathcal{U}$. So the interesting thing about this
        weight is not whether or not it beats $P_\mathcal{U}$, but rather
        whether or not it is better than $P_l$ and $P_{exp_\lambda}$. In Figure
        \ref{fig:conv_char_prediction_zoom_50_text_length_and_time} we have
        shown the performance of $P_{lexp_\lambda}$.

        \begin{figure}
            \centering
            \textbf{$P_{lexp_{0.25}}$ 0.5 Split}\par\medskip
            \includegraphics[width=0.7\textwidth]{./pictures/discussion/conv_char_nn_prediction_zoom_50_text_length_and_time}
            \caption{Illustrate important intervals for our $P_l$ prediction
                systems for the \gls{conv-char-NN} network on the 0.5 split
                dataset. On the left we have shown the beginning of the curves,
                in the middle we have shown the top accuracy and to the right we
                have shown the end of the curves.}
            \label{fig:conv_char_prediction_zoom_50_text_length_and_time}
        \end{figure}

        In that Figure we see that $P_{exp_{0.25}}$ has generally better
        performance than $P_{lexp_{0.25}}$ except right at the maximum accuracy.
        At the maximum accuracy $P_{lexp_{0.25}}$ is able to just barely beat
        $P_{exp_{0.25}}$. Almost all the networks used the $P_{lexp_{0.25}}$
        prediction system as the best prediction system and it must be this
        small difference that causes that.

        It is interesting that the performance of $P_{lexp_{0.25}}$ is only best
        right at the maximum accuracy. Maybe that is due to something similar to
        the effect we saw with $P_{exp_{\lambda}}$ where higher $\lambda$ values
        was best at the extremes and lower $\lambda$ values were better in the
        middle of the graphs.

    \item[$P_{MV}$]

        The majority vote prediction $P_{MV}$ system is similar to the
        uniform prediction system $P_{\mathcal{U}}$. We therefore expect
        about similar performance. What can actually be seen in Figure
        \ref{fig:conv_char_prediction_zoom_50_majority_vote} is that in the
        beginning $P_{MV}$ has substantially higher accuracy and accusation
        error. The picture is similar to the $P_{exp_{1.0}}$ prediction system.
        In the middle the uniform weight function is clearly better than the
        majority vote and the accusation error curves cross. At the end of the
        graph the majority vote again has the highest accuracy but now has the
        lowest accusation error.

        \begin{figure}
            \centering
            \textbf{$P_{MV}$ 0.5 Split}\par\medskip
            \includegraphics[width=0.7\textwidth]{./pictures/discussion/conv_char_nn_prediction_zoom_50_majority_vote}
            \caption{Illustrate important intervals for our $P_{MV}$ prediction
                systems for the \gls{conv-char-NN} network on the 0.5 split
                dataset. On the left we have shown the beginning of the curves,
                in the middle we have shown the top accuracy and to the right we
                have shown the end of the curves.}
            \label{fig:conv_char_prediction_zoom_50_majority_vote}
        \end{figure}

\end{description}


\subsection{Writing Style Changes}

In this section we discuss how a secondary school students writing style changes
during his/her school years. We observed that time based weights worked well
when doing authorship verification. That suggests that there is some development
in students writing style which would also be expected. Secondary school is
probably one of the time periods writing style changes the most.

To get an idea of how much the writing style of a student changes over time we
looped through each author in the \gls{D} dataset. We created a positive and a
negative sample for each of the authors and used \gls{conv-char-NN} to predict
each of the authors texts against the positive and negative sample. We then
computed the average score of the newest text, the average score of the second
newest text, and so on for both positives and negatives. If the students writing
style changes over time we would expect the newest text having an higher average
score than the oldest texts for the positive samples. We also expect that the
negative samples are to have an average of less then 0.5 but about the same
for each text. In Figure \ref{fig:writing_style_changes} we have illustrated
the average positive and negative predictions produced by our network. We only
predict the newest 20 texts as very few authors in the dataset has more than
that. So the averages of the texts above 20 vary wildly.

\begin{figure}
    \centering
    \textbf{Writing Style Changes}\par\medskip
    \includegraphics[width=0.7\textwidth]{./pictures/discussion/writing_style_change}
    \caption{Average predictions of newest to oldest texts for positive samples
        and negative samples by \gls{conv-char-NN}. The newest text is shown to
        the left and the oldest to the right. We have also shown a plot of the
        different time based weight functions we tried using.}
    \label{fig:writing_style_changes}
\end{figure}

In that figure it is very clear that writing style does change over time. The
positive samples fall from 80\% prediction on the newest assignment to just
over 50\% on the oldest assignment. That is a very large drop that suggests
that after only 20 turn ins the writing style has changed so drastically that
our network almost believes it is someone else. It is very interesting that the
drop in accuracy seem to follow closely our $P_{exp_{0.25}}$ prediction system
weights. That is probably the reason why that particular $\lambda$ value so
often gave the best results.

The negative curve is a bit weird. We expected it to stay flat below 50\% but
it seems to be rising towards 50\%. The curve is clearly more flat than the
positive curve but there does still seem to be a trend towards 50\%.
