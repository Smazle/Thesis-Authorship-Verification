% TODO: Introduce the MaCom problem. Maybe with a reference to this article.
% https://www.dr.dk/nyheder/indland/elever-bruger-ghostwritere-til-eksamen
\section{Introduction} \label{sec:introduction}

In this thesis we work on the problem of authorship verification using texts
written by Danish secondary school pupils. Authorship verification is the
problem of determining whether a text $x$ is written by the same author as the
author of a set of texts $Y$. The usual authorship verification method works by
first extracting features from each $y \in Y$ and from $x$ and then determining
whether or not the author is the same by comparing the features.

Authorship verification and authorship attribution, is the ability to
distinguish between texts, and their associated author, based on a set of
extracted textual features. The automation of this has been a lively branch of
research ever since the entering the digital age, giving birth to online digital
forensics tasks, such as \cite{pan:2015}. Initial attempts at quantifying the
writing style can be seen by \cite{Mendenhall237}, who attempted to determine
the authorship of several of Shakespeare's texts. There is a theory that
Shakespeare didn't write some or all of his texts, or that he was at least a
front for one or more unknown authors. \cite{Mendenhall237} attempted this
classification, using the frequency distribution of words of different lengths.
Throughout the years the approaches to this problem has changed quite a bit.
When authorship attribution started interesting researchers the approaches were
more Stylometric in nature. In addition to that, fully automated systems were
rare as authorship verification/attribution was used mostly in an supporting
manner. It was during the 1990's that fully automated systems became more
prevalent. The main reason for this was the Internet. Before the Internet, the
data available simply wasn't suitable for authorship attribution tasks. Books
were too big, resulting in a lack in homogeneity, and the amount of authors, and
bench-marking data was to small. The Internet paved the way for insurmountable
amount of data, and variations of that data, impacting areas such as information
retrieval, machine learning and \gls{NLP}.

This is where authorship verification as we know it enters the picture. Along
with authorship \gls{NLP} approaches such as authorship attribution and
plagiarism detection. Authorship verification is, as the name suggests, the task
of determining whether or not a person is the author of a proposed text. As such
author verification is a sub-problem of what is called authorship attribution.
Authorship attribution describes the task of determining the authorship of a
text from a set of proposed authors. In order for any fully automatic authorship
verification to work, stylometric features describing the text has to be
automatically extracted. These features span multiple linguistic layers, ranging
from the low level character n-grams, to the high level application specific
features such as text creation date, and number of edits. It is using these
features many of the current day state-of-the-art approaches are based.

Some of these are profile-based approaches where an author is represented
by all the texts in the his library of work. It is from this collective
representation on extracts the features from, thus resulting in a feature
representation that the describe the authors writing style. This yields a more
general representation of the author, rather than it describing just a singular
texts, which would be called an instance based approach. An example of such a
profile-based approach would be the concatenation of the authors library of
work, from which the features are then extracted as if a giant single text.
\cite{stamatos2009}

An example of the profile-based approach, which is highly regarded by
\cite{stamatos2009}, make use of a compression. This works, not by having
represented each author library as a set of features, but rather makes use of
compression to do so. In the initial scenario, we have a set of author-profiles,
which is the concatenation of all their individual work. When a new text is then
introduced the authorship is determined by looping through each author, adding
it library (their concatenated profile). The profile is then compressed, both
with and without the new text included in its library. The bit-wise difference
is then computed, by subtracting them from one another, resulting in what is
essentially the cross-entropy between the two texts. The author which the lowest
cross-entropy is then considered to be the author of this new text. Different
methods of comprehension can be used as the base for this model, each giving
different results. In the instance that \cite{stamatos2009} describes, RAR
compression yielded the best results, but this of cause depends on the scenario
one finds oneself in.

As for instance-based approaches they use a more classic machine learning
approach. Texts are again represented as a set of features. Then each texts
features is used as a training sample. Thus several simple machine learning
approaches can be applied to the data as either a binary classification problem,
in the case of authorship verification, and a multi-class classification problem
in the case of authorship attribution.

\subsection{Deep Learning} 

In this paper we will approach the authorship verification/attribution problem
using deep-learning. The terms deep learning was first introduced to machine
machine learning in 1989, and afterward to \gls{NN}'s in 2000. The terms quickly
became synonymous with \gls{NN}'s due to them being some of the more efficient
deep learning methods.\cite{Schmidhuber:2015} 

A standard simple \gls{NN} consists of a set interconnected processors, called
neurons. Each of these neurons has a real-valued activation associated with
it, which activates differently depending on the specific neuron. The input
neurons activate through perceiving the environment, or in other words, when
it is fed data externally. Other neurons are simply activated through the
weighted activation of previous neurons. It these weights the focus is on in
deep learning, more details regarding this will be presented later in the
paper.\cite{DBLP:journals/corr/Schmidhuber14}

\gls{NN}'s have been around since the 1940'ies. However, back then they were
merely variations of the linear regressors used at the time, and wasn't
very reminiscent of the Networks on can see today. It wasn't until the
late 1960'ies, early 70'ies, that networks comparable to the more modern
approaches surfaced. Examples of such early works, are the two publications
\cite{ivakhnenko1973cybernetic} and \cite{4308320}, which describe multi-layered
feed-forward supervised neural network architectures. While the work described
in \cite{4308320} was indeed one of the first cases of the modern \gls{NN},
actually getting the network to learn was still a problem, as the tweaking of
individual weights attributed to each neuron in the network wasn't trivial.
Little did they know, research to solve that problem was already in progress.
The basics of continuous back \gls{BP} was initially described in 1960, in
\cite{Kelley1960}, quickly followed by a simpler approach which used only the
chain rule in 1962, \cite{DREYFUS196230}. It wasn't until 1970 that the modern
version of \gls{BP} was described, using automatic differentiation as its'
basis. With this, the increase in research of usages of \gls{BP} increased the
following decades. As the computational power increased several 1000 folds in
the 90'ies and 2000, so the did the practical usage of \gls{BP}, and \gls{NN} in
general, a increase that has since increased even more. \cite{Schmidhuber:2015}

Like with the history of authorship attribution, research in this area of
science picked up more interest, as we entered the modern computational age,
and with the introduction of the \gls{CNN}. \gls{CNN}'s bases themselves on
the early work described in \cite{TJP:TJP19681951215}. They showed that cats
and monkeys visual cortexes contains a set of neurons, each individually
responding to a receptive field, or area, of their field of view. Neighboring
receptive fields all have a certain amount of overlap, however in the end
a cohesive view is created. This is what paved the way for neocognition
in 1980\cite{Fukushima1980}, the basis of \gls{CNN}'s, which works in a
very similar manner, by looking overlapping subsections of some data. These
convolutional neurons however were rarely used alone, but together with a
down-sampling neuron such Max Pooling introduced in 1993.\cite{Schmidhuber:2015}

TODO: Add more maybe? Change degree of detail


\subsection{Motivation \& Problem Definition} 

It is using this instance-based approach that we will attempt, in
the following paper, to device a deep learning approach which can
solve the authorship verification for the Danish company \texttt{MaCom
A/S}\footnote{\url{http://www.macom.dk/}}. MaCom is the company behind the
product \texttt{Lectio}\footnote{\url{https://www.lectio.dk/}}, which is a
website that allows for student administration, communication, and digital
teaching aid, and is used on schools all over Denmark. A service the website
offer, is the submission, and handling of assignments made by student throughout
their enrollment. MaCom has shown interest in determining whether or not these
assignment were possible written by someone other than the student. It is
especially final, end of school assignment, which might has an elevated risk of
plagiarism and inclusion of ghost-writers. It is this risk they want to mitigate
by having a piece of software which is able to detect such cases. However, the
accuracy of this piece of software is of paramount importance. They don't want
to wrong-fully accuse any student of not writing their assignment. For this
reason they want a specificity (\gls{TNR}, will be covered later) of at least
95\%, even if this results in only a 10\% overall accuracy.

\begin{itemize}
    \item CNN's are good at extracting features from low level data.
    \item Elaborate description of MaCom data and the "MaCom Problem".
    \item Expand in general
\end{itemize}
