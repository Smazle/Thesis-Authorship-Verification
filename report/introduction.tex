% Introduction should be written in the present tense!
%
% Your introduction needs to include background information which is generally
% accepted as fact in a discipline. You also need to explain why the research
% you are reporting is important. It is usually presented in the present tense.
%   - https://services.unimelb.edu.au/__data/assets/pdf_file/0009/471294/Using_tenses_in_scientific_writing_Update_051112.pdf

\section{Introduction} \label{sec:introduction}

% An introduction to the context or background of the topic (you could include
% interesting facts or quotations)

In this thesis we work on the problem of authorship verification using texts
written by Danish secondary school pupils. Authorship verification and
authorship attribution describe the ability to distinguish between authors
of texts based on a set of extracted textual features. The automation of
authorship attribution/verification has been a active branch of research ever
since the beginning of the digital age, giving birth to online digital text
forensics tasks such as the work by \citet{pan:2015}. Initial attempts at
quantifying writing style can be seen in \citet{Mendenhall237}, who attempted
to determine the authorship of several of Shakespeares texts. There is a theory
that Shakespeare did not write some or all of his texts, or that he was a
synonym for one of more unknown authors. \citet{Mendenhall237} attempted this
classification using the frequency distribution of words of different lengths.
Throughout the years the approaches to this problem have changed quite a bit.
When authorship attribution started to interest researchers, the approaches were
\textit{Stylometric} i.e. they were based on the linguistic style of authors.
In addition to that, fully automated systems were rare and were mostly used
in a supporting manner. It was during the 1990s that fully automated systems
became more prevalent. The main reason for this was the Internet. Before the
Internet, the data available simply was not suitable for authorship attribution
tasks. Books were too big resulting in a lack of homogeneity, and the amount of
authors and bench-marking data was too small. The Internet paved the way for
insurmountable amounts of data and variations of that data, impacting areas such
as information retrieval, machine learning and \gls{NLP}.

In order for any fully automatic authorship verification to work, Stylometric
features describing the text have to be automatically extracted. These features
span multiple linguistic layers, ranging from the low level character n-grams
to the high level application specific features such as text creation date and
number of edits. Many of the current day state-of-the-art approaches are based
on these features.

% The reason for writing about this topic:

We want to experiment with and solve an authorship verification task for the
Danish company MaCom \footnote{\url{http://www.macom.dk/}}. MaCom is the company
behind the product Lectio \footnote{\url{https://www.lectio.dk/}} which is a
website that allows for student administration, communication, and digital
teaching aid. Lectio is used in more than 90 \% of Danish secondary schools
\footnote{\url{https://www.business.dk/digital/df-ordforer-om-pivabent-lectio-de
t-kan-vaere-farligt-hvis-man-taenker}}. A service the website offers is the
submission and handling of assignments written by students throughout their
enrollment. MaCom has shown interest in determining whether or not these
assignment were written by someone other than the student (a ``ghost writer'').
Ghost writing is especially a problem on the \gls{SRP} assignment. \gls{SRP}
is an interdisciplinary assignment all Danish secondary school students turn
in at the end of their third year. There is no oral examination for the
assignment and the grade obtained is part of the students final results from
the secondary school. The combination of the importance of the assignment and
no oral examination leads to some students turning in assignments written by
ghost writers. The Danish state owned public service radio and television
company \texttt{DR} has written an article describing the ghost writer problem
\footnote{\url{https://www.dr.dk/nyheder/indland/elever-bruger-ghostwritere-til-
eksamen}}. The article describes that when asking 2000 student, 58\% got help
from friends or family, and around 15\% knew someone who had their assignment
written by someone else. In this thesis we setup a system for detecting ghost
writing using machine learning methods. The system is meant to help teachers
make decisions about whether or not an assignment turned in by a student is
written by someone else. It is not important that the system catches 100\% of
the assignments written by someone else. If the system only catches a fraction
of the cheaters, it will deter other students from cheating. What is most
important is that the system limits the number of false accusations. The system
should also be able to give evidence for why we think a particular assignment is
written by someone else. Such evidence could for example be that the frequency
of particular words is significantly different in the new assignment than in all
previously handed in by the student. Evidence for why we think an assignment is
written by someone else will help a teacher if he/she wants to accuse a student
of using a ghost writer, as the product should work in a supplementary manner.


\subsection{Notation}

We will work with several different kinds of objects. The main ones are texts
and authors. We will generally name texts $t$ and authors $\alpha$. If multiple
texts or authors are used we will generally call them $t'$ and $\alpha'$ or
$t_n$ and $\alpha_n$ if many texts or authors are needed. Let $\mathcal{A} =
{\alpha_1, \alpha_2, \dots, \alpha_n}$ denote the set of all authors, where
$T_\alpha$ denotes the set of texts written by the author $\alpha$. Let
$\mathcal{T} = \bigcup_{\alpha \in \mathcal{A}} T_\alpha$ subject to $T_{\alpha}
\cap T_{\alpha'} = \emptyset$ for all $\alpha \neq \alpha'$. Additionally
let $\overline{T_\alpha} = \mathcal{T} \setminus T_\alpha$ denote the set of
texts which are not written by author $\alpha$. Texts will be represented by a
sequence of characters. We denote the length of that sequence $|t|$. Each text
also has a point of time where it was written. This time is represented used the
function $\tau \colon \mathcal{T} \rightarrow \mathbb{N}^+$, which returns the
time in months relative to the earliest submitted $t \in \mathcal{T_\alpha}$.

For vectors we will use the standard notation of naming them in lower case bold
letters as $\mathbf{x}$. Matrices will be named upper case non bold letters.
We let elementwise multiplication of both vectors and matrices be denoted with
$\otimes$ and elementwise summation as $\oplus$. We will denote submatrices as
$X[a,b;c,d]$ which means the submatrix of $X$ that consist of rows $a$ to $b$
and columns $c$ to $d$.

% Introduce the main ideas that stem from your topic/title and the order in
% which you will discuss them?

As described the problem main problem we try to solve in this thesis is
authorship verification which is defined below.

\begin{definition}[Authorship Verification]
    \label{def:authorship_verification}

    Given a set of texts $T_\alpha$ written by author $\alpha$ and a single text
    $t$ of unknown authorship, determine if $\alpha$ is the author of $t$.

\end{definition}

Authorship verification is closely linked with the problem of authorship
attribution as can be seen in the definition of authorship attribution shown
below.

\begin{definition}[Authorship Attribution]

    Given a set of authors $A = \{\alpha_1, \alpha_2,...\alpha_n\}$, each with
    set of text $T_{\alpha_i}$, and a text of unknown authorship \texttt{t},
    determine which $\alpha_i \in A$ is the author of \texttt{t}.

\end{definition}

The problems are closely linked since an answer for authorship attribution
can be obtained by using authorship verification and an answer for authorship
verification can be obtained by using authorship attribution. Consider a case
where we are given an oracle answering the authorship verification problem
$\mathcal{S}$. $\mathcal{S}$ is a mapping from an author $\alpha$ and text $t$
to either true or false. Given an instance of the authorship attribution problem
with authors $A$ and text $t$ we solve the problem by using $\mathcal{S}$ on
each author $\alpha \in A$. We return the author where $\mathcal{S}$ reports
true. Now consider a case where we are given a solution to the authorship
attribution problem $\mathcal{S}'$. $\mathcal{S}'$ is now a mapping from a set
of authors $A$ and text $t$ to an author $\alpha \in A$. Given an instance of
the authorship verification problem with author $\alpha_i \in A$ and text $t$
and a set of texts written by different authors $\overline{T}_{\alpha}$ we solve
the verification problem by applying the attribution function to the texts
$T_{\alpha} \cup \overline{T}_{\alpha}$ and the text of unknown authorship $t$.
If $t \in T_{\alpha}$ we report true and otherwise false.


\subsection{Roadmap}

In Section \ref{sec:related_work} we will go through previous work in authorship
verification/attribution and previous work performed on MaCom's dataset. We will
specifically focus on neural network based methods as we want to expand on those
methods.

In Section \ref{sec:method} we describe the theory behind the methods we are
working with. We start out by briefly describing our baseline methods after
which we describe the method that we are going to be working with. Specifically
we describe the theoretical foundations of \textit{Siamese neural networks} that
we have focused on.

In Section \ref{sec:data} we describe the dataset we have been working with. We
present statistics about average number of texts and the number of authors and
so on. We also describe the preprocessing steps we performed on the dataset.

In Section \ref{sec:experiments} we go through the different network
architectures we used. We present the results we obtained on a validation
dataset and how we reached the final architectures we settled on.

In Section \ref{sec:results} we presents our baseline performance on the testset
and the performance of our neural network based methods performance.

In Section \ref{sec:discussion} we discuss the results we presented in the
previous section. We discuss the applicability of the method we developed to the
real world and we discuss how the system can be used if implemented.

In Section \ref{sec:conclusion} we present our conclusion based on the results
and discussion sections.

In Section \ref{sec:future_work} we go through what we would like to do in the
future and what we did not get to do in our project.
