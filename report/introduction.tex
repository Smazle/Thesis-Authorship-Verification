\section{Introduction} \label{sec:introduction}

% An introduction to the context or background of the topic (you could include
% interesting facts or quotations)

In this thesis we work on the problem of authorship verification using texts
written by Danish secondary school pupils. Authorship verification and
authorship attribution, is the ability to distinguish between authors of texts,
based on a set of extracted textual features. The automation of authorship
attribution/verification has been a lively branch of research ever since the
beginning of the digital age, giving birth to online digital text forensics
tasks, such as \cite{pan:2015}. Initial attempts at quantifying writing style
can be seen by \cite{Mendenhall237}, who attempted to determine the authorship
of several of Shakespeare's texts. There is a theory that Shakespeare didn't
write some or all of his texts, or that he was at least a front for one or more
unknown authors. \cite{Mendenhall237} attempted this classification, using the
frequency distribution of words of different lengths. Throughout the years the
approaches to this problem has changed quite a bit. When authorship attribution
started to interest researchers the approaches were Stylometric. In addition to
that, fully automated systems were rare as authorship attribution/verification
was mostly used in an supporting manner. It was during the 1990's that fully
automated systems became more prevalent. The main reason for this was the
Internet. Before the Internet, the data available simply wasn't suitable for
authorship attribution tasks. Books were too big, resulting in a lack in
homogeneity, and the amount of authors, and bench-marking data was to small.
The Internet paved the way for insurmountable amount of data, and variations of
that data, impacting areas such as information retrieval, machine learning and
\gls{NLP}.

In order for any fully automatic authorship verification to work, stylometric
features describing the text has to be automatically extracted. These features
span multiple linguistic layers, ranging from the low level character n-grams,
to the high level application specific features such as text creation date,
and number of edits. It is using these features many of the current day
state-of-the-art approaches are based.

% The reason for writing about this topic:

In this thesis we want to experiment with and solve an authorship
verification task for the Danish company \texttt{MaCom A/S}
\footnote{\url{http://www.macom.dk/}}. MaCom is the company behind the
product \texttt{Lectio} \footnote{\url{https://www.lectio.dk/}}, which is a
website that allows for student administration, communication, and digital
teaching aid. Lectio is used on schools all over Denmark. A service the
website offers, is the submission and handling of assignments written by
student throughout their enrollment. MaCom has shown interest in determining
whether or not these assignment were possible written by someone other than
the student (a "ghost writer"). Ghost writing is especially a problem on
the \gls{SRP} assignment. \gls{SRP} is an interdisciplinary assignment all
Danish secondary school students turn in on their second year. There is
no oral examination for the assignment and the grade obtained is part of
the students final results from the secondary school. The combination of
the importance of the assignment and no oral examination leads to students
turning in assignments written by ghost writers. The Danish state owned public
service radio and television company \texttt{DR} has written about the problem
\url{https://www.dr.dk/nyheder/indland/elever-bruger-ghostwritere-til-eksamen}.
In this thesis we setup a system for detecting ghost writing based on machine
learning methods. The system is meant to help teachers make decisions about
whether or not an assignment turned in by a student is written by someone else.
It is not important that the system catches 100\% of the assignments written by
someone else. If the system only catches a fraction of the cheaters it will
function as a deterrent for other students cheating. What is most important is
that the system does not accuse anyone of cheating who has turned in their own
assignment. The system should also be able to give a reason for why we think a
particular assignment is written by someone else. Such a reason could for
example be that the frequency of particular words are significantly different in
the new assignment than in all previously handed in by the student. Reasons for
why we think an assignment is written by someone else will help a teacher if
he/she wants to accuse a student of using a ghost writer.

% Introduce the main ideas that stem from your topic/title and the order in
% which you will discuss them?

A more formal definition of the problem we will be working with are,


\begin{definition}

    Authorship verification is a problem where you are given a set of texts $Y$
    that are all written by the same author $a$ and a single text $x$ of unknown
    authorship. You then have to determine whether the text $x$ is written by
    the author $a$.

\end{definition}

Authorship verification is closely linked with the problem of authorship
attribution,

\begin{definition}

    In authorship attribution you are given a set of authors $A$ and a text $x$.
    Each $a \in A$ has a set of associated texts $T(a)$. The task is then to
    find the $a \in A$ that has written the text $x$.

\end{definition}

The problems are closely linked since an answer for authorship attribution can
be obtained by using authorship verification and an answer for authorship
verification can be obtained by using authorship attribution. Consider a case
where we are given a solution to the authorship verification problem $S$. $S$ is
a mapping from an author $a$ and text $x$ to either true or false. Given an
instance of the authorship attribution problem with authors $A$ and text $x$ we
solve the problem by using $S$ on each author $a \in A$. We return the author
where $S$ reports true. Now consider a case where we are given a solution to the
authorship attribution problem $S$. $S$ is now a mapping from a set of authors
$A$ and text $x$ to an author $a \in A$. Given an instance of the authorship
verification problem with author $a$ and text $x$ and a set of different authors
$AD$ we solve the verification problem by applying the attribution function to
the authors $\{a\} \cup AD$ and the text $x$. If we get $a$ back we report
true and otherwise false.

\subsection{Deep Learning} 

In this paper we will approach the authorship verification/attribution problem
using deep-learning. The terms deep learning was first introduced to machine
machine learning in 1989, and afterward to \gls{NN}'s in 2000. The terms quickly
became synonymous with \gls{NN}'s due to them being some of the more efficient
deep learning methods.\cite{Schmidhuber:2015} 

A standard simple \gls{NN} consists of a set interconnected processors, called
neurons. Each of these neurons has a real-valued activation associated with
it, which activates differently depending on the specific neuron. The input
neurons activate through perceiving the environment, or in other words, when
it is fed data externally. Other neurons are simply activated through the
weighted activation of previous neurons. It these weights the focus is on in
deep learning, more details regarding this will be presented later in the
paper.\cite{DBLP:journals/corr/Schmidhuber14}

\gls{NN}'s have been around since the 1940'ies. However, back then they were
merely variations of the linear regressors used at the time, and wasn't
very reminiscent of the Networks on can see today. It wasn't until the
late 1960'ies, early 70'ies, that networks comparable to the more modern
approaches surfaced. Examples of such early works, are the two publications
\cite{ivakhnenko1973cybernetic} and \cite{4308320}, which describe multi-layered
feed-forward supervised neural network architectures. While the work described
in \cite{4308320} was indeed one of the first cases of the modern \gls{NN},
actually getting the network to learn was still a problem, as the tweaking of
individual weights attributed to each neuron in the network wasn't trivial.
Little did they know, research to solve that problem was already in progress.
The basics of continuous back \gls{BP} was initially described in 1960, in
\cite{Kelley1960}, quickly followed by a simpler approach which used only the
chain rule in 1962, \cite{DREYFUS196230}. It wasn't until 1970 that the modern
version of \gls{BP} was described, using automatic differentiation as its'
basis. With this, the increase in research of usages of \gls{BP} increased the
following decades. As the computational power increased several 1000 folds in
the 90'ies and 2000, so the did the practical usage of \gls{BP}, and \gls{NN} in
general, a increase that has since increased even more. \cite{Schmidhuber:2015}

Like with the history of authorship attribution, research in this area of
science picked up more interest, as we entered the modern computational age,
and with the introduction of the \gls{CNN}. \gls{CNN}'s bases themselves on
the early work described in \cite{TJP:TJP19681951215}. They showed that cats
and monkeys visual cortexes contains a set of neurons, each individually
responding to a receptive field, or area, of their field of view. Neighboring
receptive fields all have a certain amount of overlap, however in the end
a cohesive view is created. This is what paved the way for neocognition
in 1980\cite{Fukushima1980}, the basis of \gls{CNN}'s, which works in a
very similar manner, by looking overlapping subsections of some data. These
convolutional neurons however were rarely used alone, but together with a
down-sampling neuron such Max Pooling introduced in 1993.\cite{Schmidhuber:2015}

TODO: Add more maybe? Change degree of detail



