\section{Method} \label{sec:method}

In this Section we will describe the method we have used to solve the Authorship
Verification problem presented. In general there are two methods of representing
each author. There is the instance based approach and the profile based
approach. In the instance based approach each author is represented by a set of
texts they have written while in the profile based approach they are represented
by the sum of the set of texts they have written. The instance based approach is
illustrated in Figure \ref{fig:instance_based} and the profile based approach is
illustrated in Figure \ref{fig:profile_based}.

\begin{figure}[htb]
    \centering
    \textbf{Instance Based Authorship Verification or Authorship Attribution}\par\medskip
    \includegraphics[scale=0.5]{./pictures/method/InstanceBased.png}
    \caption{Illustrate the typical instance based Authorship Verification or
        Authorship Attribution solution setup. Inspired by \cite{stamatos2009} a
        set of authors are given as input each with a set of texts. Some Machine
        Learning model is trained on the input texts and the model is used to
        predict an unknown text. }

    \label{fig:instance_based}
\end{figure}

\begin{figure}[htb]
    \centering
    \textbf{Profile Based Authorship Verification or Authorship Attribution}\par\medskip
    \includegraphics[scale=0.5]{./pictures/method/ProfileBased.png}

    \caption{Illustrate the typical profile based Authorship Verification or
        Authorship Attribution setup. Inspired by \cite{stamatos2009} the texts
        of each author are combined using some combination function such as an
        average or a concatenation. Those \textit{profiles} are then given to a
        Machine Learning model to train. The output is a model which is used to
        predict unknown texts. }

    \label{fig:profile_based}
\end{figure}

We will generally use the instance based approach. The reason we use an instance
based approach is that it allows us to use extra information from each single
text. For example writing style may change over time especially for secondary
school pupils that evolve very much in a short amount of time. Since we use an
instance based approach we are able to weight similarity to newer texts higher
than similarity to older texts.

There are also another split between methods that we consider. There are
generalizing and author specific models. In a generalizing model only a single
model is trained on data from multiple authors and are able to make predictions
about previously unseen authors. In the author specific model a separate model
has to be trained for each author and is not able to make predictions for
previously unseen authors. The generalizing model has several advantages, it
only has to be trained once and after that it can be used for everyone and
it can make use of big data since it can use data from several authors for
training. The author specific model has the advantage that it can better fit
to the specific quirks of a particular author since it is trained separately
for each author. The downside of the author specific approach is that a new
model has to be trained for each new author. We will focus on the generalizing
approach since it is easier to implement for MaCom as they only have to train a
model once.

As a unit of measuring the quality of our models, and how well they adhere to
95\% specificity constraint, we will also compute the number of \gls{TP}s,
\gls{TN}s, \gls{FP}s and \gls{FN}s, as was done in a project previously created
by us.\cite{US} In these problems we get,

\begin{itemize}
    \item a \gls{TP} whenever we answer \textit{True} and the texts are written
        by the same author,
    \item a \gls{TN} whenever we answer \textit{False} and the texts are
        \textbf{not} written by the same author,
    \item a \gls{FP} whenever we answer \textit{True} and the texts are
        \textbf{not} written by the same author,
    \item a \gls{FN} whenever we answer \textit{False} and the texts are written
        by the same author.
\end{itemize}

Given those definitions the \gls{TPR}, \gls{FPR}, \gls{TNR} and \gls{FNR}
describes.

\begin{description}
    \item[\gls{TPR}: ]

        The fraction of positives that we reported \textit{True} on i.e. the
        fraction of texts written by the same author that we say are written by
        the same author.

    \item[\gls{FPR}: ]

        The fraction of negatives that we reported \textit{True} on i.e. the
        fraction of texts written by different authors that we say are written
        by the same author.

    \item[\gls{TNR}: ]

        The fraction of negatives that we reported \textit{False} on i.e. the
        fraction of texts written by different authors that we say are written
        by different authors.

    \item[\gls{FNR}: ]

        The fraction of positives that we reported \textit{False} on i.e. the
        fraction of texts written by the same author that we say are written by
        different authors.

\end{description}

And they can be computed as,

\begin{align}
    TPR &= \frac{TP}{TP + FN}, \\
    FPR &= \frac{FP}{FP + TN}, \\
    TNR &= \frac{TN}{TN + FP}, \\
    FNR &= \frac{FN}{FN + TP}.
\end{align}

Using these definition we can also describe the accuracy measure we will be
reporting on throughout our experiments,

\begin{equation}
    \text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}.
\end{equation}


In the case of MaCom, we want to minimize the \gls{FNR} as much as possible,
so as to not wrongfully accuse anyone of not having written their assignment.
This however leaves out a equally important metric. While a low \gls{FNR} is the
goal, MaCom also wants to minimize the number of \gls{FN}s compared to the total
number of accusations made. This can be described as:

\begin{equation}
    \text{Accusation Error} = \frac{FN}{FN + TN}
\end{equation}

The goal is also to keep this error under a certain threshold specified by
MaCom, which in this case is $0.1$ or $10\%$. In other words, of the accusations
we made, only $10\%$ of them are allowed to be false. In order to accommodate
this constraint we can take certain actions, which depends on the model used.
Details about these actions will be addressed in Section \ref{sec:prediction}.

\subsection{Baseline Methods}

In order to gauge the efficiency of our deep learning approaches, we have chosen
to implement some baseline methods. These methods were picked based on their
performance in a previous project written by us \cite{US}. Albeit that project
was only concerned with English texts provided by the \cite{pan:2015}, and
\cite{pan:2014} text forensics tasks, we hypothesize that the performance of
these approaches will perform just as well on Danish texts when being tuned for
the Danish language.


\subsubsection{Extended Delta Method}

One of the best performing methods of \cite{US} was the extended delta method.
As the name suggests the method extends the already existing delta method
described by \cite{evert2015towards}. The normal delta methods consists of first
extracting word frequencies from all texts and using these as the describing
features. After doing this to the entire sample space of texts, and applying a
linear transformation to their respective feature-sets, \gls{KNN} is then used
to determine the author of the introduced texts based on its closest neighbors
in the word-frequency feature-space. The extended delta method, simply expands
on the set of possible features to pick from, rather than being limited to only
using the word-frequencies of the text.


\subsubsection{Author Specific SVM}

Another algorithm used in \cite{US}. Heavily inspired by \cite{hansen2014}
starts out by fetching all texts known to be written by a specific author and an
equal number of texts known not to be written by that same author. It is upon
the feature-set extracted from these texts that a \gls{SVM} is trained, allowing
it to learn the specific author's writing style from the known texts supplied
and in contrast what the writing style of someone not him is. When a new text,
with disputed authorship is presented the hope is that the trained \gls{SVM}
will be able to determine if the author it was trained on, is in fact the author
of this new text as well.


\subsection{Deep Learning}

In this paper we will approach the authorship verification/attribution problem
using deep-learning. The term deep learning, was first introduced to machine
machine learning in 1989, and afterward to \gls{NN}'s in 2000. The terms quickly
became synonymous with \gls{NN}'s due to them being some of the more efficient
deep learning methods.\cite{Schmidhuber:2015}

With the inner workings of the brain used as the basis, a standard simple
\gls{NN} consists of a set interconnected processors, called neurons. Each of
these neurons has a real-valued activation associated with it, which activates
differently depending on the specific neuron. The input neurons activate through
perceiving the environment, or in other words, when it is fed data externally.
Other neurons are simply activated through the weighted activation of previous
neurons. More details regarding these weights will be presented later in the
paper.\cite{DBLP:journals/corr/Schmidhuber14}

\gls{NN}'s have been around since the 1940's. However, back then they
were merely variations of the linear regressors used at the time, and
wasn't very reminiscent of the Networks on can see today. It wasn't until
the late 1960's, early 70's, that networks comparable to the more modern
approaches surfaced. Examples of such early works, are the two publications
\cite{ivakhnenko1973cybernetic} and \cite{4308320}, which describe multi-layered
feed-forward supervised neural network architectures. While the work described
in \cite{4308320} was indeed one of the first cases of the modern \gls{NN},
actually getting the network to learn was still a problem, as the tweaking of
individual weights attributed to each neuron in the network wasn't trivial.
Little did they know, research to solve that problem was already in progress.
The basics of continuous \gls{BP} was initially described in 1960, in
\cite{Kelley1960}, quickly followed by a simpler approach which used only the
chain rule in 1962, \cite{DREYFUS196230}. It wasn't until 1970 that the modern
version of \gls{BP} was described, using automatic differentiation as its
basis. With this, the increase in research of usages of \gls{BP} increased the
following decades. As the computational power increased several 1000 folds in
the 90's and 2000's, so did the practical usage of \gls{BP}, and \gls{NN} in
general\cite{Schmidhuber:2015}. The real life application of \gls{BP}, will be
described in Section \ref{sec:BP}

Like with the history of authorship attribution, research in this area of
science picked up more interest, as we entered the modern computational age, and
with the introduction of the \gls{CNN}. \gls{CNN}s are based on the early work
described in \cite{TJP:TJP19681951215}. They showed that cats and monkeys visual
cortexes contain a set of neurons, each individually responding to a receptive
field, or area, of their field of view. Neighboring receptive fields all have a
certain amount of overlap, however in the end a cohesive view is created. This
is what paved the way for neocognition in 1980\cite{Fukushima1980}, the basis
of \gls{CNN}'s, which works in a very similar manner, looking at overlapping
subsections of data. These convolutional neurons however were rarely used alone,
but together with a down-sampling neuron such as Max Pooling introduced in
1993.\cite{Schmidhuber:2015}


\subsubsection{Neurons}\label{sec:neurons}

As mentioned previously a \gls{NN} consists of a collection of neurons. Each
neuron is a simplified mathematical model, which behaves much like neurons
in the brain would, receiving, processing and transmitting data/information.
Each neuron has a set of inputs called $x_{ij}$ and a single output called
$z_i$, where $i$ refers to the neuron, and $j$ refers to the specific input. The
neurons compute a weighted sum of its inputs and applies an activation function
$h$ to the weighted sum. The weights are called $w_{ij}$, and the bias $w_{i0}$.
The function each neuron computes is then,

\begin{equation}\label{eq:neuron}
    z_i = h(a_i) = h\left(
        \sum_{j = 1}^d w_{ij}x_j + w_{i0}
 \right) = h\left(
        \sum_{j = 0}^d w_{ij}x_j \right)
\end{equation}

A more intuitive model of such a neuron can be seen in Figure \ref{fig:neuron}.

\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{./pictures/method/Neuron.png}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{./pictures/method/NeuronBias.png}
  \end{minipage}
    \caption{The inner workings of a neuron, with and without an implicit
bias \cite{Igel}}
\label{fig:neuron}
\end{figure}

One has the options to arrange the neurons in what is called layers, to achieve
a certain desired behavior, more details regarding these specific layers will
be explained in later chapters. The training of a \gls{NN} consists of changing
the weights applied at each neuron, with the goal of modeling the relationships
present in the data.

\subsubsection{Activation Functions} \label{subsubsec:activation_functions}

The activation function $h$, used at each neuron, defines the output of the
node given a certain input. A simple example of this would be computer chip
circuit, which can be seen as a series of activation functions outputting 0 or
1 depending on their input. This activation function would be a linear one.
When applying activation functions to neurons in \gls{NN}s, they are usually
non-linear, as it allows for the computation of more complex problems using a
smaller amount of neurons, relative to usage of a linear activation function,
as they allow for the universal function approximation, a point also made by
\cite{6797088} A plot of different activation functions is shown in Figure
\ref{fig:activation_functions}.

\begin{figure}
    \centering
    \textbf{Activation Functions}\par\medskip
    \includegraphics[width=0.5\textwidth]{./pictures/method/activation_functions.png}
    \caption{Different activation functions that can be used in neural
        networks.}
    \label{fig:activation_functions}
\end{figure}

Each activation function has its pros and cons. We mainly made use of the
\gls{ReLu} activation function in the hidden layers of our networks. The
reason for this selection is its general purpose use. When selecting an
activation function for your neurons, the best function would be the one which
best approximates the underlying function. Without a good idea as to what
that function might be, \gls{ReLu} is a good starting point. Its simplicity
provides a quick computation time, and its below zero limitation means that a
large portion of the network won't be activating, resulting in an even smaller
computation time. In addition to that, the derivative of the function is 1
in the case of a positive input, resulting in the \gls{BP} loss having equal
influence throughout the network. In the case of other activation functions,
this might not be the case, resulting in an altering of the error as we
propagate backwards through the network. This could lead to a big error in the
deeper layers not reaching the shallow layers of the network. This property of
the \gls{ReLu} activation function, does however not come without its costs.
If the learning rate of the network isn't configured correctly, a \gls{ReLu}
activated neuron might be blasted with a gradient so large, that it never
reaches a point of activation again. In other words, the neuron "dies". As such,
one can risk a network containing a lot of dead non-activation neurons, thus
greatly decreasing its quality. On the other hand the sigmoid function, doesn't
allow its neurons to die. It can become victim to saturation. In the case of
weight being too small or too high, the output values will be placed at the far
ends of the sigmoid range of values. At this point the gradient is incredibly
small, meaning that the contribution that neuron now has is negligible. This
neuron is now only a strain on the network, slowing it down through its
activation, but contribution nothing, a problem \gls{ReLu} does not have. Its
based on these considerations we chose the \gls{ReLu} activation function,
leaving us the task of properly selecting our learning rate.\cite{JiYan,
AndrejKarpathy, AvinashSharmaV}

As the activation function of our output neurons we have generally
used the softmax function. The softmax function is shown in Figure
\ref{fig:softmax_activation}. The function is defined as

\begin{equation}
    h(x_i) = \frac{e^{x_i}}{\sum_{k=1}^n e^{x_k}}, \text{for $i = 1 \dots n$}.
\end{equation}

The softmax function takes any vector $x \in \mathbb{R}^n$ and returns a
vector $y \in (0, 1)^n$. Where the sum of the output vectors elements will
be equal to 1. The function is therefore great at constructing a probability
distribution based on an input vector. Each individual value in the vector get
a high probability if the value is high and a low probability if the value is
low. Therefore the function is often used at the end of networks to get the
probability of each class in a classification problem.

\begin{figure}
    \centering
    \textbf{Softmax Activation Function}\par\medskip
    \includegraphics[width=0.5\textwidth]{./pictures/method/softmax_function.png}
    \caption{The softmax activation function.}
    \label{fig:softmax_activation}
\end{figure}

%TODO Move Theta and Hard Sigmoid

\subsubsection{Layers} \label{subsubsec:layers}

\gls{NN}'s are organized in layers. The first layer is called the input layer
and is connected directly to the input to the model and the last layer is called
the output layer and gives the output of the model. All layers in between are
called hidden layers. The input layer could for example be a layer of neurons
where each neuron is connected to a pixel in an input image. And the output
layer could consist of a single neuron that computes the probability that
the picture contained a cat. An example layered \gls{NN} are shown in Figure
\ref{fig:example_nn}.

\begin{figure}
    \centering
    \textbf{Example Neural Network}\par\medskip
    \includegraphics[width=\textwidth]{./pictures/method/example_neural_network.png}
    \caption{Example neural network that illustrates how neurons are organized
        into different layers with a special input layer and a special output
        layer. The neurons in the input layer are connected to individual pixels
        in an input image and the output layer is a single neuron computing the
        probability that the image contains a cat.}
    \label{fig:example_nn}
\end{figure}

There are different kinds of layers. Below is a non-exhaustive description of
different layers.

\begin{description}

    \item[Dense Layer:]

        In a dense layer the input of each neuron in the layer is connected to
        the output of every neuron in the previous layer. Each neuron in the
        layer computes a weighted sum of the outputs of the previous layers'
        neurons and applies an activation function. The weights for each of
        the neurons in the layer are different allowing each neuron to learn a
        different combination of the previous layer. An example of this can be
        seen in Figure \ref{fig:example_nn}, where the hidden layers are Dense.

    \item[Convolutional Layer:]

        A convolutional layer is mainly used to extract position independent
        features from data. The convolution consist of a sliding window that
        slides over some input data and gives an output for each possible
        position in the input image. The sliding window uses the same weights
        in all the input and will therefore extract the same features from
        different locations if it is presented with the same input. The
        convolution computes a single output for each window position. The
        output is computed as the sum of the elementwise multiplication of
        the sliding window and the current part of the input it is looking at
        \cite{oshea2015}. Let us for example consider a two dimensional input,

        \begin{equation}
            I = \begin{pmatrix}
                1 & 1 & 1 & 0 \\
                1 & 0 & 0 & 1 \\
                0 & 1 & 0 & 1 \\
                0 & 0 & 1 & 0
            \end{pmatrix},
        \end{equation}

        and the convolutional filter,

        \begin{equation}
            w = \begin{pmatrix}
                1 & 0 & 0 \\
                1 & 0 & 0 \\
                0 & 1 & 0 \\
            \end{pmatrix}.
        \end{equation}

        Then we start the sliding window in the top left corner and compute the
        elementwise product of the matrices,

        \begin{equation}
            X = I_{1-3,1-3} \circ w =
            \begin{pmatrix}
                1 & 1 & 1 \\
                1 & 0 & 0 \\
                0 & 1 & 0
            \end{pmatrix} \circ
            \begin{pmatrix}
                1 & 0 & 0 \\
                1 & 0 & 0 \\
                0 & 1 & 0
            \end{pmatrix} =
            \begin{pmatrix}
                1 & 0 & 0 \\
                1 & 0 & 0 \\
                0 & 1 & 0
            \end{pmatrix},
        \end{equation}

        to then compute the final value we take the sum of all the elements in
        that matrix,

        \begin{equation}
            \sum_{i,j} X_{i,j} = 1 + 0 + 0 + 1 + 0 + 0 + 0 + 1 + 0 = 3.
        \end{equation}

        We then slide the window one to the right and repeat the same process,

        \begin{equation}
            X = I_{2-4,1-3} \circ w =
            \begin{pmatrix}
                1 & 1 & 0 \\
                0 & 0 & 1 \\
                1 & 0 & 1
            \end{pmatrix} \circ
            \begin{pmatrix}
                1 & 0 & 0 \\
                1 & 0 & 0 \\
                0 & 1 & 0
            \end{pmatrix} =
            \begin{pmatrix}
                1 & 0 & 0 \\
                0 & 0 & 0 \\
                0 & 0 & 0
            \end{pmatrix},
        \end{equation}

        and the final value,

        \begin{equation}
            \sum_{i,j} X_{i,j} = 1.
        \end{equation}

        We keep sliding the window right until we run out of input. After that
        we go back to the left and go one row down. We continue the process for
        all the input and we end up with the matrix,

        \begin{equation}
            O = \begin{pmatrix}
                \sum_{i,j} \left( I_{1-3,1-3} \right)_{i,j} &
                \sum_{i,j} \left( I_{2-4,1-3} \right)_{i,j} \\
                \sum_{i,j} \left( I_{1-3,2-3} \right)_{i,j} &
                \sum_{i,j} \left( I_{2-4,2-3} \right)_{i,j}
            \end{pmatrix} = \begin{pmatrix}
                3 & 1 \\
                1 & 2
            \end{pmatrix}.
        \end{equation}

        A vizualization of this process can be seen in Figure \ref{fig:ConvGen}.

        \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{./pictures/method/ConvGeneral.png}
        \caption{An example of 2D convolution, using a 2x2 kernel/sliding window
            size. It also uses a step size of 1, and consideres window placements
            confined to the dimensionality of the data it slides over.
            \cite{Goodfellow-et-al-2016}}
        \label{fig:ConvGen}
        \end{figure}

        It can be seen that the output size is smaller than the input size. To
        prevent that the input can be padded with some value. A normal choice is
        zero padding. For the above input that would result in,

        \begin{equation}
            \begin{pmatrix}
                1 & 1 & 1 & 0 \\
                1 & 0 & 0 & 1 \\
                0 & 1 & 0 & 1 \\
                0 & 0 & 1 & 0
            \end{pmatrix} \xrightarrow{\text{zero padding}}
            \begin{pmatrix}
                0 & 0 & 0 & 0 & 0 & 0 \\
                0 & 1 & 1 & 1 & 0 & 0 \\
                0 & 1 & 0 & 0 & 1 & 0 \\
                0 & 0 & 1 & 0 & 1 & 0 \\
                0 & 0 & 0 & 1 & 0 & 0 \\
                0 & 0 & 0 & 0 & 0 & 0
            \end{pmatrix}.
        \end{equation}

        The sliding length can be different than one and is usually referred
        to as stride. The weights the convolutional window uses are learnable
        and is updated via gradient descent. Certain convolutional filters can
        be used for edge detection and blurring the input. Some examples of
        different convolutional kernels applied to a grayscale image are shown
        in Figure \ref{fig:convolution_example}.

        \begin{figure}
            \centering
            \textbf{Examples of different convolutional kernels}\par\medskip
            \begin{tabular}{ccc}
                \textbf{Original} & \textbf{Kernel} & \textbf{Result} \\
                \includegraphics[width=0.3\textwidth]{./pictures/method/original_convolution.png} &
                \raisebox{1.2\height}{
                \begin{minipage}[b]{6cm}
                    \begin{equation*}
                        \begin{pmatrix}
                            1 & 0 & -1 \\
                            0 & 0 & 0  \\
                            -1 & 0 & 1
                        \end{pmatrix}
                    \end{equation*}
                \end{minipage}}
                    &
                \includegraphics[width=0.3\textwidth]{./pictures/method/edge_detect_convolution.png} \\

                \includegraphics[width=0.3\textwidth]{./pictures/method/original_convolution.png} &
                \raisebox{1.2\height}{
                \begin{minipage}{6cm}
                    \begin{equation*}
                        \frac{1}{16}\begin{pmatrix}
                            1 & 2 & 1 \\
                            2 & 4 & 2  \\
                            1 & 2 & 1
                        \end{pmatrix}
                    \end{equation*}
                \end{minipage}}
                    &
                \includegraphics[width=0.3\textwidth]{./pictures/method/blurred_convolution.png} \\

                \includegraphics[width=0.3\textwidth]{./pictures/method/original_convolution.png} &
                \raisebox{1.2\height}{
                \begin{minipage}{6cm}
                    \begin{equation*}
                        \begin{pmatrix}
                            0  & -1 & 0  \\
                            -1 & 5  & -1 \\
                            0  & -1 & 0
                        \end{pmatrix}
                    \end{equation*}
                \end{minipage}}
                    &
                \includegraphics[width=0.3\textwidth]{./pictures/method/sharpened_convolution.png}
            \end{tabular}
            \caption{Examples of convolutional kernels applied to an image. The
                first kernel is an edge detect kernel, the second a blurring
                kernel and the third a sharpening kernel.}
            \label{fig:convolution_example}
        \end{figure}

    \item[\gls{RNN} Layer:]

        An RNN network is a normal feed forward neural network except that
        it allows circular connections \cite{DBLP:series/sci/2012-385}. The
        circular connections can be used to remember previous inputs and an
        \gls{RNN} therefore has a sense of history of previous input and output.
        Classical feed forward neural networks can be viewed as mappings from
        and to vectors while \gls{RNN}'s can be viewed as mappings from and to
        sequences. An \gls{RNN} does not view its input as a vector but as a
        sequence of inputs in different timesteps. We have shown an example of
        an \gls{RNN} network in Figure \ref{fig:rnn_illustration}.

        \begin{figure}
            \centering
            \textbf{Illustration of an \gls{RNN} Layer}\par\medskip
            \includegraphics[width=\textwidth]{./pictures/method/RNN_figure.png}
            \caption{Illustrates the structure of an \gls{RNN}. The \gls{RNN}
                consist of three layers, an input layer, a hidden layer and an
                output layer. The \gls{RNN} shown consist of $I$ input units,
                $H$ hidden units and $K$ output units. In the hidden layer each
                neuron is connected to every other neuron in the layer including
                itself.}
            \label{fig:rnn_illustration}
        \end{figure}

        In the forward pass of an \gls{RNN} network both the activation of the
        hidden units and the activation of the output units has to be computed.
        In the computation of the activation of the hidden units the activation
        of the hidden units in the previous timestep are required. We let
        $a^{(t)}_h$ denote the activation of neuron $h$ in the hidden layer in
        timestep $t$. That is computed as,

        \begin{equation}
            a^{(t)}_h = \psi_h\left(
                \sum_{j=1}^I w_{hj} x^{(t)}_i +
                \sum_{h'=1}^H w_{hh'} a^{(t-1)}_{h'}
            \right),
        \end{equation}

        where $\psi_h$ is the activation function of the hidden unit, $I$ is the
        number of input units, $H$ is the number of hidden units and $w_{ij}$ is
        the weight between neuron $i$ and $j$'th input for that neuron $i$. That
        is the activation of the hidden units is an activation function applied
        to a weighted sum of the current inputs and the activation of all hidden
        units in the previous timestep \cite{DBLP:series/sci/2012-385}. The
        output of the \gls{RNN} is then computed as,

        \begin{equation}
            a^{(t)}_k = \sum_{h=1}^H w_{hk} a_{h}^{(t)}.
        \end{equation}

        That is the output of an \gls{RNN} layer is a weighted sum of its
        hidden units.The complete sequence of hidden activations are computed
        by starting at time $t=1$ and recursively calling the functions above
        until the end of the sequence is reached incrementing $t$ by one in
        each recursive call. The initial values of the hidden units can be
        any initialization value. The obvious choice is 0 however better
        results have been found using non zero initial hidden unit values
        \cite{DBLP:series/sci/2012-385}.

        An \gls{RNN} network are normally optimized using the backpropagation
        through time algorithm. For data where a sense of time does not make
        sense and where the context from both sides of each timesteps is usefull
        such as for text it is normal to use bidirectional \gls{RNN}'s. Then
        both a forwards and a backwards pass is made through the sequence. The
        output of each network at each time step can then depend both on the
        context before and after the input.

    \item[\gls{LSTM} Layer:]

        As described the main benefit of \gls{RNN}'s are their ability to use
        previous context to make predictions. Unfortunately the \gls{RNN}
        architecture described above in practise does not allow context from
        far away to influence current output. The problem is known as the
        vanishing/exploding gradient problem. When backpropagating through an
        \gls{RNN} network each timestep corresponds to a separate layer in a
        normal feed forward neural network. So for sequences of several thousand
        timesteps backpropagation has to go through several thousand layers.
        The magnitude of the gradient will at each layer either increase up or
        decrease and through the hundreds or thousands of layers that leads to
        the gradient either blowing up exponentially or vanishing to nothing.
        In practise the main problem is the vanishing gradient and not the
        exploding gradient. The vanishing gradient means that weights early
        in the network are not updated according to the final output since
        the gradient has disappeared while backpropagating back to the early
        weights. Therefore the network will not learn to use context over long
        periods of time but only to use the local context around a particular
        timestep.

        There are several solutions to that problem and one of them are
        \gls{LSTM} networks. \gls{LSTM} networks are designed to be able to
        remember things over long periods of time, hence the name. They have
        several \gls{RNN} constructs with specific purposes. They have an
        \gls{RNN} responsible for computing the output given the current input
        and the previous activation as described before. They have an \gls{RNN}
        responsible for deciding what part of the input to ignore. They have
        an \gls{RNN} responsible for deciding what to forget from the current
        memory. And they have an \gls{RNN} responsible for deciding what part of
        the output to select as the current output of the unit. We have showed
        the structure of an \gls{LSTM} in Figure \ref{fig:lstm}.

        \begin{figure}
            \centering
            \textbf{Structure of an \gls{LSTM}}\par\medskip
            \includegraphics[scale=0.5]{./pictures/method/LSTM.png}
            \caption{The structure of an \gls{LSTM} network. $\bigoplus$ means
                an elementwise addition and $\bigotimes$ means an elementwise
                multiplication. Dotted lines show the movement of activations in
                the previous timestep while full lines show movement of the
                input in the current timestep.}
            \label{fig:lstm}
        \end{figure}

        The input to the \gls{LSTM} is fed into 4 \gls{RNN} networks that also
        receives the output from the previous timesteps. From bottom to top in
        the Figure the networks are.

        First a network combining the current input and the previous output into
        an activation in the current timestep.

        Second a network combining the current input and the previous output to
        compute which part of the activation of the first network to keep. The
        output of the second network is combined with the first network with an
        elementwise multiplication. That means that if the second network output
        a small value in a particular part of the output vector then that value
        will disappear from the output of the first network. The \gls{LSTM}
        therefore learns which part of the output to ignore.

        Third a network combining the current input and the previous output
        to decide what to forget. The output of the network is combined with
        the current memory with an elementwise multiplication. So again if the
        network outputs a small value it can choose to forget a certain item.
        The network will learn when to throw things out of the current memory
        and when to keep them. The output of the elementwise multiplication is
        then combined with the current output via a elementwise addition. That
        allows the current memory to influence the output of the network. The
        added output and memory are transferred through an activation function
        that makes sure that nothing blows up and we get numerical instability.

        Fourth a network combining the current input and the previous output
        to choose which values of the current output to select as the actual
        output. The output of the fourth network will be combined with the
        current model with an elementwise multiplication. So again the network
        can select which values to keep by outputting large numbers and which
        values to throw away by outputting small numbers. This network is the
        last applied to the output and can therefore select which values should
        be output.

        % TODO: cite http://www.bioinf.jku.at/publications/older/2604.pdf
        % Which is the original article for LSTM's.
        % Also cite http://www.cs.toronto.edu/~graves/preprint.pdf.

    \item[Embedding:]

        The Embedding layer is a layer that maps a value into a continuous
        vector space. Given a sequence of integers it uses the weights
        associated with the layer, to map that singular value to a vector of
        predetermined size. This can be done on any sort of data, as longs as it
        is encoded as a sequence of integers. In \gls{NLP} circumstances, this
        can be applied to characters or words for example, where each element in
        the sequence is first encoded as an integer value, and then fed to the
        embedding layer, which maps it to the continuous vector space. The hope
        with using a layer such as this, that characters for example, get mapped
        to a point close to other similar characters. In addition it also adds
        another trainable layer to optimize on.

        \begin{figure}
            \centering
            \textbf{Example Character Embedding}\par\medskip
            \includegraphics[width=\textwidth]{./pictures/method/example_character_embeddings.png}
            \caption{Character embeddings learned by a neural network. The
                embeddings were originally in 5 dimensional vectorspace and what
                is shown here is the first two principal components. Vowels are
                shown in blue and consonants in green.}
            \label{fig:embeddings}
        \end{figure}

        In Figure \ref{fig:embeddings} we have shown an embedding one of our
        networks produces. The characters are embedded in 5 dimensions so the
        plot shows the first two principal components only. We can see that
        several lower and upper case letters have ended up close to each other.
        That means that the network has learned that those characters are
        interchangeable.

        Embedding layers are widely used in \gls{NLP}. They are mainly used
        to embed words in some vector space. \cite{mikolov2013linguistic}
        found that embedding layers are able to learn more that just word
        similarities. They for example found that $vec("king") - vec("man") +
        vec("woman") \approx vec("queen")$ that is the embeddings learned the
        relation between the words king and queen and not just that they are
        similar words.

    \item[Pooling Layer:]

        The purpose of a pooling layer, or down-sampling layer, is as the name
        suggest, to pool the data it receives. It does so with the goal of
        reducing the data down to a number a number of key features found within
        that same data. This of course decreases the computation time of the
        network as a whole, and with a smaller amount of parameters, comes a
        smaller chance of over-fitting.

        Working in similar fashion as the convolutional layer, the max pooling
        layer it too uses a sliding window. This sliding window of a certain
        dimensionality, is moved over the data given to it. The max value inside
        that window is then extracted, and determined to be the output for that
        specific placement of the window. The window then moved a specified
        stride size, and the process is repeated. The result is a data-set which
        is down-sampled in proportion with the sliding windows dimensions, and
        the stride. An example of this max pooling process can be seen in Figure
        \ref{fig:max_pool}.

        Pooling layers are however, not restricted to only the Max Pooling
        layer. The average pooling layers, works in a very similar fashion, but
        instead of extracting the max value, it simply averages values currently
        in the window. The L2-Norm Pooling computes the L2 norm of the contents
        of the sliding window. The Global Max Pooling Layer, which we use quite
        frequently in our networks does not use a window like the other ones
        described, but instead simply extracts the maximum value across the
        data-samples the layer is provided.

    \begin{figure}
    \centering
    \begin{equation}
        \begin{tabular}{|llll|}
        \hline
        1 & 8 & 7 & 2 \\
        9 & 7 & 9 & 8 \\
        3 & 6 & 2 & 4 \\
        5 & 7 & 9 & 9 \\\hline
        \end{tabular}
            \Longrightarrow
        \begin{tabular}{|ll|ll|}
        \hline
        \cc{blue}1 & \cc{blue}8 & \cc{red}7 & \cc{red}2 \\
        \cc{blue}9 & \cc{blue}7 & \cc{red}3 & \cc{red}8 \\ \hline
        \cc{orange}3 & \cc{orange}6 & \cc{green}2 & \cc{green}4 \\
        \cc{orange}5 & \cc{orange}7 & \cc{green}9 & \cc{green}9\\
        \hline
        \end{tabular}
            \Longrightarrow
        \begin{tabular}{|l|l|}
        \hline
        \cc{blue}9 & \cc{red}8\\\hline
        \cc{orange}7 & \cc{green}9\\
        \hline
        \end{tabular}
    \end{equation}
    \caption{An example of a max pooling performing using a 2x2 kernel, and a
    size of 2}
    \label{fig:max_pool}
    \end{figure}


    \item[Dropout Layer:]

        This layer is used for the intent of regularizing and in turn,
        generalize a network. Given enough training time, and a good optimizer
        a network will slowly fit more and more on the training data it is
        provided. This might seem like a bad thing at first, but if the
        validation error follows the training error as the network get more
        and more fit on the training data, then the network has successfully
        learning some general commonality that can be used to describe both the
        data sets, which is the goal. It is when these two error don't follow
        on another, that one has to be alarmed, as network doesn't generalize
        properly. This is where the Dropout layer is useful. By deactivating a
        certain amount of randomly selected neurons in a particular layer, the
        network is forced to learn from a larger sparse set of neurons, rather
        than focusing on a small amount of very informative one, resulting in
        a potential better understanding of the data as a whole, rather than
        small amount of key features. An example of this can be seen in Figure
        \ref{fig:dropout}.

        \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{./pictures/method/Dropout.png}
        \caption{An example of the application of a dropouts being performed
            on different layers}
        \label{fig:dropout}
        \end{figure}

        \cite{JMLR:v15:srivastava14a} investigated the use of dropout layers
        in different problem settings. They found that dropout layers reduced
        overfitting in all problems they looked at. In particular they found
        that document classification which is similar to what we are doing were
        also improved. The main drawback of a dropout layer is that it increase
        the run time of training the networks (\cite{JMLR:v15:srivastava14a}).

\end{description}


\subsubsection{Back-Propagation}\label{sec:BP}

The neural network learns by updating the weights in the network. The weights
are updated using \gls{BP}. The process can be split into three distinct steps
which are continuously repeated:

\begin{enumerate}
    \item Feed Forward
    \item Back Propagate
    \item Update Weights
\end{enumerate}

The goal of these three steps is to minimize the overall cost of the network.
The cost refers to the loss we end up with. In order to do so, we take look at
the gradient. The gradient is a multi-variable generalization of the derivative
of a function. It can be used for describing the slope of a function at a
certain value. Not only that, but its most desired property for our purposes is
the fact that contrary to the derivative it a vector, and this vector points
to the direction of greatest increase in value for the function at a specific
point. As such, we can apply Gradient Descent to our network, which is simply
the act of changing our loss function arguments, so we step in the negative
direction of the gradient, which would be the direction of greatest decrease
in value, that is, it point towards the local minimum of the error function. A
model illustrating this process can be seen in Figure \ref{fig:grad}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{./pictures/method/GradientDesc.png}
    \caption{An illustration of how Gradient Descent works. Each iteration steps
        in the direction of the global minimum of the specific function used,
        which in this case is signified by a black ring. The Figure is generated
        using code from
        \url{https://scipython.com/blog/visualizing-the-gradient-descent-method/}.}
    \label{fig:grad}
\end{figure}

When doing this, many error functions can be of interest, depending on the
problem. For examples sake we make use of the following.

\begin{align}
E(W) &= \sum_{n=1}^N E_n(W)\\
E_n(W) &= \frac{1}{2}(\hat{y}_n - y_n)^2
\end{align}

Where N is the number of training samples, $\hat{y}_n$ is the predicted results,
and $y_n$ is the target, and W is a set of weights. The goal is minimize this
function. In order to do so, we need to have a independent variable we can
tweak using Gradient Descent as per the above description. In the case of
neural networks we can view this variable as being the weights and the biases,
which are defined used as described in Section \ref{sec:neurons}. In other
words we want to update each weight vector by doing the following operation
repeatedly.

\begin{equation}\label{eq:update}
    \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} + \Delta \mathbf{w}^{(t)}
\end{equation}

Where $\mathbf{w}$ is a singular weight value, and t refers to a specific step,
in the chain of steps performed by Gradient Descent.

\begin{equation}
    \Delta \mathbf{w}^{(t)} = -\nabla E|_{\mathbf{w}^{(t)}}
\end{equation}

$\nabla E|_{\mathbf{w}^{(t)}}$ refers to the gradient of the
error function relative to that specific weight vector, or rewritten.

$$
\nabla E|_{\mathbf{w}^{(t)}} = \frac{\partial E}{\partial \mathbf{w}^{(t)}}
$$

At a later point in Section \ref{sec:optimizers} something called the learning
rate will be introduced to this equality, but for now for the specifics of
Back-Propagation, we can omit it for now. This is the function we attempting to
solve, not for one weight, but for all of them. Not only that, we also want to
do this for every training sample, averaging $\Delta \mathbf{w}^{(t)}$ for over all
of them. As such, the goal is clear, we want to find the gradient of our error
function with respect to all the weights in our network.

After performing step 1(Feed forward), which entails running a training sample
through the network, we want to determine by how much we have to tweak each
weights using Gradient Descent. Due to the hidden nature of the entire network,
the natural starting point for this process is at the output node. If we for
simplicity's sake focus on only a single training sample, we can compute the
loss as follows

$$
E = \frac{1}{2}(\hat{y} - y)^2
$$

As mentioned we want to determine the gradient of this with respect to all the
weights $w_{ij} \in W$ in our network. In order to do that, we need to find
partial derivatives for those same weights, and since each specific weight is
tied to a specific neuron, we need to differentiate the entire network. But
before doing so, we need to establish the chain rule.

\begin{lemma}[Chain Rule]
\label{lemma:chainrule}

    If functions $f$ and $g$ er both differentiable and $F$ is the composite
    function defined by $F(x) = f(g(x))$, then $F' = f'(g(x)) \cdot g'(x)$,

\end{lemma}

The reason behind the need of the chain rule is that the loss function is
essentially a giant chain of function call, that spans the entire network. This
fact becomes apparent attempt to evaluate our single sample error function $E$
with respect to a weight $w_{ij}$. If we keep in mind that the neurons in feed
forward neural networks, computes weighted sum of its inputs as per $a_i$ in
\eqref{eq:neuron}, then we known that $E$ only depends on $w_{ij}$ through the
call of $a_i$, and it is for that reason what we can apply the chain rule to get
the following:

\begin{equation}
\frac{\partial E}{\partial \mathbf{w}_{ij}} = \frac{\partial E}{\partial a_i}
\frac{\partial a_i}{\partial \mathbf{w}_{ij}}
\end{equation}

We can add a little extra notation, which we will use hence forth, for the
sake of easing understanding.

\begin{equation}\label{eq:delta}
\delta_i = \frac{\partial E}{\partial a_i}
\end{equation}

$\delta$ is often referred to as the \textit{error} of a specific neuron.
From \eqref{eq:neuron} we get that

\begin{align}
\frac{\partial a_i}{\partial \mathbf{w}_{ij}} &= \frac{\partial}{\partial \mathbf{w}_{ij}}
\sum_{j=1} \mathbf{w}_{ij} x_j + \mathbf{w}_{i0}\\
&= \frac{\partial}{\partial \mathbf{w}_{ij}} \sum_{j = 0} \mathbf{w}_{ij} x_j\\
&= x_j
\end{align}

Which when combined gives us

\begin{equation}
\label{eq:deriv}
\frac{\partial E}{\partial \mathbf{w}_{ij}} = \delta_i x_j
\end{equation}

Revealing the fact that the derivative of the cost function with respect to the
weight $\mathbf{w}_{ij}$ is simply the value of $\delta$ for the neuron at the
output end of that specific end, and $x$ for the input end of the weight for
that same neuron. This leaves us with calculating $\delta_i$ for the hidden
units, and the output units of the network. We already know that for output
units, $\delta$ is computes as follows:

\begin{equation}
\label{eq:output}
\delta_k = \hat{y}_k - y_k
\end{equation}

Where \textit{k}, refers to a specifc output neuron.
However, for the hidden units, the chain rule has to be used again.

\begin{equation}
\label{eq:bp}
\delta_i = \frac{\partial E}{\partial a_i} =
\sum_k \frac{\partial E}{\partial a_k} \frac{\partial a_k}{\partial a_i}
\end{equation}

Where the sum runs over all \textit{k} units that unit \textit{i}, sends
connections.
Where in this case, \textit{k} denotes the units that, unit \textit{i} sends
its values.

This can be rewritten using the prior established equalities and the chain rule,


\begin{align}
\sum_k \frac{\partial E}{\partial a_k} \frac{\partial a_k}{\partial a_i} &=
\sum_k \delta_k \frac{\partial a_k}{\partial a_i} & \text{By \eqref{eq:delta}}\\
&= \sum_k \delta_k \frac{\partial a_k}{\partial z_i} \frac{\partial z_i}{\partial a_i}
& \text{By \eqref{eq:neuron} and Lemma \ref{lemma:chainrule}} \\
&= \sum_k \delta_k h'(a_i) \frac{\partial a_k}{\partial z_i} \\
&= \sum_k \delta_k h'(a_i) \left( \frac{\partial}{\partial z_i}  \sum_{i=0} \mathbf{w}_{ki} z{i}\right) \\
&= \sum_k \delta_k h'(a_i) \mathbf{w}_{ki} \\
\end{align}

This highlights the fact that changes in $a_i$ only influences
the error-function, through variations of the variables $a_k$. The leaves us
with the fact that we can compute the $\delta$ for a particular hidden unit by
backpropagating the $\delta$ value back from higher up in the network.

As such, we can summarize the backpropagation algorithm like this.

\begin{enumerate}
    \item

        Provide the network with some input data $x_n$, and compute the
        activations of each hidden and output neuron using Equation
        \eqref{eq:neuron}.

    \item

        Compute $\delta_n$ for each of the output neurons using Equation
        \eqref{eq:output}

    \item

        Back-Propagateprop $\delta$ using Equation \eqref{eq:bp}, to compute
        $\delta_i$ for each neuron

    \item

        Use Equation \eqref{eq:deriv} to evaluate the required derivatives

\end{enumerate}

At this point we can update each of the weights using point 4, and Equation
\eqref{eq:update} which is the Step 3.

These three steps Feed-Forward, Back-Propagate and update weights are repeated
throughout the training cycle of the neural networks, so as to optimize the
weights after each full iteration on the training set. As is to be expected
this algorithm as very expensive computationally, as it runs in $O(W)$. This is
only for a single training sample though. Thus run-time becomes $O(N\cdot W)$,
where $N$ is the number of training samples. Additionally, if one trains over
several epochs, this run-time becomes even more inflated $O(\mathcal{E}\cdot
N\cdot W)$ iteration having do be computed, where $\mathcal{E}$ is the number
of epochs run during training. For that reason this run-time might not seem
very disable at first, and is indeed the reason why neural networks need such
a long training time. However an alternative described by \cite{Bishop}, which
involves perturbing each weight instead, and approximating the derivatives.
This method does however run in $O(W^2)$ time, making the run-time of $O(W)$
back-propagation provides the desired choice. However, in order to improve
on those that $O(W)$ time-complexity a method called batching is used. This
method, shuffles and split the training dataset up into smaller batches.
These batches are then run through the network one at a time, performing
back-propagation after each run-through. This results in the gradient not
being as accurate after each batch, but this introduced inaccuracy is
negligible compared to the computational time won, by reducing the amount of
$\mathcal{E}$ in $O(\mathcal{E}\cdot N\cdot W)$ needed for the same amount of
convergence.\cite{Bishop}


\subsubsection{Optimizers}\label{sec:optimizers}

Optimizations of the Gradient Descent algorithm in the training of a neural
network are referred to as an optimizer function. The optimizer functions
responsibility is to minimize the error function $E$ as fast as possible
without overstepping the "valleys" in the function. The easiest way to use
gradient information for weight updating is to take a small step in the opposite
direction of the gradient since we might overstep the valley if we take to large
a step. To take a small step we introduce a parameter $\eta > 0$ that scales the
magnitude of the gradient \cite{Bishop}. The parameter is often referred to as
the learning rate. The algorithm for weight update becomes,

\begin{equation}
    \mathbf{w}^{(t+1)} =
        \mathbf{\mathbf{w}}^{(t)} -
        \eta\Delta E|_{\mathbf{w}^{(t)}}.
\end{equation}

In that algorithm a single updating of the weights require computing the
gradient of the error function on each training sample. For large datasets
that might not be feasible as backpropagation is expensive to compute for such
datasets. Stochastic Gradient Descend is an attempt at estimating the gradient
of all datapoints using only a single datapoint. That means that in Stochastic
Gradient Descend weights are updated much more often and is updated on a noisy
target \cite{Bishop}. The advantage of Stochastic Gradient Descend is that
convergence to a local minimum might be much quicker since the algorithm doesn't
have to go through all training samples for each weight update. The weight
updating for Stochastic Gradient Descend is computed as,

\begin{equation}
    \mathbf{w}^{(t+1)}_i =
        \mathbf{\mathbf{w}}^{(t)}_i -
        \eta\Delta E_n|_{\mathbf{w}^{(t)}_i}.
\end{equation}

In practise each weight update in Stochastic Gradient Descend are computed
not based on a single training sample but on some small number of training
samples. That allows the computation to take advantage of highly optimized
matrix libraries and making each update on a slightly less noisy target
\footnote{\url{http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochas
ticGradientDescent/}}. The version of Stochastic Gradient Descend that use more
than one sample per weight update is known as Stochastic Minibatch Gradient
Descend.

To see how Stochastic Gradient Descend might lead to faster convergence consider
a dataset with some number of training samples. To create redundancy in the
dataset we copy all training samples in the dataset to create a new dataset of
twice the size. In classical Gradient Descend the only effect will be that the
error increases by a magnitude of 2 but will take twice as long to compute.
While Stochastic Gradient Descend will converge at the exact same speed as each
minibatch will still represent an estimate of the true underlying function and
each weight update still takes the same amount of time \cite{Bishop}.

Another advantage of the Stochastic method is that the optimizer might escape
from a non optimal local minima and reach a better local minima. To see why that
might happen consider the fact that each minibatch consist of a different sample
of training samples. Even though when computed on all training samples the
gradient points towards the non-optimal local minima the gradient of a single
batch might point towards a better local minima. Therefore the optimizer has the
possibility of following the gradient of single batches that leads to a final
better local minima \cite{Bishop}.

A problem in the Stochastic Gradient Descend algorithm is the need to choose
the learning rate $\eta$. If the learning rate is too high the algorithm
will diverge and if it is to low it will be very slow to converge. Therefore
several extensions to basic Stochastic Gradient Descend has been developed that
automatically tunes the learning rate $\eta$. We will discuss three different
extension \gls{AdaGrad}, \gls{RMSProp} and \gls{Adam}.

In the description of those three algorithms we will slightly abuse vector
notation. We will keep using $\mathbf{a} \circ \mathbf{b}$ to mean an
elementwise multiplication but besides that we use
$\frac{\mathbf{a}}{\mathbf{b}}$ to mean an elementwise division,
$\sqrt{\mathbf{a}}$ to mean an elementwise square root and $\mathbf{a}^n$ to
mean an elementwise power.

\begin{description}

    \item[\gls{AdaGrad}:]

        The algorithm was invented by \cite{Duchi:2011:ASM:1953048.2021068}.
        The algorithm attempts to solve the problem of choosing a learning
        rate and the problem of how to handle infrequent features. Infrequent
        features are often the most important features for a classification task
        but since they are infrequent they are hard to learn on. \gls{AdaGrad}
        assigns a separate learning rate to each separate parameter. The
        algorithm gives infrequently observed parameters a very high learning
        rate and frequently observed parameters a very low learning rate. The
        method incorporates knowledge of the data from previous iterations to
        make choices in the current iteration. In \gls{AdaGrad} the weight
        update function is,

        \begin{equation}
            \mathbf{w}^{(t+1)} =
                \mathbf{w}^{(t)} -
                \eta \text{diag}\left(G^{(t)}\right)^{-\frac{1}{2}} \circ
                \Delta E|_{\mathbf{w}^{(t)}},
        \end{equation}

        where $G^{(t)}$ is a matrix containing the sum of the outer product
        of the gradient in all previous timesteps $1, \dots, t$. $G^{(t)}$ is
        defined as,

        \begin{equation}
            G^{(t)} = \sum_{\tau=1}^t \Delta E|_{\mathbf{w}^{(\tau)}}
                \left(
                    \Delta E|_{\mathbf{w}^{(\tau)}}
                \right)^T.
        \end{equation}

        For the updating of a single weight $w_i$ the above becomes
        \cite{Duchi:2011:ASM:1953048.2021068},

        \begin{equation}
            \label{eq:individual_adagrad}
            \mathbf{w}_i^{(t+1)} =
                \mathbf{w}_i^{(t)} - \frac{\eta}{\sqrt{G^{(t)}_{i,i}}} \circ
                \Delta E|_{\mathbf{w}_i^{(t)}}.
        \end{equation}

        $G_{i,i}$ works as a scaling factor for weight $\mathbf{w}_i$.
        Since $\sqrt{G_{i,i}} = \sqrt{\sum_{\tau=1}^t \left(\Delta
        E|_{\mathbf{w}^{(\tau)}_i}\right)^2}$, $G_{i,i}$ is the L2 norm
        of the gradient in the previous timesteps. That means that if the
        gradient has generally been large for a particular weight the norm
        will be large and the updating of the weight will be scaled down
        (slower) which can be seen in Equation \eqref{eq:individual_adagrad}.
        Similarly when the gradients of a weight has been small or 0 the
        weight is scaled less down and the upgrade will be larger. That has
        the effect of having infrequent parameters of the model train faster
        \cite{Duchi:2011:ASM:1953048.2021068}.

        % TODO: Describe why AdaGrad is great for NLP.

    \item[\gls{RMSProp}:]

        \gls{RMSProp} is an unpublished optimization algorithm.
        It was introduced by Geoff Hinton in Lecture 6e of
        his Coursera Class \cite{DBLP:journals/corr/Ruder16}.
        This section is based primarily on his slides
        \footnote{\url{http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_s
        lides_lec6 .pdf}}. The idea behind \gls{RMSProp} is similar to
        \gls{AdaGrad} each weight is updated at an independent rate. Unlike
        \gls{AdaGrad} the rate is chosen by a moving average instead of the
        L2 norm of all previous steps. That allows the algorithm to adapt
        quickly to changes in the gradient. When the gradient of a weight is
        large the algorithm tunes down the learning rate and when the gradient
        is consistently small it tunes up the learning rate. That functions
        well since a large gradient is found in a steep volatile area while a
        consistently small gradient is found in a flat non volatile area. In the
        steep areas we are likely to overshoot the target if the learning rate
        is to high while in flat areas we don't have that problem. The internal
        state of the algorithm is a vector $\mathbf{v}$ of the running average
        of gradient magnitudes. It is updated with the equation,

        \begin{equation}
            \label{eq:rms_prop_state}
            \mathbf{v}^{(t+1)} =
                \gamma\mathbf{v}^{(t)} +
                (1 - \gamma)\left(
                    \Delta E|_{\mathbf{w}^{(t + 1)}} \circ
                    \Delta E|_{\mathbf{w}^{(t + 1)}}
                \right).
        \end{equation}

        The parameter $\gamma$ can be seen as a remembering rate. If $\gamma$
        is high we keep much of the previous running average and if $\gamma$
        is low we update the average much quicker. The actual weight update is
        performed by using the internal state $\mathbf{v}$ to scale the learning
        rate $\eta$,

        \begin{equation}
            \mathbf{w}^{(t+1)} =
                \mathbf{w}^{(t)} -
                \frac{\eta}{\sqrt{\mathbf{v}^{(t)}}} \circ
                \Delta E|_{\mathbf{w}^{(t)}}.
        \end{equation}

    \item[\gls{Adam}:]

        \gls{Adam} is an upgrade of the \gls{RMSProp} algorithm invented by
        \cite{DBLP:journals/corr/KingmaB14}. The difference from \gls{RMSProp}
        is that \gls{Adam} use both the gradient and the squared gradient while
        \gls{RMSProp} use only the squared gradient. Consequently \gls{Adam} has
        two remembering factors instead of the single one for \gls{RMSProp}. The
        internal state of \gls{Adam} is updated via,

        \begin{align}
            \mathbf{m}^{(t+1)} &=
                \gamma_1\mathbf{m}^{(t)} +
                (1 - \gamma_1) \Delta E|_{\mathbf{w}^{(t+1)}}, \\
            \mathbf{v}^{(t+1)} &=
                \gamma_2\mathbf{v}^{(t)} +
                (1 - \gamma_2) \left(
                    \Delta E|_{\mathbf{w}^{(t+1)}} \circ \Delta E|_{\mathbf{w}^{(t+1)}}
                \right), \\
            \mathbf{\hat{m}}^{(t+1)} &=
                \frac{\mathbf{m}^{(t+1)}}{1 - \gamma_1^{t + 1}}, \\
            \mathbf{\hat{v}}^{(t+1)} &=
                \frac{\mathbf{v}^{(t+1)}}{1 - \gamma_2^{t + 1}}.
        \end{align}

        The first two equations are very similar to Equation
        \eqref{eq:rms_prop_state} for \gls{RMSProp}. In those equations
        $\gamma_1$ and $\gamma_2$ are the decay rates of the gradient and the
        squared gradient. They keep a running average of the gradient and the
        size of the gradient. By using the running average of the gradient
        \gls{Adam} is able to move more smoothly in a direction since the
        direction chosen is no longer just the current gradient but previous
        gradients influence the choice of direction. That means that a single
        weird gradient will not drastically change the direction \gls{Adam}
        moves \cite{DBLP:journals/corr/KingmaB14}. Similarly to \gls{RMSProp}
        \gls{Adam} use the running average of the squared gradient to scale how
        large steps are taken. The two bottom equations are scaling the internal
        state based on the time step $t$. As $t$ increases the denominators
        increase meaning that the value of the whole fraction decreases. That
        means that in the beginning the step size is adjusted upwards and as $t$
        increases smaller and smaller steps are taken. The actual weight update
        in the \gls{Adam} algorithm are performed as,

        \begin{equation}
            \mathbf{w}^{(t + 1)} = \mathbf{w}^{(t)} -
                \eta\frac
                    {\mathbf{\hat{m}}^{(t+1)}}
                    {\sqrt{\mathbf{\hat{v}}^{(t+1)}} + \epsilon}.
        \end{equation}

        \gls{Adam} prevents division by 0 via the $\epsilon$ parameter which is
        just some small number. It can be seen in the above equation that the
        direction of change is chosen by $\mathbf{\hat{m}}^{(t)}$ while the size
        of the step taken is chosen by a combination of the learning rate $\eta$
        and $\mathbf{\hat{v}}^{(t)}$.

\end{description}


\subsubsection{Siamese Networks}

Classical machine learning approaches for text analysis and all our baseline
methods are based on handcrafted feature sets. Deep learning has shown
promising results in extracting features from raw images and raw text
\cite{hongxiaosunyuan}. We wanted to use deep learning to automatically learn
features from a large amount of data. At the same time we want to solve the
MaCom problem. Siamese neural networks are as described earlier networks that
compares two inputs. The networks has been used by \cite{Koch2015SiameseNN},
\cite{NIPS1993_769} and \cite{qian:2018} for comparing text, images and
signatures. \cite{qian:2018} use a Siamese network but with no convolutions and
a distance function on top for text analysis while \cite{Koch2015SiameseNN}
used a Siamese network with convolutions and fully connected network on top
for image analysis. Our approach is to use convolutions in the Siamese network
to learn important features from the texts. We also use a fully connected
network on top of the convolutional layers to learn from the features the
convolution extracted. An example of this architecture can be seen in Figure
\ref{fig:siamese_example}.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{./pictures/method/Siamese.png}
    \caption{The basic architecture of a Siamese neural network, which takes in
        two sources of data, and runs it through two parallelle networks,
        sharing their weights at each layer}
    \label{fig:siamese_example}
\end{figure}

The network will take two inputs, a text $t_k \in T_\alpha$ and a text $t_u
\in T_\beta$. The network will then try to figure out whether $\alpha =
\beta$. The convolutions in the network will look at $n$ characters at a
time and give an output. Therefore the convolutional layers will be able to
learn important n-grams and extract a representation of $t_k$ and $t_u$. This
representation will be given to some dense layers which will learn how to
compare the feature-sets extracted. The Siamese part of the network is the
convolutional part of the network which will share weights when extracting
n-grams from the two texts.

\cite{DBLP:journals/corr/0001KYS17} presented a comparative study of \gls{CNN}'s
and \gls{RNN}'s in common \gls{NLP} tasks. They found that \gls{RNN}'s generally
performed better in tasks where understanding the whole text was important while
\gls{CNN}'s performed better when understanding were secondary and it was mainly
about finding key phrases or statistical information from a text. In authorship
verification the understanding of the text is generally irrelevant since we
don't have to say anything about what a text is about but only whether or not
it is written by the same author as some other text. Furthermore classical
authorship verification methods have relied on statistical information from the
texts and not on an actual understanding of the texts. We have therefore chosen
to start out with using convolutional neural networks.

The final output of the network will be a probability that $\alpha = \beta$.
Since the MaCom dataset consist of multiple known texts per author and this
network architecture only compares two texts we define a separate system for
making the final prediction.

\subsection{Prediction System}\label{sec:prediction}

In the MaCom problem we are given a set of texts for each author so we can make
use of multiple texts when making our predictions. We can also use metadata
about the individual texts to make our prediction. An advantage of using
multiple texts for each author to make a prediction is that outliers will be
averaged out.

\begin{definition}[Prediction System]

    \label{def:prediction_system}

    Our prediction system is a function $P:f, w, T, t_{unkown}, \theta
    \rightarrow \{0, 1\}$ where $f$ is a function taking two texts and giving
    the probability that the texts are written by the same author, $T$ is a set
    of texts written by a candidate author, $t_{unknown}$ is a text of unknown
    authorship, $w:T \rightarrow [0,1]$ is a function where $\sum_{t_{known}
    \in T} w(t_{known}) = 1$ and $\theta \in [0,1]$. The prediction system then
    returns,

    \begin{equation}
        P(f, w, T, t_{unkown}, \theta) = \begin{cases}
            1 & \text{if } \sum_{t_{known} \in T} w(t_{known})
                f(t_{known}, t_{unkown}) > \theta \\
            0 & \text{otherwise}.
        \end{cases}
    \end{equation}

\end{definition}

That is the prediction system returns 1 if the weighted average of the
probability that $t_{unknown}$ is written by the same author as $t_{known}$
for each $t_{known} \in T_\alpha$ is greater than the threshold $\theta$ and 0
otherwise.

The $f$ parameter can be anything that takes two texts and returns a
probability. We are going to present several different networks that can be used
as $f$ as they will take two texts and end with a softmax layer that outputs a
probability distribution.

The $w$ parameter in the prediction system can be used to weigh the known texts
differently. We are going to try several different weight functions that weigh
texts based on metadata such as the time they were written and the length of
the texts. The only constraint on a weight function $w$ is that $\sum_{t \in
T_\alpha} w(t) = 1$. We will ignore that constraint in the following definitions
of weight functions. Any function $w: T_\alpha \rightarrow \mathbb{R}^+$
that does not follow the constraint can be transformed into a version $w^*:
T_\alpha \rightarrow [0,1]$ that does follow the constraint where $w^*(t) =
\frac{w(t)}{\sum_{t \in T_\alpha} w(t)}$. We can therefore define the weight
functions $w$ while ignoring the constraint but use $w^*$ as the actual weight
function in the prediction system.

The most obvious weighing scheme is to just use a uniform weighting. That way
we simply take an average of the predictions of our networks over the different
texts an author has written.

\begin{definition}[Uniform Weighing]

    The uniform weight function $w_\mathcal{U}: T_\alpha \rightarrow \{1\}$ is
    defined as,

    \begin{equation}
        w_\mathcal{U}(t) = 1
    \end{equation}

\end{definition}

The uniform weight function does not use any information we know about the
texts. It is mostly part of our weight functions as a baseline to make sure we
don't make any weight functions that are worse than a simple uniform weighing.
Our other weight functions makes use of metadata we know about the texts. As an
example we have a timestamp of when a text was turned in to the MaCom servers.
We therefore know when the text was written and we assume that newer texts will
better reflect the current writing style of a student than older texts. We
have therefore defined several weight functions using the times the texts was
written. Let $\tau: T_\alpha \rightarrow \mathbb{N}$ be the function returning
the number of months between when the newest text $t \in T_\alpha$ and the text
it is given were turned in where a month is considered 30 days. Our time based
weight function are then defined as,

\begin{definition}[Exponential Dropoff]

    The exponential dropoff function $w_e: T_\alpha \rightarrow [0, 1]$ is
    defined as,

    \begin{equation}
        w_e(t) = e^{-\lambda \tau(t)}
    \end{equation}

\end{definition}

The $\lambda$ parameter in that weight function can be used to control how
important the newer texts are. When $\lambda = 0$ the Exponential Dropoff is
equivalent to the Uniform Weighing and as $\lambda \rightarrow \infty$ more
weight is given to the latest text. We have shown the weights given to different
assignments for different $\lambda$ values in Figure \ref{fig:weights}.

\begin{figure}
    \centering
    \textbf{Exponential Dropoff Weights}\par\medskip
    \includegraphics[width=0.49\textwidth]{./pictures/method/weights.png}
    \includegraphics[width=0.49\textwidth]{./pictures/method/weights_normalized.png}
    \caption{Illustrate the Exponential Dropoff weight function for different
        values of $\lambda$. We apply the weight function to the numbers $0, 1,
        \dots, 35$ since a typical student will attend high school for 3 years
        (36 months). On the left the pure output of the weight function $w_e$ is
        shown and on the right the normalized weights $w_e^*$. We wary $\lambda$
        from 0 to 1 with step size 0.05.}
    \label{fig:weights}
\end{figure}

% TODO: Define a weight function that also looks at the length of the texts.
% TODO: Describe maximum weight function.
% TODO: Describe minimum weight function.
% TODO: Describe majority vote weight function.

The $\theta$ parameter in the prediction system determines when we consider an
unknown text to be written by an author. The natural choice for $\theta$ is 0.5
since that is what the network use when it computes the loss of each training
sample. However the $\theta$ parameter can be used to enforce how sure we have
to be of a decision to accuse an author of not having written an assignment. As
described earlier MaCom does not want to accuse innocent students of cheating.
That means that it is very important to MaCom to minimize the number accusation
error. We can use the $\theta$ parameter to control that error. Hopefully the
\gls{FN}s generally end up with a value closer to 1 than the \gls{TN}s. If that
is the case then lowering $\theta$ value will lower the fraction since there
will be fewer \gls{FN}s. That might lower the overall accuracy of the prediction
system but will make sure that at few students as possible are falsely accused.
