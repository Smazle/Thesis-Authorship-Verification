\section{Conclusion} \label{sec:conclusion}

We have created three deep neural networks. The networks work on different
layers of an author's texts: The character layer, the word layer and the
sentence layer. We evaluated all networks on a dataset with 96\% positive cases
and 4\% negative cases. That split reflects the currently presumed real world
data. All of our networks performed better than the baseline methods on that
test dataset. Furthermore, one of our networks \gls{conv-char-NN} performed
slightly better than previous work on MaCom's dataset that we have found.

We did not hit the targets set by MaCom at the beginning of the project. The
specifications set out by MaCom were to have an accusation error below 10\% and
still catch 95\% of the cheaters. We knew that those goals were very ambitious
and per MaCom's request we focused on fulfilling the first specification. We
ended up with an accusation error of 23\% and caught 8.5\% of the cheaters with
our best network. The fact that we used a dataset with only 4\% negatives made
it very hard to keep the accusation error low.

We looked at what kind of feedback we could provide to teachers. It is possible
for some of our networks to report the largest differences between the texts.
A teacher would then be able to use thousands of those differences to explain
why an assignment might be ghostwritten. We also performed an experiment which
showed that we can find the areas of a text most likely to be ghostwritten. The
results were not overwhelmingly positive as the difference between ghostwritten
and non ghostwritten paragraphs were small, which suggests that our networks
have problems handling such small amounts of text.

In our opinion the networks we have implemented show great potential. We also
believe that it is possible to continue working with the methods we have
explored and obtain something that is ready for real world integration. We have
used previous work on MaCom's datasets as a starting point for our own methods.
Similarly future work on the dataset might be based on the work in this thesis.

By beating our baselines, and getting slightly better results than previous work
done on Macom's data we have shown the applicability of neural networks on raw
text data. This raw text applicability is one of the great advantages of our
methods. There is no need for manual feature engineering as classic authorship
attribution and verification requires. Another big advantage is that our model
is generalizing so it can be trained once and used many times. As explained
before, classifying a single text is fast and classifying $n$ texts is very
parallelizable. The classification of a single text contains no dependencies on
any other classifications and can therefore be performed on arbitrarily many
machines.

% TODO: Maybe use AUROC scores to explain how good we are.
