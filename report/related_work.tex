\section{Related Work} \label{sec:related_work}

Our work in this thesis is inspired by the previous work of several researchers.
\cite{DBLP:journals/corr/RuderGB16c} shows a Neural Network for authorship
attribution. Authorship attribution is closely connected to authorship
verification as every authorship attribution problem can be transformed into a
series of authorship verification problems. To attribute the author of a text
you can perform a series of authorship verifications of each candidate author
and return the author that reported true. Their experiment consisted of a
network where they first had a Convolutional layer, after that a max-over-time
pooling layer and then a densely connected network on the top of that. Character
level features has previously been shown to be important for both authorship
verification and attribution (TODO: cite something). The hope was that the
convolutional layer would learn important features from sequences of characters.
The max-over-time pooling would take the most important value from each
convolutional filter and would extract a similar number of features for each
text even though the texts are of differing length. The dense network was then
supposed to take the features extracted from the text and determine authorship
of the text from them.

\cite{DBLP:journals/corr/RuderGB16c} also used multiple channels in their
network. Each channel was a different token sequence some of them were word
embeddings and some were character embeddings. Some of the channels were static
while some of the channels were non-static meaning that the word/char-embedded
vectors would change during training. The point of the channels was that the
network were able to extract features from multiple levels of features (TODO:
reference some explanation of different levels of features). Specifically they
used networks with the following channels

\begin{description}
    \item[CNN-char:] Single non-static character channel.
    \item[CNN-word:] Single non-static word channel.
    \item[CNN-word-word:] Two word channels, one non-static and one static.
    \item[CNN-word-char:] Two non-static channels one for words and one for
        characters.
    \item[CNN-word-word-char:] One static word channel, one non-static word
        channel and one non-static character channel.
\end{description}

The best performing configuration was the CNN-char.
