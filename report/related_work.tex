\section{Related Work} \label{sec:related_work}
%The problem of authorship verification

Our work in this thesis is inspired by the previous work of several researchers.
\cite{DBLP:journals/corr/RuderGB16c} shows a Neural Network for authorship
attribution. Authorship attribution is closely connected to authorship
verification as every authorship attribution problem can be transformed into a
series of authorship verification problems. To attribute the author of a text
you can perform a series of authorship verifications of each candidate author
and return the author that reported true. Their experiment consisted of a
network where they first had a Convolutional layer, after that a max-over-time
pooling layer and then a densely connected network on the top of that. Character
level features has previously been shown to be important for both authorship
verification and attribution (TODO: cite something). The hope was that the
convolutional layer would learn important features from sequences of characters.
The max-over-time pooling would take the most important value from each
convolutional filter and would extract a similar number of features for each
text even though the texts are of differing length. The dense network was then
supposed to take the features extracted from the text and determine authorship
of the text from them.

\cite{DBLP:journals/corr/RuderGB16c} also used multiple channels in their
network. Each channel was a different token sequence some of them were word
embeddings and some were character embeddings. Some of the channels were static
while some of the channels were non-static meaning that the word/char-embedded
vectors would change during training. The point of the channels was that the
network were able to extract features from multiple levels of features (TODO:
reference some explanation of different levels of features). Specifically they
used networks with the following channels

\begin{description}
    \item[CNN-char:] Single non-static character channel.
    \item[CNN-word:] Single non-static word channel.
    \item[CNN-word-word:] Two word channels, one non-static and one static.
    \item[CNN-word-char:] Two non-static channels one for words and one for
        characters.
    \item[CNN-word-word-char:] One static word channel, one non-static word
        channel and one non-static character channel.
\end{description}

The best performing configuration was the CNN-char.

The method implemented by \cite{shrestha2017}, was their attempt at \gls{AA}
on short texts. The reasoning behind the only focusing on short texts was the
advent of social media, and the great usage of E-mail. Their approach makes use
of a \gls{CNN}. This \gls{CNN} only takes in a sequence of character-n-grams.
The reasoning for this usage of only char-n-grams was the small amount of text
in each sample. By passing these N-grams through a Embedding layer, a 25\%
dropout layer, 3 convolutional layers and then using max-over-time, they get
a compact representation of the text. They hypothesize this representation
captures the morphological, lexical and syntactic level of the supplied text.
This compact representation is then parsed through a fully connected soft-max
layer, to produce a probabilistic distribution over all authors. In order to
test their method they made use of a twitter data set, containing approximately
9000 user, all having over a 1000 tweets to their name. They made use of two
different configurations of their networks. One using character-1-grams and one
using character-2-grams. After removing bot-like authors, they got an accuracy
of 0.678, and 0.683 respectively. This however, was only with 35 authors used,
and 1000 tweets per author. In the case where either the authors count was
increased or the number of tweets was decreased, the accuracy quickly worsened.
In order to extract some sort of meaning from the predictions they made using
this approach, they made use of the saliency score to determine the impact each
n-gram had on the final decision.

The approach proposed by \cite{ding2016} was somewhat more complex. While they
too made use of a \gls{CNN}, the application of this \gls{CNN} was split up into
four different modalities. They describe their approach as "Mining stylometric
representations for authorship analysis". This is done by having 4 different
\gls{CNN}s, one that focuses on 1 (or 2) stylometric levels of the text. The
points of having it split up, was so one could pick and choose which modality
one wanted to use on the specific example. The first network looks that the
word level, and topical modalities of the text. In doing so it makes use of
3 different author word biases. The topical bias describes a texts tendency
towards words that are specific to the topic of the entire text. The local
contextual bias describes the texts tendency towards specific words based on the
topic of a specific part of the text, as the overall topic is rarely consistent
throughout the text. Finally there is the lexical bias, which describes the bias
the author has towards words that have a similar meaning, such as "nice day"
vs "wonderfull day". Using these thee biases, where the local contextual bias
is derived from a numerical representation of a sequence of words, the network
predicts the word most likely to fit into the local context. The second network
looks and the character modality, and works in a somewhat similar fashion,
but on the character level. It represents each word in the text as a set of
character bigrams. After converting each of these bigrams to the numerical
representation, it is fed through a fully connected sigmoid layers, establishing
the relation between the bigrams and the word they come together to represent.
The third and last network addresses the syntactic modality. This focuses on
the \gls{POS}-tags of the text. By looking at the N neighboring \gls{POS}-tag
bigrams, the network determines the most likely \gls{POS} of that word. All
these three modality-network attempts to optimize the forward log probability of
its' prediction. After training each of these models on the texts of a specific
author, they are applied in turn to a two texts. The similarity of each of these
two texts, is then computed using a simple cosine distance. When applying this
approach to the training set consisting of english novels of 28,054 sentences,
and 705,751 tokens, and english essays of 30,038 sentences and 676,966 tokens,
and a testing set of novels containing 29,375 sequences and 653,981 tokens and
essays of 119,202 sentences and 2,781,425 tokens, they got the best results
using only the combined lexical topical network, which produced an average
accuracy of 0.7950.

\cite{qian:2018} performed a study of different deep learning methods for
authorship attribution. They used both a \gls{GRU} network and a \gls{LSTM}
network. They implement 4 different networks, sentence-level-\gls{GRU},
article-level-\gls{GRU}, article-level-\gls{LSTM} and
article-level-Siamese-network. Of those 4 networks the Siamese network is of
special interest to our project. The Siamese network solves the authorship
verification problem and not the authorship attribution problem. A Siamese
network is a network that use the same weights and parameters in multiple parts
of the network. The architecture for authorship verification chosen by
\cite{qian:2018} started by using a \gls{GRU} network on the two texts to verify
authorship on. On top of the \gls{GRU} network they used an average pool. The
output of the average pool is then seen as
