\section{Related Work} \label{sec:related_work}

In this section we describe the previously work which has served as sources and
inspiration of this thesis. All of these papers explore \gls{NLP}, authorship
attribution and authorship verification. Additionally, some of these papers has
a special focus on deep learning based approaches to \gls{NLP}. We have also
described previous experiments performed on MaCom's dataset that we have been
able to find.


\subsection{Neural Network Methods}

\citet{DBLP:journals/corr/RuderGB16c} shows a neural network for authorship
attribution. They present a network consisting of a convolutional layer followed
by a max-over-time pooling layer and then a densely connected network on the top
of that. The input for this network was a sequence of characters. The hope was
that the convolutional layer would learn important features from the sequence
of characters. The max-over-time pooling would take the most important value
from each convolutional filter and would extract a similar number of features
for each text even though the texts were of differing lengths. The dense network
was then supposed to take the features extracted from the text and determine
authorship of the text from them.

\citet{DBLP:journals/corr/RuderGB16c} also experimented with a network that
used multiple channels. Each channel was a different token sequence, some of
them were word embeddings and some were character embeddings. Some of the
channels were static while some of the channels were non-static meaning that
the word/char-embedded vectors would change during training. The point of
the multiple channels was that the network was able to extract features from
multiple linguistic layers, all describing a different stylometric feature of
text. They specifically tried the following networks:

\begin{description}
    \item[CNN-char:] Single non-static character channel.
    \item[CNN-word:] Single non-static word channel.
    \item[CNN-word-word:] Two word channels, one non-static and one static.
    \item[CNN-word-char:] Two non-static channels one for words and one for
        characters.
    \item[CNN-word-word-char:] One static word channel, one non-static word
        channel and one non-static character channel.
\end{description}

The best performing configuration was the CNN-char network.

\citet{shrestha2017} experimented with classifying the author of short texts
such as messages on social media, and emails. Their approach used a \gls{CNN}
working on the character level of the text. They used the character level due
to the small amount of text in each sample. By passing these n-grams through
an embedding layer, a dropout layer (of 25\%), 3 convolutional layers and then
using max-over-time pooling, they got a compact representation of the text.
They hypothesized this representation captures the morphological, lexical
and syntactic level of the supplied text. This compact representation was
then passed through a fully connected soft-max layer to produce a probability
distribution over all authors. In order to test their method they used a Twitter
dataset containing approximately 9000 users that had all written more than 1000
Tweets. They tried two different configurations of their networks. One using
character-1-grams and one using character-2-grams. After removing bot-like
authors they got an accuracy of 0.678, and 0.683 respectively. That was however
only with 35 authors used, and 1000 Tweets per author. In the case where either
the number of authors were increased or the number of Tweets were decreased the
accuracy fell.

% TODO This has to be rewritten if we are to include it.

%The approach proposed by \citet{ding2016} was somewhat more complex. While they
%too made use of a \gls{CNN}, the application of the network was split into
%four different modalities. They described their approach as \textit{Mining
%stylometric representations for authorship analysis}. They used four different
%\glspl{CNN}, each one focusing on 1-2 stylometric levels of the text. The point
%of splitting it up, was so they could pick and choose which modality they wanted
%to use on the specific example. The first network looked at the word level,
%and topical modalities of the text. In doing so it made use of 3 different
%author word biases. The topical bias described a text's tendency towards words
%that were specific to the topic of the entire text. The local contextual
%bias described the text's tendency towards specific words based on the topic
%of a specific part of the text, as the overall topic was rarely consistent
%throughout the text. Finally the lexical bias described the author's bias
%towards words with a similar meaning, such as "nice day" vs "wonderfull
%day". Using these thee biases, where the local contextual bias is derived
%from a numerical representation of a sequence of words, the network predicted
%the word most likely to fit into the local context. The second network looked
%at the character modality, and worked in a somewhat similar fashion, but
%on the character level. It represented each word in the text as a set of
%character bigrams. After converting each of these bigrams to the numerical
%representation, it was fed through a fully connected sigmoid layer, establishing
%the relation between the bigrams and the word they came together to represent.
%The third and last network addressed the syntactic modality. This focused on
%the \gls{POS}-tags of the text. By looking at the N neighboring \gls{POS}-tag
%bigrams, the network determined the most likely \gls{POS} of that word. All
%these three modality-networks attempted to optimize the forward log probability
%of their prediction. After training each of these models on the texts of a
%specific author, they were applied to two texts. The similarity of each of these
%two texts was then computed using a simple cosine distance. When applying this
%to a set of long English novels and essays, they got the best results
%using only the combined lexical topical network, which produced an average
%accuracy of 0.7950.

Siamese neural networks are networks that share weights across multiple parts
of the network. Siamese networks were first introduced by \citet{NIPS1993_769}
for handwritten signature verification. The idea behind siamese networks is
that the parts of the networks that share weights will give similar output for
similar input. That makes Siamese Networks very good at comparing objects.
\citet{NIPS1993_769} used a device to collect data from peoples signatures which
were able to give $x$ and $y$ coordinates for a pen's position in different time
steps. Their network was set up to take two inputs. Each input was a series
of feature extractions from 200 timesteps. One such feature could be a binary
value indicating if the pen was on the board or in the air at a specific time
step. The time input was given to the siamese network which used convolutions to
look at multiple timesteps at once. After the convolutions the two inputs were
reduced to a higher level, 18 element long feature vector. The output of the
siamese network was therefore 2 feature vectors representing the two signatures
given as input. \citet{NIPS1993_769} then used a distance function with a
threshold on the two feature vectors to verify whether or not the signatures
were written by the same person.

This makes siamese networks for verification tasks the obvious choice. A siamese
network takes two signals as input and will output features extracted from the
two signals. The features will be similar if the signals are similar since the
same function is computed by both parts of the network.

Siamese networks has also been used by \citet{Koch2015SiameseNN} for one-shot
image classification. One-shot classification is a task that humans are very
good at but machines tend to be very bad at. If you show a human a single
picture of a camel, he/she will be able to almost instantly determine if a camel
is contained within other pictures. \citet{Koch2015SiameseNN} trained a siamese
network to compare images and determine whether or not they contain the same
object. The model could then be used to pair-wise compare new images to provided
images. Thus the network learned to determine if two images was of the same
class but not which class specifically.

Most relevant for this thesis \citet{qian:2018} performed a study of different
deep learning methods for authorship attribution. They used both a \gls{GRU}
network and a \gls{LSTM} network. They implement 4 different networks,
sentence-level-\gls{GRU}, article-level-\gls{GRU}, article-level-\gls{LSTM} and
article-level-Siamese-network. Of those 4 networks the siamese network is of
special interest to our project, because it solves the authorship verification
problem and not the authorship attribution problem. The architecture for
authorship verification chosen by \citet{qian:2018} started by using a \gls{GRU}
network which would be supplied with the two texts for comparison. On top of
the \gls{GRU} network they used an average pool. The output of the average pool
is then seen as features extracted from the two texts. They then used a softmax
layer to get a distribution over the probability of who has written the two
texts. These probabilities were then used to see whether or not the same author
is predicted for both texts. On top of the softmax output they added the cosine
similarity between the probability distributions outputted by the softmax layer.
From that cosine similarity they could compute a binary output saying whether
or not the texts were written by the same author or by different authors.
\citet{qian:2018} obtained excellent results in the authorship attribution and
authorship verification case on the siamese network.


\subsection{Previous Work Using MaCom's Dataset}
\label{subsec:previous_work_using_macoms_dataset}

\citet{hansen2014} and \citet{aalykke2016} have both worked with MaCom's dataset
before we did. \citet{hansen2014} focused on authorship attribution with a
temporal focus. They found that ignoring some of the older texts an author had
written did not affect performance very much. The method they used was based
on character-n-gram frequencies. They extracted the frequencies of a set of
character-n-grams and trained an \gls{SVM} on those frequencies. They used cross
validation to find the best hyperparameters for the \gls{SVM} and weighted the
assignments by submission time. They ended up with an accuracy of 84 \%.

\citet{aalykke2016} did not work with the authorship attribution problem but
rather the authorship verification problem. They used a distance based approach.
They extracted features from the text like \citet{hansen2014} but instead of
using an \gls{SVM} they found the nearest text in the space of features. They
experimented using different distance functions and then used thresholding to
determine if the proposed author was correct. \citet{aalykke2016} found that for
Danish texts character-n-grams of size 8 worked best. His implementation lead to
a plagiarism detection accuracy of 71.9\%, and a false accusation rate of under
5\%.

\subsection{Summary}

Several of the methods for authorship attribution obtained excellent results
using convolutional methods on the character level. However that was in the
authorship attribution case where performance will drop of as the number of
different authors increase and a new network has to be trained for new authors.
Similarly good results were obtained by the siamese networks. Since we work with
authorship verification we try a siamese network structure as it is good at
comparing inputs.
