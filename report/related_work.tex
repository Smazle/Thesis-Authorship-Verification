\section{Related Work} \label{sec:related_work}

In this section we will describe some previous work which served as sources
of inspiration for this paper. It describes the work done on the subject of
authorship verification and authorship attribution with a special focus on
methods that use deep learning.


\subsection{Neural Network Based Authorship Verification/Attribution}

\citet{DBLP:journals/corr/RuderGB16c} shows a Neural Network for authorship
attribution. Authorship attribution is closely connected to authorship
verification as every authorship attribution problem can be transformed into a
series of authorship verification problems. To attribute the author of a text
you can perform a series of authorship verifications of each candidate author
and return the author that reported true. Their experiment consisted of a
network where they first had a Convolutional layer, after that a max-over-time
pooling layer and then a densely connected network on the top of that.
Character level features has previously been shown to be important for both
authorship attribution and in turn authorship verification as well. A point
which is emphasized when looking at the results of \citet{hansen2014}, and
\citet{aalykke2016}, who both not only got good results using Character level
features, but also did so working on data from the same source as we did during
the creation of this paper. The hope was that the convolutional layer would
learn important features from sequences of characters. The max-over-time pooling
would take the most important value from each convolutional filter and would
extract a similar number of features for each text even though the texts are
of differing length. The dense network was then supposed to take the features
extracted from the text and determine authorship of the text from them.

\citet{DBLP:journals/corr/RuderGB16c} also used multiple channels in their
network. Each channel was a different token sequence some of them were
word embeddings and some were character embeddings. Some of the channels
were static while some of the channels were non-static meaning that the
word/char-embedded vectors would change during training. The point of the
channels was that the network were able to extract features from multiple
linguistic layers, all describing a different stylometric feature of text.
These layers span from character specific features, all the way to application
specific features, as explanation in \citet[Section 2]{stamatos2009}. We will
elaborate more on these features, as we get to feature extraction in Section
\ref{subsec:baseline_methods}. They specifically used networks with the
following channels

\begin{description}
    \item[CNN-char:] Single non-static character channel.
    \item[CNN-word:] Single non-static word channel.
    \item[CNN-word-word:] Two word channels, one non-static and one static.
    \item[CNN-word-char:] Two non-static channels one for words and one for
        characters.
    \item[CNN-word-word-char:] One static word channel, one non-static word
        channel and one non-static character channel.
\end{description}

The best performing configuration was the CNN-char.

\citet{shrestha2017} experimented with classifying the author of short texts
such as messages on social media, and emails. Their approach made use of a
\gls{CNN}. This \gls{CNN} only takes in a sequence of character-n-grams. The
reasoning for this usage of only char-n-grams was the small amount of text
in each sample. By passing these n-grams through a embedding layer, a 25\%
dropout layer, 3 convolutional layers and then using max-over-time, they got
a compact representation of the text. They hypothesized this representation
captures the morphological, lexical and syntactic level of the supplied text.
This compact representation is then parsed through a fully connected soft-max
layer, to produce a probabilistic distribution over all authors. In order to
test their method they used a twitter data set, containing approximately 9000
user, all having written more than 1000 tweets. They made use of two different
configurations of their networks. One using character-1-grams and one using
character-2-grams. After removing bot-like authors, they got an accuracy of
0.678, and 0.683 respectively. This however, was only with 35 authors used,
and 1000 tweets per author. In the case where either the authors count was
increased or the number of tweets was decreased, the accuracy quickly worsened.
In order to extract some sort of meaning from the predictions they made using
this approach, they made use of the saliency score to determine the impact each
n-gram had on the final decision.

The approach proposed by \citet{ding2016} was somewhat more complex. While they
too made use of a \gls{CNN}, the application of the network was split into
four different modalities. They described their approach as \textit{Mining
stylometric representations for authorship analysis}. They did it by using 4
different \gls{CNN}s, each one focusing on 1-2 stylometric levels of the text.
The point of splitting it up, was so they could pick and choose which modality
they wanted to use on the specific example. The first network looked at the
word level, and topical modalities of the text. In doing so it made use of
3 different author word biases. The topical bias described a texts tendency
towards words that were specific to the topic of the entire text. The local
contextual bias described the texts tendency towards specific words based on the
topic of a specific part of the text, as the overall topic was rarely consistent
throughout the text. Finally there is the lexical bias, which described the bias
the author had towards words that have a similar meaning, such as "nice day"
vs "wonderfull day". Using these thee biases, where the local contextual bias
is derived from a numerical representation of a sequence of words, the network
predicted the word most likely to fit into the local context. The second network
looked at the character modality, and worked in a somewhat similar fashion,
but on the character level. It represented each word in the text as a set of
character bigrams. After converting each of these bigrams to the numerical
representation, it was fed through a fully connected sigmoid layer, establishing
the relation between the bigrams and the word they come together to represent.
The third and last network addressed the syntactic modality. This focused on
the \gls{POS}-tags of the text. By looking at the N neighboring \gls{POS}-tag
bigrams, the network determined the most likely \gls{POS} of that word. All
these three modality-network attempted to optimize the forward log probability
of their prediction. After training each of these models on the texts of a
specific author, they were applied to two texts. The similarity of each of these
two texts, were then computed using a simple cosine distance. When applying this
to a set of set of long English novels and essays, they got the best results
using only the combined lexical topical network, which produced an average
accuracy of 0.7950.


\subsection{Siamese Neural Networks}

Siamese Neural Networks are networks that shares weights across multiple parts
of the network. Siamese Networks were first introduced by \citet{NIPS1993_769}
for signature verification. The idea behind Siamese Networks is that the parts
of the networks that share weights will give similar output for similar input.
That makes Siamese Networks very good at comparing objects. \citet{NIPS1993_769}
used a device for getting data from the signatures which were able to give $x$
and $y$ coordinates for a pens position in different time steps. Their network
were set up to take two inputs. Each input was a series of feature extractions
from 200 timesteps. The features extracted were for example if the pen were
on the board or in the air at the time step. The time input were given to the
Siamese Network which used convolutions to look at multiple timesteps at a time.
After the convolutions the two inputs were reduced to a higher level 18 element
long feature vector. The output of the Siamese Network is therefore 2 feature
vectors representing the two signatures given as input. \citet{NIPS1993_769}
then used a distance function on the two feature vectors and a threshold to
verify whether or not the signatures were written by the same person.

We can see that it is natural to use Siamese networks for verification tasks.
The network takes two signals as input and will output features extracted from
the two signals. The features will be similar if the signals are similar since
the same function is computed by both parts of the network.

Siamese networks has also been used by \citet{Koch2015SiameseNN} for one-shot
image classification. One-shot classification is a task that humans are very
good at but machines tend to be very bad at. If you show a single picture to
a human of a camel he/she will be able to almost instantly classify images
into images containing camels and images that do not. \citet{Koch2015SiameseNN}
trained a Siamese Network to learn to compare images and say whether or not
they contain the same object. Then when presented with a single instance that
instance can be given as one of the inputs to the Siamese Network. The network
is then able to compare the single sample given to any other image.

Most relevant for this thesis \citet{qian:2018} performed a study of different
deep learning methods for authorship attribution. They used both a \gls{GRU}
network and a \gls{LSTM} network. They implement 4 different networks,
sentence-level-\gls{GRU}, article-level-\gls{GRU}, article-level-\gls{LSTM}
and article-level-Siamese-network. Of those 4 networks the Siamese network is
of special interest to our project. The Siamese network solves the authorship
verification problem and not the authorship attribution problem. A Siamese
network is a network that use the same weights and parameters in multiple
parts of the network. The architecture for authorship verification chosen by
\citet{qian:2018} started by using a \gls{GRU} network on the two texts to
verify authorship on. On top of the \gls{GRU} network they used an average
pool. The output of the average pool is then seen as features extracted from
the two texts. They then use a softmax layer to get a distribution over the
probability of who has written the two texts. These probabilities are then used
to see whether or not the same author is predicted for both texts. On top of
the softmax output they added the cosine similarity between the probability
distributions outputted by the softmax layer. From that cosine similarity they
can get a binary output saying whether or not the texts are written by the same
author or by different authors. \citet{qian:2018} obtained excellent results in
the authorship attribution case on the Siamese network.


\subsection{Other Previous Work in Authorship Verification}

\citet{hansen2014} and \citet{aalykke2016} both describe approaches very
applicable to our specific scenario, as they use Danish texts as the basis of
their authorship-attribution task. \citet{aalykke2016} made use of a distance
based approach. After extracting a set of features from the text, they made
use of different distance measures, on which the attribution was based.
\citet{hansen2014} on the other hand, started by extracting character-n-grams
from the texts and used those as the features. Using these features, some 5-fold
cross validation, and weighing the features based on the submission date of the
text, they trained a SVM classifier, which ended up with an accuracy of 84\%

\subsection{Extracted Knowledge}

Several of the methods for authorship attribution obtained excellent results
using convolutional methods on the character level. That was however in the
authorship attribution case where performance will drop of as the number of
different authors increases and a new network has to be trained for new authors.
Similarly good results were obtained by the Siamese networks. Since we are in
the authorship verification case we try a Siamese network structure since it is
good at comparing things. 

