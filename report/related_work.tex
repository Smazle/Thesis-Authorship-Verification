\section{Related Work} \label{sec:related_work}

In this section we will describe previous work in authorship verification and
authorship attribution with a special focus on methods that use deep learning.


% TODO: Add description of general CNN based on some article.
\subsection{Neural Network Based Authorship Verification/Attribution}

\cite{DBLP:journals/corr/RuderGB16c} shows a Neural Network for authorship
attribution. Authorship attribution is closely connected to authorship
verification as every authorship attribution problem can be transformed into a
series of authorship verification problems. To attribute the author of a text
you can perform a series of authorship verifications of each candidate author
and return the author that reported true. Their experiment consisted of a
network where they first had a Convolutional layer, after that a max-over-time
pooling layer and then a densely connected network on the top of that. Character
level features has previously been shown to be important for both authorship
verification and attribution (TODO: cite something). The hope was that the
convolutional layer would learn important features from sequences of characters.
The max-over-time pooling would take the most important value from each
convolutional filter and would extract a similar number of features for each
text even though the texts are of differing length. The dense network was then
supposed to take the features extracted from the text and determine authorship
of the text from them.

\cite{DBLP:journals/corr/RuderGB16c} also used multiple channels in their
network. Each channel was a different token sequence some of them were word
embeddings and some were character embeddings. Some of the channels were static
while some of the channels were non-static meaning that the word/char-embedded
vectors would change during training. The point of the channels was that the
network were able to extract features from multiple levels of features (TODO:
reference some explanation of different levels of features). Specifically they
used networks with the following channels

\begin{description}
    \item[CNN-char:] Single non-static character channel.
    \item[CNN-word:] Single non-static word channel.
    \item[CNN-word-word:] Two word channels, one non-static and one static.
    \item[CNN-word-char:] Two non-static channels one for words and one for
        characters.
    \item[CNN-word-word-char:] One static word channel, one non-static word
        channel and one non-static character channel.
\end{description}

The best performing configuration was the CNN-char.

\cite{shrestha2017} experimented with classifying the author of short texts
such as messages on social media, and emails. Their approach made use of a
\gls{CNN}. This \gls{CNN} only takes in a sequence of character-n-grams. The
reasoning for this usage of only char-n-grams was the small amount of text
in each sample. By passing these N-grams through a embedding layer, a 25\%
dropout layer, 3 convolutional layers and then using max-over-time, they got
a compact representation of the text. They hypothesized this representation
captures the morphological, lexical and syntactic level of the supplied text.
This compact representation is then parsed through a fully connected soft-max
layer, to produce a probabilistic distribution over all authors. In order to
test their method they used a twitter data set, containing approximately 9000
user, all having written more than 1000 tweets. They made use of two different
configurations of their networks. One using character-1-grams and one using
character-2-grams. After removing bot-like authors, they got an accuracy of
0.678, and 0.683 respectively. This however, was only with 35 authors used,
and 1000 tweets per author. In the case where either the authors count was
increased or the number of tweets was decreased, the accuracy quickly worsened.
In order to extract some sort of meaning from the predictions they made using
this approach, they made use of the saliency score to determine the impact each
n-gram had on the final decision.

The approach proposed by \cite{ding2016} was somewhat more complex. While they
too made use of a \gls{CNN}, the application of the network was split into four
different modalities. They describe their approach as \textit{Mining stylometric
representations for authorship analysis}. They did it by using 4 different
\gls{CNN}s, each one focusing on 1-2 stylometric levels of the text. The point
of splitting it up, was so they could pick and choose which modality they wanted
to use on the specific example. The first network looks at the word level,
and topical modalities of the text. In doing so it makes use of 3 different
author word biases. The topical bias describes a texts tendency towards words
that are specific to the topic of the entire text. The local contextual bias
describes the texts tendency towards specific words based on the topic of a
specific part of the text, as the overall topic is rarely consistent throughout
the text. Finally there is the lexical bias, which describes the bias the
author has towards words that have a similar meaning, such as "nice day" vs
"wonderfull day". Using these thee biases, where the local contextual bias is
derived from a numerical representation of a sequence of words, the network
predicts the word most likely to fit into the local context. The second network
looks at the character modality, and works in a somewhat similar fashion,
but on the character level. It represents each word in the text as a set of
character bigrams. After converting each of these bigrams to the numerical
representation, it is fed through a fully connected sigmoid layer, establishing
the relation between the bigrams and the word they come together to represent.
The third and last network addresses the syntactic modality. This focuses on
the \gls{POS}-tags of the text. By looking at the N neighboring \gls{POS}-tag
bigrams, the network determines the most likely \gls{POS} of that word. All
these three modality-network attempts to optimize the forward log probability of
their prediction. After training each of these models on the texts of a specific
author, they are applied in turn to two texts. The similarity of each of these
two texts, is then computed using a simple cosine distance. When applying this
to a set of set of long English novels and essays, they got the best results
using only the combined lexical topical network, which produced an average
accuracy of 0.7950.


\subsection{Siamese Neural Networks}

Siamese Neural Networks are networks that shares weights across multiple parts
of the network. Siamese Networks were first introduced by \cite{NIPS1993_769}
for signature verification. The idea behind Siamese Networks is that the parts
of the networks that share weights will give similar output for similar input.
That makes Siamese Networks very good at comparing objects. \cite{NIPS1993_769}
used a device for getting data from the signatures which were able to give $x$
and $y$ coordinates for a pens position in different time steps. Their network
were set up to take two inputs. Each input was a series of feature extractions
from 200 timesteps. The features extracted were for example if the pen were on
the board or in the air at the time step. The time input were given to the
Siamese Network which used convolutions to look at multiple timesteps at a time.
After the convolutions the two inputs were reduced to a higher level 18 element
long feature vector. The output of the Siamese Network is therefore 2 feature
vectors representing the two signatures given as input. \cite{NIPS1993_769} then
used a distance function on the two feature vectors and a threshold to verify
whether or not the signatures were written by the same person.

We can see that it is natural to use Siamese networks for verification tasks.
The network takes two signals as input and will output features extracted from
the two signals. The features will be similar if the signals are similar since
the same function is computed by both parts of the network.

Siamese networks has also been used by \cite{Koch2015SiameseNN} for one-shot
image classification. One-shot classification is a task that humans are very
good at but machines tend to be very bad at. If you show a single picture to a
human of a camel he/she will be able to almost instantly classify images into
images containing camels and images that don't. \cite{Koch2015SiameseNN} trained
a Siamese Network to learn to compare images and say whether or not they contain
the same object. Then when presented with a single instance that instance can be
given as one of the inputs to the Siamese Network. The network is then able to
compare the single sample given to any other image.

Most relevant for this thesis \cite{qian:2018} performed a study of different
deep learning methods for authorship attribution. They used both a \gls{GRU}
network and a \gls{LSTM} network. They implement 4 different networks,
sentence-level-\gls{GRU}, article-level-\gls{GRU}, article-level-\gls{LSTM}
and article-level-Siamese-network. Of those 4 networks the Siamese network is
of special interest to our project. The Siamese network solves the authorship
verification problem and not the authorship attribution problem. A Siamese
network is a network that use the same weights and parameters in multiple
parts of the network. The architecture for authorship verification chosen by
\cite{qian:2018} started by using a \gls{GRU} network on the two texts to
verify authorship on. On top of the \gls{GRU} network they used an average
pool. The output of the average pool is then seen as features extracted from
the two texts. They then use a softmax layer to get a distribution over the
probability of who has written the two texts. These probabilities are then used
to see whether or not the same author is predicted for both texts. On top of
the softmax output they added the cosine similarity between the probability
distributions outputted by the softmax layer. From that cosine similarity they
can get a binary output saying whether or not the texts are written by the same
author or by different authors. \cite{qian:2018} obtained excellent results in
the authorship attribution case on the Siamese network.


\subsection{Other Previous Work in Authorship Verification}

\cite{hansen2014} and \cite{aalykke2016} both describe approaches very
applicable to our specific scenario, as they use Danish texts as the basis of
their authorship-attribution task. \cite{aalykke2016} made use of a distance
based approach. After extracting a set of features from the text, they made
use of different distance measured, on which the attribution was based.
\cite{hansen2014} on the other hand, started by extracting character-n-grams
from the texts and used those as the features. Using these features, some
5-fold cross validation, and weighting the features based on the submission 
date of the text, they trained a SVM classifier, which ended up with an
accuracy of 84\%

An example of a profile-based authorship attribution approach, which is highly
regarded by \cite{stamatos2009}, make use of a compression. This works, not
by having represented each author library as a set of features, but rather
makes use of compression to do so. In the initial scenario, we have a set of
author-profiles, which is the concatenation of all their individual work.
When a new text is then introduced the authorship is determined by looping
through each author, adding it library (their concatenated profile). The
profile is then compressed, both with and without the new text included in its
library. The bit-wise difference is then computed, by subtracting them from
one another, resulting in what is essentially the cross-entropy between the
two texts. The author which the lowest cross-entropy is then considered to be
the author of this new text. Different methods of comprehension can be used as
the base for this model, each giving different results. In the instance that
\cite{stamatos2009} describes, RAR compression yielded the best results, but
this of cause depends on the scenario one finds oneself in.

As for instance-based approaches they use a more classic machine learning
approach. Texts are again represented as a set of features. Then each texts
features is used as a training sample. Thus several simple machine learning
approaches can be applied to the data as either a binary classification problem,
in the case of authorship verification, and a multi-class classification problem
in the case of authorship attribution.


% TODO: Add subsection explaining our choices based on this related work
% section.
