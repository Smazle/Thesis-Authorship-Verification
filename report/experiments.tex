\section{Experiments} \label{sec:experiments} 

We have previously described the theory behind the methods we are implementing
to solve the authorship verification problem. In this section we describe
in detail the experiments we performed to find the optimal hyperparameters and
network structures. We will test each method implemented against the external
validation set \gls{C}. We start by describing experiments on our baseline
methods and after that we present the Siamese networks we implemented.

\subsection{Baseline Methods} \label{subsec:baseline_methods}

The implementation of both the \gls{SVM} and the Extended Delta method closely
resembles the implementation used by \citet{US}. There is however a big
difference with the application of the delta method. Contrary to the scenario
in \citep{US} where PAN data was used there is not an instance where we only
have 1 text per author. Therefore the original version of the delta method can
be used as described by \citet{evert2015towards}. In that method when given
a new text $t$ supposedly written by an author $\alpha \in \mathcal{A}$ his
texts $T_\alpha$ and a set of texts $\hat{T}_\alpha$ where $|\hat{T}_\alpha|
= |T_\alpha|$ are given to a \gls{KNN} classifier. $T_\alpha$ will serve as
positive examples of the authors writing style while $\hat{T}_\alpha$ will serve
as negative examples. The text $t$ is then classified by finding the $K$ closest
texts and taking a majority vote between them.

\subsubsection{Feature Selection}

The experiments performed on our baseline methods was centered around parameter
tuning. Not only parameters such as the $K$ in the extended delta method
but also the set of features supplied to the methods. Other methods such as
random forest has a built in filter for bad and noisy features (see Section
\ref{subsubsec:applying-other-machine-learning-methods}). This is not the case
for the extended delta method which is just a distance metric that has no way
of weighing the features. Therefore we chose to perform a feature selection for
rather than just having them use the entire set available to them. Contrary to
what was done by \citet{US} this process did not consist of trying out some
random selection of features but rather a more systematic approach was used.

We started by generating a set of features. The feature set is simply a set of
n-grams. In order for us to find as good a feature set as possible we created
a large initial feature set to increase the search space. To generate the
features we used a corpus of text. Specifically we used the \gls{B} dataset as a
corpus. From the corpus we enumerated all the different n-grams in the different
categories and computed the frequency of each n-gram. The corpus had,

\begin{itemize}
    \item 188,472 different word-2-grams,
    \item 2,178 different character-2-grams,
    \item 219 different \gls{POS}-tag-2-grams, and
    \item 524 special-character-2-grams.
\end{itemize}

The quantities here are large due to the size of the corpus and will continue
to rise as $n$ increases. Rather than extracting all of those features from
our texts we chose to focus on the n-grams with highest frequency as those are
the most important features \citep{stamatos2009}. To see why, consider a case
where a very infrequent feature were chosen. That feature might only exist
in the corpus and in none of the texts in the test set. That feature would
therefore be useless in determining the authorship of those texts. Using the
quantities listed above as inspiration we generated a candidate feature set of
4950 features. The set consisted of,

\begin{itemize}

    \item

        The frequencies of the 500 most frequent word-n-grams for $n \in \{1, 2,
        3, 4\}$,

    \item

        The frequencies of the 300 most frequent character-n-grams for $n \in
        \{2, 3, ..., 10\}$,

    \item

        The frequencies of the 50 most frequent \gls{POS}-tag-n-grams for $n \in
        \{3, 4\}$, and

    \item

        The frequencies of the 50 most frequent special-character-n-grams for $n
        \in \{2, 3, 4\}$.

\end{itemize}

The quantities extracted reflects the inherent number of different n-grams of
that type. Since there are many more different words than characters there will
also be many more different word-n-grams than character-n-grams of the same
size. We therefore also use more of the most frequent word-n-grams. The sizes
of the different character n-grams we extracted were based on the amount used by
\citet{aalykke2016}. They found that character n-grams of size 8 worked very
well on MaCom's data which is why we search all the way up past 8. \citet{US}
showed that when the $n$ in word-n-grams reach a value of 4, the probability of
that sequence actually occurring in any given text is drastically reduced. A
similar thing can be seen with the special-character-n-grams where an increase
in n would not contribute anything. We used so few \gls{POS}-tag-n-grams since
the number of different \gls{POS} tags are very small. Therefore the number of
different sequences of \gls{POS} tags are also small compared with the other
n-gram types.

Having the features we could start our feature selection. The process proved
very computationally expensive. Having to check every combination of 4950
different features simply was not feasible. Therefore we implemented a greedy
search algorithm instead in combination with using only a small dataset \gls{I}
of 100 authors for the feature selection.

The greedy algorithm as described by \citet{kanDeng} is a simple forward feature
selection. Having our previously created feature set we loop through each single
feature, validating its accuracy when applied to each $\alpha \in \mathcal{A}$,
where $\mathcal{A}$ is the dataset \gls{I}. The validation on each author
$\alpha$ consists of fetching $T_{\alpha}$ and a set $\hat{T}_{\alpha} \subset
\overline{T}_\alpha$, where $|\hat{T}_\alpha| = |T_\alpha|$. Using these sets
of positive and negative cases we used k-fold cross validation to determine
the performance of the feature for that author $\alpha$. This is done for all
authors and when averaged we have the performance of that single feature. This
process is then repeated for all features. The best performing feature is then
added to a set of candidate features. In the next iteration we loop through all
the features again but we validate against each feature in combination with
the already selected features. This process is repeated until a set number of
features are selected. At that point the feature set that generated the highest
score is chosen.

Due to the increased run-time when using leave one out cross validation we
had to use of some other model selection approach. Under normal circumstances
normal X-fold cross validation would work out fine but since we had so little
data available for some authors we had to use a stratified X-fold. The average
assignment count for our authors just above 13. Some authors had as little as 2
assignments in their library. When so few datapoints are available an unlucky
split between training and validation in a cross validation might result in a
uneven split between the positives and negatives in the training dataset. It
is not a problem when more data points are available since the large numbers
involved make unlucky splits very rare. The stratified k fold cross validation
will make sure that a similar number of positives and negatives are in each
split and we can avoid that problem.

For both the \gls{SVM} and extended delta method we ran this algorithm until 450
features were selected. This was done over 3 stratified folds. The \gls{SVM}
performed its feature selection using the default RBF kernel, a C with value of
1, and $\gamma = \frac{1}{\text{n\_features}}$.

Due to some authors having below 3 texts,
the default K value of \glspl{KNN} K parameter was set to 3 for the feature
selection.

\begin{figure}
    \centering
    \textbf{Greedy Feature Selection}\par\medskip
    \includegraphics[scale=0.6]{./pictures/experiments/feature_selection}
    \caption{The process of the greedy feature selection of the SVM and the
        extended delta Method. The x axis shows the number of selected features
        so far in the search and the y axis the average accuracy when using that
        feature set for each author. The extended delta method peaks with a very
        small feature set while the \gls{SVM} needed more features to maximize
        accuracy.
    }
    \label{fig:fs_results}
\end{figure}

The results of both the \gls{SVM} and the extended delta method feature
selection can be seen in Figure \ref{fig:fs_results}. The amount of selected
features were 221 for the SVM and 5 for the extended delta Method, having an
accuracy of 0.697 and 0.672 respectively. The features chosen by the \gls{SVM}
method was: 27 word-4-grams, 22 word-2-grams, 21 char-10-grams, 18 word-3-grams,
19 char-4-grams, 16 char-3-grams, 15 char-5-grams, 15 char-9-grams, 15
word-1-grams, 14 char-2-grams, 14 char-6-grams, 10 char-7-grams, 10
char-8-grams, 3 pos-3-grams and 2 pos-4-grams. The features chosen by the
extended delta method was 2 char-5-grams, 2 word-3-grams and 1 word-4-gram.


\subsubsection{Hyper Parameter Selection}\label{sec:hyp_select}

As mentioned in the previous section time constraints rendered us unable to
do the hyper parameter selection in parallel with the feature selection. For
that reason we chose to select our features first and then tuning the hyper
parameters on those selected features. As in the feature search we search for
our parameters in a set of parameters. Recall that the \gls{SVM} need the $C$
and $\gamma$ parameters and the extended delta method need the $p$ and $K$
parameters. We search for parameters in a grid search,

\begin{align}
    \text{SVM} &:
    \begin{array}{lr}
        C=\{10^{-3}, 10^{-1}, 10^{1}, 10^{3}, 10^{5}, 10^7\}\\
        \gamma=\{10^{-3}, 10^{-1}, 10^{1}, 10^{3}, 10^{5}, 10^7\}
    \end{array} \\
    \text{Extended Delta} &:
    \begin{array}{lr}
        p=\{1,2,3,4,5\}\\
        K=\{1,3,5,7,9,11,13,15\}
    \end{array}.
\end{align}

For each variation of the hyper parameters we average the performance of the
method over all unique authors in the training set in a manner similar to the
feature selection. We use the same 3-fold stratified cross validation and
arrange each author with set of opposing set of texts. The accuracy of each
parameter configuration can be seen in Table \ref{table:KNN} for the extended
delta method, and Table \ref{table:SVM} for the \gls{SVM}. The parameter search
was performed on the \gls{J} dataset consisting of 5,400 authors.

\begin{table}[h]
    \centering
    \textbf{\gls{KNN} Cross Validation Results}\par\medskip
    \begin{tabular}{|c|ccccc|}
        \hline
        \backslashbox{$K$}{$p$} & 1 & 2 & 3 & 4 & 5 \\\hline
        1 & \textbf{0.630} & 0.603 & 0.588 & 0.577 & 0.570\\
        3 & 0.628 & 0.589 & 0.572 & 0.565 & 0.555 \\
        5 & 0.621 & 0.576 & 0.559 & 0.549 & 0.545 \\
        7 & 0.612 & 0.564 & 0.546 & 0.537 & 0.533 \\
        9 & 0.600 & 0.554 & 0.537 & 0.527 & 0.524 \\
        11 & 0.587 & 0.542 & 0.527 & 0.520 & 0.517 \\
        13 & 0.581 & 0.537 & 0.524 & 0.517 & 0.514 \\
        15 & 0.571 & 0.532 & 0.519 & 0.514 & 0.512 \\\hline
    \end{tabular}
    \caption{The results from performing a grid search of the $p$ and $K$
        parameters of the \gls{KNN} algorithm.}
    \label{table:KNN}
\end{table}

\begin{table}[h]
    \centering
    \textbf{\gls{SVM} Cross Validation Results}\par\medskip
    \begin{tabular}{|c|cccccc|}
        \hline
        \backslashbox{$C$}{$\gamma$} & $10^{-3}$ & $10^{-1}$ & $10^{1}$ & $10^{3}$ & $10^{5}$ & $10^{7}$ \\\hline
         $10^{-3}$ & 0.626 & 0.626 & 0.626 & 0.641 & 0.558 & 0.500 \\
         $10^{-1}$ & 0.626 & 0.626 & 0.626 & 0.641 & 0.558 & 0.500 \\
         $10^{1}$  & 0.626 & 0.626 & 0.626 & \textbf{0.689} & 0.576 & 0.501 \\
         $10^{3}$  & 0.626 & 0.626 & 0.681 & 0.687 & 0.576 & 0.501 \\
         $10^{5}$  & 0.626 & 0.680 & 0.678 & 0.687 & 0.576 & 0.501 \\
         $10^{7}$  & 0.674 & 0.678 & 0.678 & 0.687 & 0.576 & 0.501 \\\hline
    \end{tabular}
    \caption{The results from performing a grid search for $C$ and $\gamma$ of
        the \gls{SVM} algorithm}
    \label{table:SVM}
\end{table}

We performed a validation of the methods on the \gls{C} dataset. While
applying these methods to a validation set wont change anything in terms of
the parameters we have selected. It will give us the ability to estimate the
accuracy of our methods when applied to a never before seen test dataset.
Using this untouched data set as the base the validation determines what text
from each author is the newest one ending up with a set of samples of the
form $(t, \alpha)$ where $t$ is a text and $\alpha$ is an author. For each
of these samples $T'_\alpha = T_\alpha \setminus \{t\}$ is extracted from
\gls{C}. Additionally another randomly sampled set $\hat{T}_\alpha \subset
\overline{T}_\alpha$ is extracted where $|\hat{T}_\alpha| = |T'_\alpha|$. Using
the combined set $T'_\alpha \cup \hat{T}_\alpha$, we can train the model in
question having provided it with both positive and negative examples of author
$\alpha$ writing style. To include samples that depict a student that used a
ghostwriter we change the text we do our prediction on. As described earlier in
the real world about 4\% of assignments are ghostwritten. We therefore evaluate
the models on both a dataset of 50\% negatives and a dataset of 4\% negatives.
We have shown the results on both datasets in Table \ref{tab:baseline-val-res}.

\begin{table}[h]
    \begin{adjustbox}{center}
    \begin{tabular}{|c|c|c|c|c|c|c||c|c|}
        \hline
        Negatives & Classifier & Params & TP & TN & FP & FN & \textbf{Accuracy} & \textbf{Accu Err} \\ \hline
        50\% & SVM & \{C:10, $\gamma$:1000\} &  759 & 692 & 308 & 241 & \textbf{0.7255} & \textbf{0.2583} \\ \cline{2-9} 
        & ED & \{K:1, p:1\} & 667 & 583 & 417 & 333 & \textbf{0.625} & \textbf{0.36353} \\ \hline
        4\% & SVM & \{C:10, $\gamma$:1000\} & 762 & 23 & 25 & 238 & \textbf{0.749} & \textbf{0.91187} \\ \cline{2-9} 
        & ED & \{K:1, p:1\} & 680 & 23 & 23 & 320 & \textbf{0.67208} & \textbf{0.93294} \\ \hline
    \end{tabular}
    \end{adjustbox}
    \caption{The results of running our two baseline methods the author specific
        \gls{SVM} and Extended Delta method, on the dataset \gls{C} the
        validation set.}
    \label{tab:baseline-val-res}
\end{table}

Neither the extended delta and the author specific \gls{SVM} methods output a
probability. Therefore they are much harder to threshold and keep under the
accusation error limits set by MaCom. We therefore just report the results when
maximizing the accuracy of the methods.


\subsection{Deep Learning}

In the prediction system from Definition
\ref{def:weighted_average_prediction_system} we needed a function $f \in
\mathcal{F}$ that takes two texts and returns the probability that those
texts are written by the same author. As described earlier siamese neural
networks are well suited for comparing objects. In this Section we describe
the experiments we performed with different network architectures and we will
present three different networks. The three networks is a \gls{conv-char-NN}
in Section \ref{subsubsec:conv_char_nn}, a \gls{rec-sent-NN} in Section
\ref{subsubsec:rec_sent_nn} and a \gls{conv-char-word-NN} in Section
\ref{subsubsec:conv_char_word_nn}.

The data we trained our networks on is described in Section \ref{sec:data}. We
trained the networks on the \gls{G} dataset that contains 3,000 authors. We used
early stopping based on a validation dataset \gls{H} that contains 500 authors.
Through experimentation we found that the validation dataset had to contain
completely different authors than the training dataset. In the beginning of
our project we had a validation set that contained different problem instances
(two texts to compare) from the training set but contained some of the same
authors. That lead to the validation set not being a good estimate of the
networks performance on different authors than the ones seen during training.
The networks were therefore overfitting on the specific authors and we could not
see that in the validation datasets accuracy. All accuracy graphs shown in this
Section are based on a validation dataset with 500 completely unseen authors.

The general structure of our networks will be that they take two texts as input.
The texts are first embedded into a format that can be used by the network.
Then the texts are transformed into two sets of features representing the texts
via a weight sharing network (siamese part of the network). Then the text
representation will be combined in some way and a dense network will take the
combined representations and decide whether or not the texts are written by the
same author. We call the different parts of the network \textit{Embedding},
\textit{Feature Extraction}, \textit{Combining} and \textit{Decision}.

When we show graphs of the networks we have produced we use specific names for
different layers in the networks. A glossary of the layer names and parameters
of the layers are shown in Table \ref{tab:glossary}.

\begin{landscape}
    \begin{table}
        \centering
        \textbf{Glossary}\par\medskip
        \begin{tabular}{|L{3cm}|L{9cm}|L{11cm}|}
            \hline
            \multicolumn{1}{|c|}{\textbf{Layer}}                               &
            \multicolumn{1}{|c|}{\textbf{Description}}                         &
            \multicolumn{1}{|c|}{\textbf{Actively Used Parameters}}           \\
            \hline

            Input                                                              &
            Serves as the entrypoint of the network.                           &
            \begin{minipage}[t]{\linewidth}
            \begin{compactdesc}
                \item[Shape] The dimensions of each sample give to the input.
            \end{compactdesc}
            \end{minipage}                                                    \\
            \hline

            Embedding                                                          &
            Take a sequence of one hot encoded vectors and produce a dense
            vector of each element in the sequence. More details can be found
            in Section \ref{subsubsec:layers}                                  &
            \begin{minipage}[t]{\linewidth}
            \begin{compactdesc}
                \item[Output Dim] Size of vector used to represent embedding.
            \end{compactdesc}
            \end{minipage}                                                    \\
            \hline

            Convolutional                                                      &
            Applies convolutions to the data it receives according to the
            description found in Section \ref{subsubsec:layers}.               &
            \begin{minipage}[t]{\linewidth}
            \begin{compactdesc}
                \item[Filters] Dimensionality of the output, ie. number of
                    filter in the convolution.
                \item[Kernel Size] Size of convolution window.
                \item[Strides] Stride length of the convolutional window.
                \item[Activation] The activation function to be applied after
                    the convolution.
            \end{compactdesc}
            \end{minipage}                                                    \\
            \hline

            Global Max Pooling                                                 &
            Extracts the maximum.                                              &
            No parameters.                                                    \\
            \hline

            Concatenation                                                      &
            Concatenates the data it receives from different layers.           &
            No parameters.                                                    \\
            \hline

            Merge                                                              &
            Merges its inputs using a specified function to generate a single
            output.                                                            &
            \begin{minipage}[t]{\linewidth}
            \begin{compactdesc}
                \item[Function] The function used to merge the recieved data.
            \end{compactdesc}
            \end{minipage}                                                    \\
            \hline

            Dense                                                              &
            A fully connected layer taking in data and applying the
            function described in Section \ref{sec:neurons}.                   &
            \begin{minipage}[t]{\linewidth}
            \begin{compactdesc}
                \item[Units] Number of neurons in the layer.
                \item[Activation] The activation function to be applied.
            \end{compactdesc}
            \end{minipage}                                                    \\
            \hline

            Dropout                                                            &
            Randomly remove a specified fraction of neurons in each weight
            update.                                                            &
            \begin{minipage}[t]{\linewidth}
            \begin{compactdesc}
                \item[Rate] The fraction of neurons to drop.
            \end{compactdesc}
            \end{minipage}                                                    \\
            \hline

            Lambda                                                             &
            Applies a specified function to the input it receives              &
            \begin{minipage}[t]{\linewidth}
            \begin{compactdesc}
                \item[Function] The function applied.
            \end{compactdesc}
            \end{minipage}                                                    \\
            \hline

            \gls{LSTM}                                                         &
            An \glsdesc{LSTM} layer which works according to the description in
            Section \ref{subsubsec:layers}.                                    &
            \begin{minipage}[t]{\linewidth}
            \begin{compactdesc}
                \item[Unit] Number of hidden units.
            \end{compactdesc}
            \end{minipage}                                                    \\
            \hline
        \end{tabular}
        \caption{Glossary used when performing experiments, and creating their
            associated models \citep{chollet2015keras}.}
        \label{tab:glossary}
    \end{table}
\end{landscape}


\subsubsection{Character Level Convolutional Siamese Neural Network}
\label{subsubsec:conv_char_nn}

The idea behind the \gls{conv-char-NN} is that we wanted to use convolutions to
look for n-grams in texts. Traditional authorship verification and attribution
is based on carefully engineered n-grams that are compared between two texts
\citep{stamatos2009}. Instead of choosing the n-grams ourselves we wanted the
network to learn which features are important for the authorship verification
task. The features are learned through convolutions. The convolutions look
at some number of characters at a time and gives a single output for those
characters. Assume for example that the network has learned that the character
sequence "ould " is important for deciding the author of a text. Then we
expect the network to react strongly to character sequences that looks
like "ould ". We have illustrated such a convolutional filter in Figure
\ref{fig:convolution_text_example}. The structure of the network is,

\begin{figure}
    \centering
    \textbf{Convolutions for Text Feature Extraction}\par\medskip
    \includegraphics[width=\textwidth]{./pictures/experiments/convolution_example}
    \caption{Illustration of character level convolutions using a filter that
        looks for the character sequence "ould ". Notice that high values are
        produced when the characters the filter looks at match the characters it
        is looking for and low values are produced otherwise. We have
        illustrated 3 different filter locations but the filter is similarly
        placed in all possible locations (also overlapping). We have padded the
        text with zeros to get the same size output as input.}
    \label{fig:convolution_text_example}
\end{figure}

\begin{description}

    \item[Embedding:]

        The embedding takes as input a sequence of integers. Each different
        integer is a compact one-hot encoding of each character. The one-hot
        encoded character stream is embedded in a five dimensional space. The
        hope is that the layer will learn that similar characters should be
        placed close to each other in the output space and dissimilar characters
        should be placed far apart. The same embedding is performed on both of
        the input texts and the layer is therefore part of the siamese fraction
        of the network.

    \item[Feature Extraction:]

        Features are extracted from the two texts via a layer of convolutions
        with different sizes. We used both convolutions with a window size of 8
        and convolutions with a window size of 4. We used 700 of size 8 and 500
        of size 4. We used a window size of 8 since \citet{aalykke2016} found
        that n-grams of size 8 worked the best for Danish texts and we also
        added 4 as we found that that worked well. We used a stride of 1 such
        that each convolutional filter could observe all parts of the texts and
        give an output for each one. The convolutional part of the network also
        shares weights such that the same features are extracted from both the
        input texts. We use the \gls{ReLu} activation function for the reasons
        described in Section \ref{subsubsec:activation_functions}.

        After the convolutional layer we added a global max pooling layer. The
        layer takes the maximum output over each convolutional filter. We did
        that as the output size of the convolutional layer depends on the length
        of the input text. The output of the max pooling layer is $700 + 500 =
        1200$ values, one for each convolutional filter. That means that the
        network learns to extract 1200 different features from each text.

    \item[Combining:]

        The features of the texts are combined using the absolute difference
        function. As described each text is represented as a vector of 1200
        features and to combine them we subtract them from each other and take
        the elementwise absolute value.

    \item[Decision:]

        The decision part of the network consists of 4 dense layers each with
        500 neurons, a dropout layer and an output layer. The 4 dense layers
        also use the \gls{ReLu} activation function. The dropout layer is added
        just before the output layer and performs 30\% dropout to try to combat
        overfitting. The actual prediction is performed in the output layer. The
        output layer use the softmax function to return a probability of the
        texts are written by the same author or written by different authors.

\end{description}

We have shown an illustration of the network in Figure \ref{fig:conv-char-NN}.
In the figure we have illustrated the siamese part of the network as a blue
box. The 4 phases of the network is not illustrated in the figure but it
should be possible to find them by reading the description above. We trained
the network on the dataset described in the Section \ref{sec:data} and in
the beginning of this section. We used the \gls{Adam} optimizer during the
training of the network. We used a learning rate of $\eta = 0.0005$, half of
what was suggested by \citet{DBLP:journals/corr/KingmaB14}. We did that as we
had problems with neurons dying during the training of the network. Other than
the learning rate we used the parameters suggested in the article. That is, the
remembering rates were set to $\gamma_1 = 0.9$ and $\gamma_2 = 0.999$. As the
error function we used the \textit{binary cross-entropy} function defined in
Lemma \eqref{eq:binary_ce}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./pictures/experiments/conv_char_nn/model}
    \caption{The structure of network \gls{conv-char-NN}. Weights are shared
    by the embedding layers and the convolutional layers as shown by the blue
    box. The input to the network is at the top and the output is at the bottom.
    Information flows downwards through the layers. The final softmax layer
    produces a probability distribution over the two possibilities, which are
    whether the texts are either written by the same author or not.}
    \label{fig:conv-char-NN}
\end{figure}

\begin{lemma}[Binary Cross-Entropy]

    The \textit{binary cross-entropy} loss function is defined as,

    \begin{equation}\label{eq:binary_ce}
        L(y, p) = -(y \cdot \log(p) + (1- y)\cdot\log(1 - p)),
    \end{equation}

    where $y : \{0,1\}$ is a label and $p \in [0,1]$ is a predicted probability.

\end{lemma}

When we trained the network we were able to obtain a validation accuracy of
0.71773 in epoch 5. The training accuracy continued rising but we used early
stopping since the validation accuracy had stopped improving. We have shown
a graph of the training and validation accuracies during training in Figure
\ref{fig:conv-char-NN-accuracies}. During the training we used minibatch
learning with a minibatch size of 8. We used a size of 8 as that was the largest
batch size that could fit in the GPUs memory. During the training we padded
all texts with zeros such that all texts in each batch had the same length. We
did that because equal dimensionality was required by Keras.

\begin{figure}
    \centering
    \textbf{Training and Validation Accuracy, \gls{conv-char-NN}}\par\medskip
    \includegraphics[width=0.5\textwidth]{./pictures/experiments/conv_char_nn/training_accuracy}
    \caption{The training and validation accuracy of the \gls{conv-char-NN}
    during training. Each epoch contains several thousand minibatches which is
    why the training and validation accuracies rises so much in the first epoch.
    The first data point (x=0) is computed accuracies before any training.}
    \label{fig:conv-char-NN-accuracies}
\end{figure}

To arrive at the network architecture described above we experimented with
several similar architectures. We started with a network that was not too
far from the final one. All the work computed in this network was applied
to the same one-hot encoded character level as the final network was. The
feature extraction layer was however quite different. It made use of a single
convolutional layer consisting of 1000 filters with a window size of 10 the
selection of which did not have any proper argument and was selected solely to
determine the initial performance of convolutions on the data. In addition to
that the combination part of the network also differed. Rather than computing
the absolute difference as was done in the final network, this initial network
simply concatenated the two siamese branches. If we let $t_f = \{f_1, f_2,
\dots, f_n\}$ be the extracted features of the text $t$, then the concatenation
would be computed by,

\begin{equation}
    combine(t_f, t'_f) \mapsto (f_1, f_2, \dots, f_n, f'_1, f'_2, \dots f'_n)
\end{equation}

Finally the decision part of the network simply consisted of a single 500 neuron
dense network. The network gave promising results which is why we continued
working with it but it quickly overfitted on the training data so we added some
dropout. Like in the network presented above we added the dropout layer just
before the output layer and used 30\% dropout. That regularization allowed the
training accuracy and validation accuracy to follow each other for more epochs
making the final validation accuracy better.

The original combining function had the drawback that the network would have
to learn the positions of the features compared to each other. Instead of just
concatenating the vectors we started computing the absolute difference between
them. Then the network would not have to learn that the first feature in the
feature vector belonged together with the 1201 feature in the vector. The new
combination function is computed as,

\begin{equation}\label{eq:abs}
    combine(t_f, t'_f) \mapsto \left(
        |f_0 - f'_0|, |f_1 - f'_1|, \dots, |f_n - f'_n|
    \right)^T.
\end{equation}

We also tried other combining functions such as the cosine difference but we
did not get any better results. After that change we changed the convolutional
filters from using 1000 filters of size 10 to using 500 filters of size 8 and
500 of size 4. We used those window sizes for the reasons described above. At
the same time we added more dense layers to the model. We again observed the
validation accuracy increasing further. Since our architecture seemed to work
well we scaled up the size of the network to having 4 dense layers instead of 1
as in the final network.

After expanding the network we wanted to figure out which features the network
were looking at. Since the convolutional layer (feature extractor) is followed
by a global max pool we knew that the feature a specific filter was looking
at could be determined by finding the maximum activation of that filter.
Unfortunately we found that the network were looking at metadata of the texts.
Specifically the network were looking at student names and school class numbers.
Obviously a real ghostwriter would make sure that the correct name and school
class were written on the assignment so in the real world those features should
not be used. The features are a product of the creation of the dataset we are
training on. Since different students assignments are combined to create the
negative samples we are training on (almost) all negative samples will have
different names written in them. Similarly (almost) all negative samples will
have different school classes written on them. Therefore such metadata will be
great features in our training dataset but not necessarily great features in
the real world. In Table \ref{tab:name_features} we have shown the activation
strings of the convolutional filter looking at Danish names.

\begin{table}
    \begin{tabular}{ll}
        \textbf{Activation String} & \textbf{Danish Surnames Matching} \\
        \hline
        \verb!adsen\n\n\n! & Madsen. \\
        \verb!ndsen\n\n\n! & Svendsen, Frandsen. \\
        \verb!elsen\n\n\n! & Nielsen, Mikkelsen. \\
        \verb!ersen\n\n\n! & Pedersen, Andersen, Petersen, Iversen, Jespersen. \\
        \verb!ansen\n\n\n! & Hansen, Christiansen, Johansen, Kristiansen. \\
        \verb!ensen\n\n\n! & Jensen, Christensen, S\o rensen, J\o rgensen,
                             Kristensen, Mortensen, Mogensen. \\
        \verb!arsen\n\n\n! & Larsen. \\
        \verb!ulsen\n\n\n! & Poulsen.
    \end{tabular}
    \caption{Showing the mapping from a convolutional filter looking at endings
        of common Danish surnames and the actual surnames. We found the list of
        the most common Danish surnames at \url{https://bit.ly/1d5WUmT}}
    \label{tab:name_features}
\end{table}

As described in Section \ref{sec:data} we tried to remove as much personal
information as possible from the texts by removing the first 200 characters and
by deleting names from the texts. When we trained the network again with those
preprocessing steps we saw a substantial drop in network performance. However
the network was hopefully not looking at metadata anymore.

As described in Section \ref{subsubsec:tuning_parameters} we also had to choose
the best prediction system and best threshold $\theta$ for the network. The
dataset we use to tune the prediction system $P_x$ and threshold $\theta$ are
the \gls{F} dataset. The set consists of 2,000 previously unseen authors. We
find parameters for two cases. One where we generate 50\% positive problems and
50\% negative problems and one where we generate 96\% positive problems and 4\%
negative problems. In the 50/50 case we end up with 4,000 different problems.
For each of the authors we generate a positive sample by taking the newest text
as the unknown text and a negative sample by choosing a random text from some
other author in the set.

As described in Section \ref{subsubsec:tuning_parameters} the best threshold
for a prediction system is found by taking the threshold that maximizes
accuracy while having an accusation error of less than 10\%. The best
prediction system is then the system that has the highest accuracy for
their best threshold. We try 1,001 different thresholds $\theta = 0.000,
0.001, \dots, 1.000$ and we try the prediction systems $P_\mathcal{U}$,
$P_{exp_{0.25}}$, $P_{exp_{0.50}}$, $P_{exp_{0.75}}$, $P_{exp_{1.00}}$,
$P_{lexp_{0.25}}$, $P_{MV}$, $P_{max}$, $P_{min}$ and $P_l$. We have shown
the accuracy and accusation error for the dataset containing 50\% negatives
in Figure \ref{fig:conv-char-NN-pred-50} and for the dataset containing 4\%
negatives in Figure \ref{fig:conv-char-NN-pred-4}.

\begin{figure}
    \centering
    \textbf{Prediction system results for 50\% negative dataset, \gls{conv-char-NN}}\par\medskip
    \includegraphics[scale=0.33]{./pictures/experiments/conv_char_nn/prediction_system_50}
    \caption{Results of running the prediction system with the
        \gls{conv-char-NN} on a validation dataset with 50\% positive samples
        and 50\% negative samples. In the upper graph we show the accuracies
        obtained as a function of $\theta$ for different prediction systems
        $P_x$. At the bottom we have shown the accusation error as a function
        $\theta$ again with one line for each prediction system. We can see that
        as the threshold increases and we accuse more people of cheating the
        accusation error rises.}
    \label{fig:conv-char-NN-pred-50}
\end{figure}

\begin{figure}
    \centering
    \textbf{Prediction system results for 4\% negative dataset, \gls{conv-char-NN}}\par\medskip
    \includegraphics[scale=0.33]{./pictures/experiments/conv_char_nn/prediction_system_04}
    \caption{Results of running the prediction system with the
    \gls{conv-char-NN} on a validation dataset with 96\% positive samples and
    4\% negative samples. In the upper graph we show the accuracies obtained as
    a function of $\theta$ for different prediction systems $P_x$. At the bottom
    we have shown the accusation error as a function $\theta$ again with one
    line for each prediction system. We can see that as the threshold increases
    and we accuse more people of cheating the accusation error rises.}
    \label{fig:conv-char-NN-pred-4}
\end{figure}

The best configuration for both the 50\% and 4\% negative dataset was the
following,

\begin{adjustbox}{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Negatives  & Prediction System & $\theta$ & TP   & TN   & FP  & FN  &
        Acc        & A-Error
        \\ \hline
        50\%       & $P_{lexp_{0.25}}$ & 0.390    & 1843 & 1482 & 517 & 156 &
        0.8316     & 0.095
        \\ \hline
        4\%        & $P_{mv}$          & 0.057    & 1999 & 8    & 74  & 0   &
        0.9644     & 0.000
        \\ \hline
    \end{tabular}
\end{adjustbox}


\subsubsection{Sentence Level Recurrent Neural Network}
\label{subsubsec:rec_sent_nn}

After having attempted a convolutional approach as was just described, we
proceeded with some recurrent experiments as well. Looking at previous
experiments described by \cite{qian:2018} which showed promising authorship
attribution results we considered this a natural next step after our
convolutional approaches. Every \gls{RNN} experiment was performed on the same
data as the \gls{conv-char-NN} and the \textit{Embedding}, \textit{Feature
Extraction}, \textit{Combining} and \textit{Decision} structure was used as
well. The best model we generated is a \gls{rec-sent-NN} and can be seen
depicted in Figure \ref{fig:rec-sent-NN}. After 10 epochs this network peaked
with a validation accuracy of 0.657. We have shown a plot of the training and
validation accuracies during training in Figure \ref{fig:rec-sent-NN-training}.

\begin{figure}
    \centering
    \textbf{Training and Validation Accuracy, \gls{rec-sent-NN}}\par\medskip
    \includegraphics[width=0.5\textwidth]{./pictures/experiments/rec_sent_nn/training}
    \caption{Shows the training and validation accuracies during training of the
    \gls{rec-sent-NN}. We stopped training after we observed that the validation
    accuracy no longer followed the training accuracy.}
    \label{fig:rec-sent-NN-training}
\end{figure}

\begin{description}

    \item[Embedding:]

        Like with the previous network this network is also a siamese neural
        network. It takes two texts as input and outputs the probability that
        they are written by the same author. The network works on the sentence
        level rather than the character level. That clearly has consequences for
        the embedding part of the network.

        The input to the network is given as a sequence of sequences. The outer
        sequence consists of sentences and the inner sequence consisting of the
        words in each sentence. In order for the dimensionality to work the
        amount of words in a sentence is padded or truncated such that they
        consist of 25 words each. This amount which was chosen based on the
        analysis of the training dataset \gls{B}. Contrary to earlier networks
        the embedded representations of these words were not trained as part of
        the network. Instead we used pre-trained embeddings produced by Facebook
        \footnote{\url{https://github.com/facebookresearch/fastText/blob
        /master/ pretrained-vectors.md}} using the method described by
        \citet{bojanowski2016enriching}. That resulted in each word being
        mapped to a 300 dimensional vector. At this point we have a sequence of
        sentences each containing a sequence of 300 dimensional word-embeddings.
        The network then proceeds to take the average of the word sequence
        contained within each sentence, resulting in each sentence being
        represented as the average of its words. We loose some information about
        the sentences when we just average the words but at the same time we
        get a much more compact definition of each sentence. The information
        we loose is the individual words used and the position of those words
        in the sentence. 

    \item[Feature Extraction:]

        The feature extraction part of this network consists of recurrent
        layers. Specifically we used \gls{LSTM} layers described in Section
        \ref{layer:LSTM}. Two \glspl{LSTM} layers are placed right after one
        another. Both of them consists of 50 hidden units and has the flag
        \textit{return\_sequences} set to true. This results in the output
        of all time-steps of the \gls{LSTM} being returned rather than just
        the last one, thus giving us a sequence of outputs. We then proceed
        to combine all the returned sequences using a global average pooling
        layer. Rather than using a simple \gls{ReLu} which was the activation
        function we used on most layers for the reason explained in Section
        \ref{subsubsec:activation_functions} the \gls{LSTM} uses 2 other
        activation functions. It uses the tahn activation function from Equation
        \eqref{eq:tanh} after the last time step and the hard sigmoid activation
        function from Equation \eqref{eq:h_sig} after each individual time-step.
        This configuration is the default of the \gls{LSTM} layer provided by
        keras making the only change to the default parameters the number of
        units in the layer, and the returning of all sequences.

        The idea behind this feature extraction was that we would extract
        50 features from each sentence. We would then take the average over
        the whole text to extract 50 features that represented the author.
        The feature extraction approach is very similar to what is done by
        \citet{qian:2018} who got very good results.

    \item[Combining:]

        Like the networks described in the earlier sections the combination of
        our two siamese paths is done by taking the absolute difference of
        their two outputs, which in this case is global average pooling.

    \item[Decision:]

        The decision part of this network is quite simple. It simply consists
        of a single dense layer of 100 units which is followed by a 30\%
        dropout layer for regularization. The result of the dropout layer is
        then provided to the last dense layer of 2 neurons using the soft-max
        activation function thus mapping it into a probability distribution.
        Since two neurons were used to depict the final output the loss function
        used for the network was categorical cross-entropy. The optimizer chosen
        was \gls{Adam} with the learning rate of 0.0005, 50\% of the default
        value. The reason for this reduced learning rate is the same as in the
        \gls{conv-char-NN}.

\end{description}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./pictures/experiments/rec_sent_nn/model}
    \caption{The structure of the \gls{rec-sent-NN}. The weights are shared
    from the embedding layer all the way down to right before the two paths
    merge. The lambda layer takes each sentence and averages along the first
    axis causing the dimensionality to change from a matrix with 25 rows and
    300 columns to a column vector with 300 elements. The LSTM layers are
    bidirectional meaning its runs through the layer from both directions
    resulting in an output dimensionality twice the unit count of 50.}
    \label{fig:rec-sent-NN}
\end{figure}

The initial network designs for our recurrent networks was based on a
combination of the work done by \citet{qian:2018} and the \gls{conv-char-NN}
network described in Section \ref{subsubsec:conv_char_nn}. We simply replaced
the feature extraction component in the \gls{conv-char-NN} from convolutional
layers to \gls{GRU} layers. In addition that the first \gls{RNN} network
also used the cosine similarity as its method of combination. Like the
previous network this one also worked on the character level. However it was
unfortunately not possible for us to train this network. The first iteration of
the training needed an estimated 140 hours to run through a single epoch. That
was too long to and simply not feasible for us to try in our limited time.

An attempt at saving training time was the inclusion of a convolutional layer
in the feature extraction part of the network. This would be placed before the
recurrent layers in an attempt to minimize the amount of data that was given to
the recurrent layers. The convolution used a window size of 8 and a stride of 8
meaning that each text would be 8 times smaller after the convolutional layer.
This did decrease the runtime but resulted in a validation accuracy of 0.532
at its peak. This was slightly increased when replacing the cosine similarity
with a series of dense layers instead and using a bidirectional \gls{LSTM} layer
rather than a single \gls{GRU} layer.

Instead of using convolutions to limit the text size we also tried changing the
linguistic level our network operated at. Again inspired by \cite{qian:2018}
we elevated our network from using the characters of the texts to using the
sentences. This sentence representation was described in the \textbf{Embedding}
part above. This approach significantly decreased the training time to around
$1.5$ hours. But it also revealed the reason why a recurrent network might not
be the best choice for a task like this. The training accuracy increased much
more than for the convolutional networks while the validation accuracy quickly
stalled. It therefore seemed like this approach was very prone to overfitting on
particular authors. This lead to the final network described above. This network
included an extra bidirectional \gls{LSTM} layer, and the 30\% dropout for extra
regularization.

We also chose the best prediction system $P_x$ and best threshold $\theta$ for
this network. We tried the same thresholds and prediction systems as before.
The results can be seen in the Figures, \ref{fig:rec-sent-NN-pred-50} and
\ref{fig:rec-sent-NN-pred-4}

\begin{figure}
    \centering
    \textbf{Prediction system results for 50\% negative dataset, \gls{rec-sent-NN}}\par\medskip
    \includegraphics[scale=0.33]{./pictures/experiments/rec_sent_nn/prediction_system_50}
    \caption{Results of running the prediction system with the \gls{rec-sent-NN}
    on a validation dataset with 50\% positive samples and 50\% negative
    samples. In the upper graph we show the accuracies obtained as a function
    of $\theta$ for different prediction systems. At the bottom we have shown
    the accusation error as a function $\theta$ again with one line for each
    prediction system. We can see that as the threshold increases and we accuse
    more people of cheating the accusation error rises.}
    \label{fig:rec-sent-NN-pred-50}
\end{figure}

\begin{figure}
    \centering
    \textbf{Prediction system results for 4\% negative dataset, \gls{rec-sent-NN}}\par\medskip
    \includegraphics[scale=0.33]{./pictures/experiments/rec_sent_nn/prediction_system_04}
    \caption{Results of running the prediction system with the \gls{rec-sent-NN}
    on a validation dataset with 96\% positive samples and 4\% negative samples.
    In the upper graph we show the accuracies obtained as a function of $\theta$
    for different prediction systems. At the bottom we have shown the accusation
    error as a function $\theta$ again with one line for each prediction system.
    We can see that as the threshold increases and we accuse more people of
    cheating the accusation error rises.}
    \label{fig:rec-sent-NN-pred-4}
\end{figure}

The best configurations for the \gls{rec-sent-NN}, using both the 50\% and 4\%
negative dataset was the following,

\begin{center}
    \begin{adjustbox}{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Negative  & Prediction System & $\theta$ & TP   & TN  & FP   & FN  &
        Acc       & A-Error
        \\ \hline
        50\%      & $P_{lexp_{0.25}}$ & 0.033    & 1965 & 310 & 1689 & 34  &
        0.5690    & 0.099
        \\ \hline
        4\%       & $P_{lexp_{0.25}}$ & 0.002    & 1999 & 1   & 79   & 0   &
        0.9620    & 0.000
        \\ \hline
    \hline
    \end{tabular}
    \end{adjustbox}
\end{center}

These result are to expected when looking at the graphs. The immediate spike in
accusation error leaves the model very little room to increase the accuracy in.
This is most likely the result of the aforementioned author specific learning
the network does.


\subsubsection{Character and Word Level Convolutional Siamese Neural Network}
\label{subsubsec:conv_char_word_nn}

The last network we produced is what we call the \gls{conv-char-word-NN}. This
network is similar to the \gls{conv-char-NN} as it is also convolutional. This
network was an attempt at expanding on the successful architecture of the
\gls{conv-char-NN} by using looking at words in addition to characters. Applied
to the same data as the previous networks, we got a maximum accuracy of 0.697 on
the second epoch. We have shown the training and validation accuracies in Figure
\ref{fig:conv-char-word-NN-training}. The design of the network in the different
parts was as follows.

\begin{figure}
    \centering
    \textbf{Training and Validation Accuracy, \gls{conv-char-word-NN}}\par\medskip
    \includegraphics[width=0.5\textwidth]{./pictures/experiments/conv_char_word_nn/training}
    \caption{Training and validation accuracies of the \gls{conv-char-word-NN}
        during training. We stop training when the validation accuracy no longer
        follow the training accuracy.}
    \label{fig:conv-char-word-NN-training}
\end{figure}

\begin{description}

    \item[Embedding:]

        Like in the \gls{conv-char-NN} the embedding layers takes in a text as
        a stream of characters where they are then mapped to a 5 dimensional
        vector. The difference is however that we also added a word channel.
        It uses the pre-trained Facebook word embeddings mentioned earlier
        to map each word of the text to a 300 dimensional vector. In the
        \gls{rec-sent-NN} these embeddings were averaged to represent a sentence
        but in this network the embeddings are used as is. This is the basic
        idea of this network a multi-channeled convolutional approach. Thus the
        network does not take 2 inputs in the form of two texts, it takes 4
        inputs, a character input for each text and a word input for each text.
        Both of these channels have their own path through the network with
        their own convolutional layers.

    \item[Feature Extraction:]

        The feature extraction in this network differs between the character
        network and the word network. Like previously, we make use of two
        different convolutional layers in the character network. They have a
        sliding window size of 8 and 4 respectively, and a filter size of 200.
        This combination of window sizes had proven to work quite well in the
        \gls{conv-char-NN}. Therefore we saw no reason to change it as the
        creation of this network was done in an additive manner. The embedded
        stream of words are given to its own convolutional layer with a sliding
        window size of 8, and a filter size of 100. The idea with the new word
        channel is that the network will be able to learn which words and author
        use and if an author has a prevalence for certain word sequences.

    \item[Combining:]

        Due to the addition of the extra word input the combining part of
        the network has an additional step. The network initially combine
        the output of both the word and the character sub-networks. This is
        done by concatenating the output of the two convolutional layers in
        the character network, and the single convolutional layer in the word
        network. This results in two concatenated sets one for each of the two
        texts. The elementwise absolute difference of the two sets are then
        computed as in the other networks, and passed along to the decision part
        of the network.

    \item[Decision:]

        The decision part of this network is simply 2 fully connected layers
        consisting of 500 units followed by a 0.3 dropout layer. The output of
        that dropout layer is then given to a 2 unit soft-max layer for the
        final decision.

\end{description}

We have the network structure in Figure \ref{fig:conv-char-NN-model}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./pictures/experiments/conv_char_word_nn/model}
    \caption{The structure of network the \gls{conv-char-word-NN}. The weights are
        shared from the embedding layer all the way down to right before
        the four paths merge. The texts are given as both a sequence of
        characters and as a sequence of words. That allows the network to look
        at both. Different convolutions are trained for characters and words.}
    \label{fig:conv-char-NN-model}
\end{figure}

The design of this network was derived from the \gls{conv-char-NN}. The
convolutional approach used in the \gls{conv-char-NN} showed promise and this
network was an attempt to improve on that.

Initially these attempted improvements were small. They took the form of a
different amount of convolutional layers with a differing amount of sliding
window sizes. These small changes also included the addition of regularization
measures, such as dropout layers. The largest change came when we incorporated
the multi-channel approach seen in \cite{DBLP:journals/corr/RuderGB16c}. The
thought process was that if characters was a good features for this task another
channel could maybe provide better results. This was what yielded the network
described above which works on the character level and the word level. A 3
channel convolutional network was also produced during experimentation. Results
from that seemed to indicate that the linguistic level of sentences was too high
thus hurting the model. We also experimented with using stacked convolutional
layers for the feature extraction also with bad results.

Similarly to the two other networks we found the best prediction system and
tuned the threshold for this network. We have shown the accuracy and accusation
error over different thresholds for the dataset containing 50\% negatives in
Figure \ref{fig:conv-char-word-NN-pred-50} and for the dataset containing 4\%
negatives in Figure \ref{fig:conv-char-word-NN-pred-4}.

\begin{figure}
    \centering
    \textbf{Prediction system results for 50\% negative dataset, \gls{conv-char-word-NN}}\par\medskip
    \includegraphics[scale=0.33]{./pictures/experiments/conv_char_word_nn/prediction_system_50}
    \caption{Results of running the prediction system with the
        \gls{conv-char-word-NN} on a validation dataset with 50\% positive
        samples and 50\% negative samples. In the upper graph we show the
        accuracies obtained as a function of $\theta$ for different prediction
        systems. At the bottom we have shown the accusation error as a function
        $\theta$ again with one line for each prediction system. We can see that
        as the threshold increases and we accuse more people of cheating the
        accusation error rises.}
    \label{fig:conv-char-word-NN-pred-50}
\end{figure}

\begin{figure}
    \centering
    \textbf{Prediction system results for 4\% negative dataset, \gls{conv-char-word-NN}}\par\medskip
    \includegraphics[scale=0.33]{./pictures/experiments/conv_char_word_nn/prediction_system_04}
    \caption{Results of running the prediction system with the
        \gls{conv-char-word-NN} on a validation dataset with 96\% positive
        samples and 4\% negative samples. In the upper graph we show the
        accuracies obtained as a function of $\theta$ for different prediction
        systems. At the bottom we have shown the accusation error as a function
        $\theta$ again with one line for each prediction system. We can see that
        as the threshold increases and we accuse more people of cheating the
        accusation error rises.}
    \label{fig:conv-char-word-NN-pred-4}
\end{figure}

The best configuration for the 50\% and 4\% negative datasets was the
following,

\begin{center}
    \begin{adjustbox}{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Negative  & Prediction System & $\theta$ & TP   & TN   & FP  & FN  &
        Acc       & A-Error
        \\ \hline
        50\%      & $P_{lexp_{0.25}}$ & 0.433    & 1856 & 1302 & 697 & 143 &
        0.7898    & 0.099
        \\ \hline
        4\%       & $P_{exp_{0.25}}$  & 0.127    & 1999 & 9    & 73  & 0   &
        0.9649    & 0.000
        \\ \hline
    \end{tabular}
    \end{adjustbox}
\end{center}


\subsection{Summary}

In this section we described the experiments we performed. We started by
explaining how we selected the features our baselines work on. After that we
described how we performed the parameter selection for the baseline methods. We
then presented three different network architectures. Two convolutional and one
recurrent. The networks worked on different linguistic layers,

\begin{description}
    \item[\gls{conv-char-NN}] The character level,
    \item[\gls{rec-sent-NN}] The sentence level,
    \item[\gls{conv-char-word-NN}] The word and the character level.
\end{description}

For each of the networks we used a validation dataset to determine the best
configurations for the prediction system. A summarizing view of the
validation performance of the different networks can be seen in Table
\ref{tab:experiment-validation-results}.

\begin{table}[h]
    \begin{adjustbox}{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
        \hline
        Negative               & Network                 & Prediction System &
        $\theta$               & TP                      & TN                &
        FP                     & FN                      & Acc               &
        A-Error
        \\ \hline
        \multirow{3}{*}{50\%}  & \gls{conv-char-NN}      & $P_{lexp_{0.25}}$ &
        0.390                  & 1843                    & 1482              &
        517                    & 156                     & 0.8316            &
        0.095
        \\
        \cline{2-10}           & \gls{rec-sent-NN}       & $P_{lexp_{0.25}}$ &
        0.033                  & 1965                    & 310               &
        1689                   & 34                      & 0.5690            &
        0.099
        \\
        \cline{2-10}           & \gls{conv-char-word-NN} & $P_{lexp_{0.25}}$ &
        0.433                  & 1856                    & 1302              &
        697                    & 143                     & 0.7898            &
        0.099
        \\ \hline
        \multirow{3}{*}{4\%}   & \gls{conv-char-NN}      & $P_{mv}$          &
        0.057                  & 1999                    & 8                 &
        74                     & 0                       & 0.9644            &
        0.000
        \\
        \cline{2-10}           & \gls{rec-sent-NN}       & $P_{lexp_{0.25}}$ &
        0.002                  & 1999                    & 1                 &
        79                     & 0                       & 0.9620            &
        0.000
        \\
        \cline{2-10}           & \gls{conv-char-word-NN} & $P_{exp_{0.25}}$  &
        0.127                  & 1999                    & 9                 &
        73                     & 0                       & 0.9649            &
        0.000
        \\ \hline
    \end{tabular}
    \end{adjustbox}
    \caption{The performance of all neural networks we have implemented on the
        validation dataset.}
    \label{tab:experiment-validation-results}
\end{table}
