\section{Experiments} \label{sec:experiments}


\subsection{Baseline Methods}

In this section we will go through the experimentation performed in order to get
our baseline results. While many different features can be used when performing
\gls{NLP}, we chose made use of the same features, as was used in \cite{US},
since they spanned several several of the different linguistic layers. The
features available to pick from are:

\begin{itemize}
    \item Word-n-grams,
    \item Character-n-grams,
    \item Word Frequencies (word-1-grams),
    \item \gls{POS}-tag-n-grams, and
    \item Special-Character-n-grams,
\end{itemize}

where an n-gram describes the combinations of sequential elements of size n. In
the case where that element is characters, and n is 3, the string "hello" would
produce the character-3-grams "hel", "ell" and "llo". This also means that a
feature such as word frequencies can be considered word-1-grams.

For all of these experiments a third party corpus, provided by
\texttt{NLTK}\footnote{\url{http://www.nltk.org/index.html}}, was used as the
basis for all the feature extraction, as was done in \cite{US} as well. This
corpus consists of 22,476 sentences, and 563,358 words.


\subsubsection{Extended Delta Method}

A large part of the experiments performed when applying the extended delta
method was parameter tuning. The extended delta method, does not do any
mitigation of noisy data or feature selection it self. Thus we would have
to do the feature selection ourselves. This was a problem as the amount of
combinations of features and opposing authors we would have to consider in
order to find the best performing ones, would take a lot of time.

Rather than performing a random selection of features as was done in \cite{US},
we chose to use a more systematic approach. We opted for a greedy approach. It
starts out with a large, but still limited, set of features from which we would
then remove the feature, whose removal yielded the best accuracy on a separate
validation set. This is done until the removal of any feature doesn't improve
the validation accuracy.

In order for us to determine this starting feature set, we had a look the
amount of different feature there was available when looking at the corpus.

\begin{itemize}
    \item Word-2-grams - 188,472
    \item Character-2-grams - 2,178
    \item Word Frequencies - 27,535
    \item \gls{POS}-tag-2-grams - 219
    \item Special-character-2-grams - 524
\end{itemize}

These numbers are of course inflated due to the size of the corpus relative
to the actual text we are training on, and will continue to increase as we
increase the n in the n-grams. As a result of this, the frequency of each n-gram
decreases, and so does the impact each individual n-gram has. For this reason
we choose to only focus on the K most frequent instances of each feature, which
also serves to limit the parameter search space.

Using the listed quantities, a feature set of 8,600 features was produced,
containing

\begin{itemize}
    \item the 500 most frequent words,
    \item the 500 most frequent word-n-grams for $n \in {2,...,10}$,
    \item the 300 most frequent character-n-grams for $n \in {2,...,10}$,
    \item the 50 most frequent \gls{POS}-tag-n-grams for $n \in {2,...,10}$, and
    \item the 50 most frequent special-character-n-grams for $n \in {2,...,10}$.
\end{itemize}
TODO: Will probably need more limiting.

It is using this feature set we then apply our greedy deleting too. In order
to determine the best combination of features. In addition to selecting the
features, the number of opposing author was also a parameter that needed tuning.
Thus, this greedy approach was also performed on different configurations of
opposing authors $A$, where $A \in TODO$
