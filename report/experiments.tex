\section{Experiments} \label{sec:experiments}


\subsection{Baseline Methods}

In this section we will go through the experiments performed in order to get
our baseline results. While many different features can be used when performing
\gls{NLP}, we chose the same features, as was used in \cite{US}, since they
span several several of the different linguistic layers. The linguistic
layers are different areas which all serve to describe a certain text. An
example this could be the character level layer, which as the name implies,
focuses on the individual characters used in a specific text. Other examples
of this, could the sentence, that focuses on the sentences of the text,
and the meta layer, that focuses on things such as publishing/delivery date,
and file format. The features available to pick from in this case are:

\begin{itemize}
    \item Word-N-grams,
    \item Character-N-grams,
    \item Word Frequencies (word-1-grams),
    \item \gls{POS}-tag-N-grams, and
    \item Special-Character-N-grams,
\end{itemize}

Where an N-gram describes the combinations of sequential elements of size n. In
the case where that element is characters, and n is 3, the string "hello" would
produce the character-3-grams "hel", "ell" and "llo". This also means that a
feature such as word frequencies can be considered word-1-grams.

For all of these experiments a Danish third party corpus, provided by
\texttt{NLTK}\footnote{\url{http://www.nltk.org/index.html}}, was used as the
basis for all the feature extraction, as was done in \cite{US} as well. This
corpus consists of 22,476 sentences, and 563,358 words.


\subsubsection{Extended Delta Method}

A large part of the experiments performed when applying the extended delta
method was parameter tuning. The extended delta method, does not do any
mitigation of noisy data or feature selection it self. Thus we would have
to do the feature selection ourselves. Not only that, but contrary to the
circumstances our previous paper \cite{US} was written in, we have more than
one text per author. As such, we don't need the opposing author approach any
more. When given a text $x$ and a proposed author $\alpha$, we pick out all
text written that author $T(\alpha)$, and a set different authored texts
$T_{diff}(\alpha)$ of same length. Using those sets, we create a \gls{KNN}
model, and apply it to $x$, returning 1 if $x \in T(\alpha)$ and zero otherwise.
Additionally we also apply the model to the negative case, where we pick text
randomly from the training-set, returning 1 if this new text is predicted as
being in $T_{diff}(\alpha)$ and 0 otherwise.

Thus we need to tune the K in \gls{KNN}, instead of the number of opposing
author that was used in \cite{US}

So the parameter selection consists of finding the best features, and the best
K.

Rather than finding the best features, using the somewhat random approach used
in our previous paper\cite{US}, we chose to do it more systematically this time
around. This however proved a greater task than anticipated. In order to find
the best set of feature, one would have to extract a very large set of features,
thousands in our case and determine the best combination of those features. A
task like that would take an incredible amount of computing power, and time.
Thus, we opted for a forward greedy selection approach.

We initially wanted to extract of large set of features, to perform our greedy
selection on. The corpus used had the following quantities of the different
features.

\begin{itemize}
    \item Word-2-grams - 188,472
    \item Character-2-grams - 2,178
    \item Word Frequencies - 27,535
    \item \gls{POS}-tag-2-grams - 219
    \item Special-character-2-grams - 524
\end{itemize}

These numbers are of course inflated due to the size of the corpus relative
to the actual text we are training on, and will continue to increase as we
increase the N in the N-grams. As a result of this, the frequency of each N-gram
decreases, and so does the impact each individual N-gram has. For this reason we
choose to only focus on a subset most frequent instances of each feature, which
also serves to limit the parameter search space.

Using the quantities listed above as inspiration, a feature set of 8,600
features was produced, containing

\begin{itemize}
    \item The 500 most frequent words,
    \item The 500 most frequent word-N-grams for $N \in \{2,3,4\}$,
    \item The 300 most frequent character-N-grams for $N \in \{2,...,10\}$,
    \item The 50 most frequent \gls{POS}-tag-N-grams for $N \in \{3,4\}$, and
    \item The 50 most frequent special-character-N-grams for $N \in \{2,3,4\}$.
\end{itemize}

The selection the quantities, bases on the number available in the corpus,
as listed earlier. As the for the N in the N-grams, \cite{aalykke2016} found
out that char-8-grams worked very well in his case, which is why N goes all
the way to 10 in the case of char-N-grams . Looking at the results
from \cite{US}, Word-N-grams mostly stopped occurring in texts when getting
to 4-grams, when looking at the result from \cite{US}. A similar thing
could be seen with the special-character-N-grams, where an increase in N,
wouldn't contribute anything. \gls{POS}-tag-N-grams were a really big burden
computationally, as such a reduction in possible N-values, had to be made.

These features were extracted from subset of the data-set described earlier
in section \ref{sec:data}. However it wasn't beholden to the same upper limit
character and unique character constraint, as the lack of memory isn't nearly
as severe as in the case of our \gls{NN}s. This data-set contained 14646
entries. The distribution of the texts over authors in this data-set isn't
very evenly spread. As such, we removed some data-entries, so $$\{\forall \alpha
\in \mathcal{A}: |T(\alpha)| = T_{min}\}$$ Where $T_{min}$ is the smallest amount
of texts associated with one author, in the case 4. Doing this decreased the
data-set entry quantity to 3748, with 937 different authors.

Having the features, we could then proceed with the greedy algorithm, which
works as follows. It starts out only using the most frequent of each of
the different features. Using this set it loops through each $\alpha \in
\mathcal{A}$ applying the extended delta methods, as described earlier in this
section. After each iteration the next, most frequent of each of the feature
is added to our active set of features, and then the extended delta method is
applied again. For each of these iteration, the best result is saved. If the
result doesn't improve for three iterations, or no more features are available,
the saved best result is returned. As each feature only has a certain of amount
of most frequent values, the greedy algorithm will reach a point where it want
to take grab the next most frequent feature, but it isn't available. In this
case, simple stop taking entries from that feature, but we keep taking from the
others. This greedy approach was applied using different configurations of the
N in KNN and distance metric, with $N \in {1,...,7}$, and the distance metric
being either Manhattan or Euclidean.

The results of the selection can be seen in figures \ref{fig:resultsMan} and 
\ref{fig:resultsEuc}. 

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    KNN & Frequent Feature Count & Result          \\ \hline
    1   & 12                     & \textbf{0.7705} \\ \hline
    2   & 10                     & 0.6927          \\ \hline
    3   & 6                      & 0.6742          \\ \hline
    4   & 2                      & 0.6378          \\ \hline
    5   & 13                     & 0.6391          \\ \hline
    6   & 2                      & 0.6053          \\ \hline
    7   & 10                     & 0.5989          \\ \hline
    \end{tabular}
    \caption{Results of the forward greedy selection of the extended delta
        method using the Manhattan distance. The point where a single feature
        had no more entries to pick from, was never reached. As such Frequent
        Feature Count describes how many most frequent entries were picked out
        from each feature.}
    \label{fig:resultsMan}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    KNN & Frequent Feature Count & Result          \\ \hline
    1   & 10                     & \textbf{0.7772} \\ \hline
    2   & 13                     & 0.6990          \\ \hline
    3   & 3                      & 0.6791          \\ \hline
    4   & 4                      & 0.6499          \\ \hline
    5   & 2                      & 0.6288          \\ \hline
    6   & 2                      & 0.6065          \\ \hline
    7   & 2                      & 0.5959          \\ \hline
    \end{tabular}
    \caption{Results of the forward greedy selection of the extended delta
        method using the Euclidean distance. The point where a single feature
        had no more entries to pick from, was never reached. As such Frequent
        Feature Count describes how many most frequent entries were picked out
        from each feature.}
    \label{fig:resultsEuc}
\end{table}

As such, the best configuration is Euclidean distance, with the 10 most frequent
of the different features, is what we will be using on the test data.

\subsubsection{Author Specific SVM}

The author specific SVM is a authorship verification method that learns from
hand crafted features. Similar to the Extended Delta method we perform a feature
search in a large set of features to find the optimal features for the SVM. To
evaluate different feature sets and different hyperparameters we use the
algorithm shown in Figure \ref{fig:svm_evaluation_algorithm}. The algorithm
computes the average accuracy of the SVM via leave one out cross validation. For
each author it takes a set of texts written by that author and a set of texts of
similar size written by another author. Then it successively leaves out one of
the texts, train an SVM on the rest and then use that SVM to predict the class
of the text that was left out. The score is then the average accuracy of that
prediction.

\begin{figure}
    \begin{algorithm}[H]
        \KwData{$A$ a set of authors, $F$ a set of features, $C$ SVM
        hyperparameter, $\gamma$ SVM hyperparamter}
        Let $S = \emptyset$\;
        \For {$\alpha \in A$} {
            Let $T_k = T(\alpha)$\;
            Let $T_u = \{
                x_i\ |\ x_i \in \mathcal{U}\left(
                    \overline{T}(\alpha)
                \right), 1 \leq i \leq \left| T_k \right|
            \}$\;
            \label{line:draw_opposition}
            Let $F_k = \{ f(t, F)\ |\ t \in T_k\}$\;
            \label{line:feature_extract_1}
            Let $F_u = \{ f(t, F)\ |\ t \in T_u\}$\;
            \label{line:feature_extract_2}
            \For {$x \in F_k \cup F_u$} {
                Let $m = SVM(F_k \setminus x, F_u \setminus x, C, \gamma)$\;
                \uIf {$m(x) = a(x)$} {
                    Let $S = S \cup \{1\}$\;
                } \uElse {
                    Let $S = S \cup \{0\}$\;
                }
            }
        }
        \Return {$\mu(S)$}
        \caption{SVM evaluation}
    \end{algorithm}
    \caption{Algorithm to evaluate an SVM on a set of authors. The algorithm is
        given a set of authors $A$ to evaluate over, a set of features $F$ to
        use and the hyperparameters for the SVM. On Line
        \ref{line:draw_opposition} $\mathcal{U}$ denotes the uniform
        distribution and $T_u$ is a set of texts of the same size as $T_k$ where
        each text in the set is written by a different author than $\alpha$. On
        Line \ref{line:feature_extract_1} and Line \ref{line:feature_extract_2}
        $f$ denotes a function that computes the features $F$ of the text $t$.
        The algorithm computes the average accuracy of an SVM with the paramters
        given over all authors given.}
    \label{fig:svm_evaluation_algorithm}
\end{figure}

The search in a set of features $F$ are performed by starting with an
empty set of features $FC$. Then in each iteration we add a single
$f \in F$ to $FC$ and evaluate the SVM with the algorithm in Figure
\ref{fig:svm_evaluation_algorithm}. At the end we add only the single feature
that gives the best performance in the evaluation algorithm and repeat again
with the rest of the features. The feature set we search in are the same feature
set as the feature set used in the Extended Delta Method but without limiting
the number of texts per authors. The search are performed on only 100 random
authors in the set as it otherwise takes to long. The feature search is done
with fixed SVM hyperparameters which we tune afterwards. The hyperparameters
used during the search was $C = 100$ and $\gamma = 1000$, and using the default
rbg kernel. The feature search surprisingly found only 6 features before no
features could improve the result further. The specific features were the
character-2-gram " -", the character-3-gram ", o", the character-4-gram ", at",
the character-4-gram ", so", the character-6-gram ", der " and the word-1-gram
"giver". Most of the features seem to look at what authors are writing
immediately following commas. We found the optimal hyper parameters for the SVM
by splitting the dataset in two parts. 80\% of the authors went in the training
part and 20\% of the authors went in the validation part. Then for each author
in the training dataset we varied $C \in (10^{-2}, 10^0, \dots, 10^{10})$ and
$\gamma \in (10^{-9}, 10^{-7}, \dots, 10^3)$ and evaluated an SVM using the
algorithm in Figure \ref{fig:svm_evaluation_algorithm} with the fixed feature
set for each combination of hyperparameters. After that we took a majority
vote between the authors chosen hyperparameters and chose that as the final
hyperparameters. The final hyperparameters were $C = 100$ and $\gamma = 1000$.
We then evaluated our chosen parameters both feature set and hyperparameters on
the validation set. We obtained an accuracy of 0.62980.

Since we were surprised that only 6 features were used we also tried to take a
feature set of the 50 most frequent features in each category. We again ran the
search for hyperparameters. This time we found that $C = 100$ and $\gamma = 10$
worked the best. We obtained an accuracy of 0.69552. So we were able to obtain a
higher accuracy by arbitrarily choosing the 50 most frequent features instead of
doing a structured search.

TODO: Try using all features.


\subsection{Siamese Networks}

The main attraction of our authorship verification task, is the implementation
and testing of different \gls{NN}. In the following sections we will go through
the different iterations of networks we have produced. A glossary of the
different layers used can be seen in figure \ref{glossary}. Throughout out
experiments, we used a 0.5 threshold for classification.

\begin{landscape}
\begin{table}[]
\centering
\caption{Glossary used when performing experiments, and creating their associated models.\cite{chollet2015keras}}
\label{glossary}
\begin{tabular}{|c|c|l|}
\hline
\textbf{Layer}     & \textbf{Description}                                                                                                                                                                        & \multicolumn{1}{c|}{\textbf{Actively Used Parameters}}                                                                                                                                                                                                                                                                                         \\ \hline
Input        & \begin{tabular}[c]{@{}c@{}}Serves as the entrypoint of the network, \\ by receiving a set of texts and feeding it trough the network.\end{tabular}                                          & \textbf{Shape} - The shape of each sample give to the input.                                                                                                                                                                                                                                                                                            \\ \hline
Embedding          & \begin{tabular}[c]{@{}c@{}}Taking in an encoded sample, \\ it produces a dense vector representation for each different. \\ More details can be found in section *TODO:Section Reference*\end{tabular}                                                            & \begin{tabular}[c]{@{}l@{}}\textbf{Input Dim} - Size of vocabulary.\\\textbf{Output Dim} - Size of vector used to represent embedding.\end{tabular}                                                                                                                                                                                                             \\ \hline
Convolutional      & \begin{tabular}[c]{@{}c@{}}Applies convulution to the data it \\ recieves according to the decription, \\ found in section *TODO:Section Reference*\end{tabular}                            & \begin{tabular}[c]{@{}l@{}}\textbf{Filters} - Dimensionality of the output, \\ ie. number of filter from the convolution. \\ \textbf{Kernel Size} - Integer or list describing \\ size of convolution window.\\ \textbf{Strides} - Stride length of the \\ convolutional window.\\ \textbf{Activation} The activation function to be \\ applied after the convolution.\end{tabular} \\ \hline
Global Max Pooling & \begin{tabular}[c]{@{}c@{}}Extracts the maximum value \\ from each of the provided data\end{tabular}                                                                                        & Has no parameters.                                                                                                                                                                                                                                                                                                                             \\ \hline
Concatenation      & \begin{tabular}[c]{@{}c@{}}As the name suggests, this layers simply \\ concatenates the data it receives from different layers\end{tabular}                                                 & Has no parameters.                                                                                                                                                                                                                                                                                                                             \\ \hline
Merge              & \begin{tabular}[c]{@{}c@{}}Merges its inputs, \\ using a specified function to generate a single output.\end{tabular}                                                                       & \textbf{Function} - The function used to merge the recieved data.                                                                                                                                                                                                                                                                                       \\ \hline
Dense              & \begin{tabular}[c]{@{}c@{}}A simple fully connected layer, \\ taking in data and applying \\ the function described in Section \ref{sec:neurons}\end{tabular}                               & \begin{tabular}[c]{@{}l@{}}\textbf{Units} - Number of neurons in in the layer.\\ \textbf{Activation} - The activation function to be applied.\end{tabular}                                                                                                                                                                                                       \\ \hline
Dropout            & \begin{tabular}[c]{@{}c@{}}Drops a fraction it receives, \\ with the goal of preventing overfitting.\end{tabular}                                                                            & \textbf{Rate} -The fraction of data it receives dropped.                                                                                                                                                                                                                                                                                                \\ \hline
Lambda             & Applies a specified function to the input it receives                                                                                                                                       & \textbf{Function} - The function applied.                                                                                                                                                                                                                                                                                                               \\ \hline
Reshape            & Reshapes the data it receives                                                                                                                                                               & \textbf{Dim} - Dimensionality to reshape to.                                                                                                                                                                                                                                                                                                            \\ \hline
GRU                & \begin{tabular}[c]{@{}c@{}}A gated recurrent unit is applied to the data \\ according to the description \\ found in *TODO: Section Reference*\end{tabular}                                 & \textbf{Unit} - Number of neurons in the layer.                                                                                                                                                                                                                                                                                                        \\ \hline
LSTM               & \begin{tabular}[c]{@{}c@{}}A Long Short-Term Memory layer, \\ which works according to the description in \\ section *TODO: Section Reference" is applied \\ to the given data\end{tabular} & \textbf{Unit} - Number of neurons in the layer.                                                                                                                                                                                                                                                                                                        \\ \hline
\end{tabular}
\end{table}
\end{landscape}

\subsubsection{Siamese Neural Network - Iteration 1}

The network we used to try and solve the problem is shown in Figure
\ref{fig:network_1}. The Siamese part of the network is the Convolutional
Layer. We used 1000 filters of size 10. That means that 1000 different features
are supposed to be learned by the network and each feature can use a local
context of 10 characters to extract a feature. After the convolution we have a
max-over-time pooling layer. The layer takes the maximum value of each feature
such that we have 1000 features from each text. We then have a normal dense
neural network on top of that which are given the features of both texts. The
dense network is then supposed to learn how to compare the features from the two
texts. We have 1 dense hidden layer each with 500 neurons. At the end we have
a output layer with two outputs. The activation function for all layers except
the last one is the rectified linear unit and the activation function of the
last layer is the softmax function. The output of the network is a probability
distribution over the two classes.

The input to the network first goes through an embedding layer. The embedding
layer transforms integers into dense vectors of floating point numbers. The
embedding layer functions as a lookup table such that each integer is mapped
to the same dense vector. The embedding is trainable meaning that better
embeddings will be learned while the network is training.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.6\textwidth]{./pictures/experiments/network1.png}
    \caption{Illustrate the structure of our first Siamese Neural Network
        Architecture.}
    \label{fig:network_1}
\end{figure}

The network obtained a validation accuracy of 0.68684. We have shown both the
training and validation accuracies in the different epochs in Figure
\ref{fig:network1_accuracies}. In that plot we can see that the network very
quickly overfits the training data and no longer learns anything general.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{./pictures/experiments/network_1_accuracies.png}
    \caption{Training and validation accuracies for the iteration 1 network in
        the different epochs of the networks execution.}
    \label{fig:network1_accuracies}
\end{figure}


\subsubsection{Siamese Neural Network - Iteration 2}

% TODO: Illustrate how dropout corresponds to training different subnetworks in
% each iteration.

In our last network we observed that the network after a few number of epochs
overfitted on the training dataset. The validation accuracy quickly began
stalling as the training accuracy went to 100\%. We therefore focused our
second network architecture on limiting overfitting. We added a dropout layer
before the output layer. The layer prevents overfitting by making sure that
the network cannot rely on the output of all neurons during training. This
unpredictability makes it harder for the network to learn specific quirks of
the training dataset since small quirks will only be picked up by a small
amount of neurons while larger trends will be represented by a larger set
of neurons. \cite{JMLR:v15:srivastava14a} investigated the use of dropout
layers in different problem settings. They found that dropout layers reduced
overfitting in all problems they looked at. In particular they found that
document classification which is similar to what we are doing were also
improved. The main drawback of a dropout layer is that it increase the run time
of training the networks (\cite{JMLR:v15:srivastava14a}).

In iteration 1 the function we used to merge features from the known and unknown
text together were a concatenation. If we let the extracted features of the
known text be $K_i$ and the extracted features of the unknown text be $U_i$ then
$K_0$ and $U_0$ correspond to the same feature extracted from the two texts. The
concatenation merge function would then produce,

\begin{equation}
    merge(K, U) \rightarrow \left(
        K_0, K_1, \dots, K_n, U_0, U_1, \dots, U_n
    \right)^T.
\end{equation}

That means that the neural network would have to figure out by itself that input
$0$ were related in particular to input $n + 1$. To save the network that task
we also replaced the merging function to the absolute difference of the feature
vectors. That means that our merge function became,

\begin{equation}
    merge(K, U) \rightarrow \left(
        (|K_0 - U_0|), (|K_1 - U_1|), \dots, (|K_n - U_n|)
    \right)^T.
\end{equation}

So the network no longer has to learn arbitrary indexes. Instead it can learn
which features is important for authorship verification and learn thresholds for
when each feature is important. With the new merging we will have a large number
whenever the two features are far apart and a small number whenever they are
close to each other.

We changed the convolutional filters we used from 1000 convolutional filters of
size 10 to 500 convolutional filters of size 4 and 500 convolutional filters of
size 8. The idea was that the filters could learn different features where some
of the features would consist of a large number of characters and other of the
features would consist of a small number of characters. The architecture of our
network from iteration 2 is shown in Figure \ref{fig:network_2}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./pictures/experiments/network2.png}
    \caption{Illustrate the structure of our second Siamese Neural Network
        Architecture.}
    \label{fig:network_2}
\end{figure}

We also added more dense layers to the model. A plot of the
training and validation accuracies per epoch can be seen in Figure
\ref{fig:network2_accuracies}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{./pictures/experiments/network_2_accuracies.png}
    \caption{Shows the training and validation accuracies on the second
        network.}
    \label{fig:network2_accuracies}
\end{figure}

The maximum validation accuracy obtained were 0.74464 in epoch 12.


\subsubsection{Siamese Neural Network - Iteration 3}

In iteration 2 we observed that the network seemed to learn everything in the
first epoch and didn't improve much after that. We also observed that the
accuracy improved compared to the first network. We therefore wanted to try a
larger network which would be able to hopefully learn more and obtain a higher
accuracy than our first two networks.

We both tried adding more convolutions and adding more dense layers. Our third
network use 700 convolutions of size 8 (currently drawn confusingly) and 500
convolutions of size 4. The network then contains 4 dense layers all with 500
hidden neurons with \gls{ReLu} activation function. Then a dropout layer with
30\% dropout and finally the output layer with 2 neurons and a softmax
activation function. The function we use to combine the features from the two
texts are still the absolute difference as that seemed to work fine for the
previous network. We have shown the structure of the third network in Figure
\ref{fig:network3}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./pictures/experiments/network3.png}
    \caption{Illustrate the structure of our third Siamese Neural Network.
        Weights are shared by the embedding layers and the convolutional
        layers.}
    \label{fig:network3}
\end{figure}

Our third network were able to obtain an accuracy of 0.83612. We have shown the
training and validation accuracies in Figure \ref{fig:network_3_accuracies}. We
can see that the network quickly improve to about 80\% accuracy and after that
not much happens.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{./pictures/experiments/network_3_accuracies.png}
    \caption{Shows the training and validation accuracies over the epochs of
        training on the third network.}
    \label{fig:network_3_accuracies}
\end{figure}

To get an idea of which features the network were looking at we looked at the
output of the convolutional layers. After the convolutional layers we have a
max-over-time pool so we know that higher values are important. We could then
take a text, feed it to the network, find the index of the maximum output of
the convolutional layers and then show the text snipped (char-N-gram) that
produced that high value. We then did that for all the texts in the dataset. The
first filter in the network for example yielded many short strings ending with 3
newlines. Some examples of these are,

\begin{lstlisting}[gobble=4]
    'adsen\n\n\n'
    'ndsen\n\n\n'
    'umeer\n\n\n'
    'endte\n\n\n'
    'ingen\n\n\n'
    'ehren\n\n\n'
    'elsen\n\n\n'
    'ommer\n\n\n'
    'Ruter\n\n\n'
    'ummer\n\n\n'
    'ersen\n\n\n'
    'ansen\n\n\n'
    'heden\n\n\n'
    'ensen\n\n\n'
    'arsen\n\n\n'
    'orten\n\n\n'
    'ulsen\n\n\n'
    'orgen\n\n\n'
\end{lstlisting}

Many of those short string looks like the ending of common Danish
names followed by three newlines. Here are some examples of the
names taken from a list of the 100 most frequent surnames in Denmark
\footnote{http://www.mydanishroots.com/surnames-meaning-and-origin/the-100-most-
common-surnames-in-denmark.html},

\begin{description}
    \item[adsen:] Madsen.
    \item[ndsen:] Svendsen, Frandsen.
    \item[elsen:] Nielsen, Mikkelsen.
    \item[ersen:] Pedersen, Andersen, Petersen, Iversen, Jespersen.
    \item[ansen:] Hansen, Christiansen, Johansen, Kristiansen.
    \item[ensen:] Jensen, Christensen, S\o rensen, J\o rgensen, Kristensen,
        Mortensen, Mogensen.
    \item[arsen:] Larsen.
    \item[ulsen:] Poulsen.
\end{description}

The network therefore seems to have learned that a name followed by whitespace
are an important feature when determining the authorship of a text. Clearly when
a student buy an assignment from a ghost writer the text will contain the
students own name. Therefore these features are counter productive since the
network learns that as a result of how we have created the training dataset
where each text has the correct authors name in it.


\subsubsection{Siamese Neural Network - Iteration 4}

In this iteration we trained the third network again but now using a dataset
where names were substituted by empty strings. We wanted to see how well the
network would perform when it was not able to use peoples names as a feature.
A graph of the training and validation accuracies can be seen in Figure
\ref{fig:network_4_accuracies}. The best validation result we found was in epoch
22 with an accuracy of 0.79843. That means that we lost about 4\% accuracy now
that we can no longer look at peoples names.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{./pictures/experiments/network_4_accuracies.png}
    \caption{Shows the training and validation accuracies over the epochs of
        training on the fourth iteration of the network.}
    \label{fig:network_4_accuracies}
\end{figure}

We wanted to verify whether or not the network still looked at the names of the
authors. It is not possible for us to look at all 1200 filters but we looked
at a couple and did not find any that looked like they were looking at names.
However we did find that the network now seems to be looking at which class
the student is in. The classes in Danish schools are typically written as 1.p,
1.q, 2.p, 2,q, etc. and we found that one of the convolutional 4 filters were
looking at character sequences such as those. We have generated a couple of
tables that shows what the network is now looking at. We generated the tables by
taking the first 50 texts from the dataset and extracting the maximum value of a
particular filter from each text. That gives a single string of the convolution
size from each text and a number which is higher the more important that string
is. We then sorted the extractions by the importance score. The result of a
couple of the filters are shown in Figures \ref{fig:features_convolution_8_1},
\ref{fig:features_convolution_8_5}, \ref{fig:features_convolution_8_100} and
\ref{fig:features_convolution_4_100}. The first figure looks at whether or not
authors use the phrase "s\aa\ at" (such that). Inexperienced Danish writers will
often use the phrase which is considered grammatically incorrect in most Danish
sentences. Therefore the network seems to have learned that some writers use
"s\aa\ at" while others don't and that it is an important phrase when verifying
authorship. The second figure seems to look at whether or not an authors uses
the phrase "meget" (most). Some authors apparently often describe things using
"meget" while others do not. The third figure shows that the network looks at
which authors use the phrase "at de" or "at der" ("that they" or "that there").
The fourth filter is the one that seems to look at which class a student is in.
In our constructed dataset the class of a student will always be a good feature
since we don't have any actual ghost writers. However a ghost writer would write
the correct class of the student on their submission. Therefore we don't want
the network to learn that particular feature.

Our main problem is that the networks keep using metadata from the texts to
make the predictions. The metadata will always match even though a ghost writer
has written the assignment. It is probably not possible for us to remove all
metadata from all texts. Even though names are supposed to have been removed
the dataset still contains a couple of them and there is so many different ways
of writing the class of a student that it is not feasible for us to remove
all of them. The metadata will typically occur in the beginning of the text
and sometimes throughout the text in headers. When training a neural network
on images it is normal practice to rotate, shift, shear and zoom the images
randomly during training. The shifting of the images produces new images where
part of the image is no longer part of the training set. In each epoch the
image is shifted a different amount in different directions and the network can
therefore learn to recognise objects even though part of the image is not there.
We wanted to do something similar for the texts we are training on. At the
moment the network relies on finding metadata in the text. That is only possible
when it is given the whole text since metadata will only occur in parts of the
text. We therefore want to try to only give part of the texts to the network
during training. We can do that by replacing some random part of the input text
with a special value during training. For example each time a text is used
during training we might replace 50\% of it with a special garbage character.
Therefore the network will no longer be able to rely on finding metadata about
the text since the metadata will not always be present.

Another problem our current networks has is that each filter extract
only a single value from a text. For example the filter shown in Figure
\ref{fig:features_convolution_8_1} that looked at the phrase "s\aa\ at". The
filter is able to find out whether or not a text contains the phrase but are not
able to say anything about how often that phrase is used by the author. To find
out how often a phrase is used we need to stop using max over time pooling and
start using some kind of average.


\subsubsection{Siamese Neural Network - Iteration 5}

The problems in the previous network were mainly due to the use of a global
max pool. That allowed our networks to look at the presence of certain strings
in texts. Our fifth iteration therefore does not include any global max pools.
Furthermore the network is a \gls{RNN}. We used the same basic architecture
as before. Two texts are presented to the network at a time and features
are extracted from the texts by a Siamese Network. The features are then
compared using a normal dense network. In this iteration we used a combination
of convolutions and \gls{RNN}'s to extract the features. The input is as
usual encoded in an embedding layer. The embedding layer is followed by a
convolutional layer of size 8 and stride 1. The idea is that the convolutional
layer can look at 8 characters at a time and learn features from that. After the
convolutional layer we have a max pool with pool size 8. That is mainly to save
computation power as the size of the texts are reduced to $\frac{1}{8}$ of the
original size. After the max pool we have a \gls{LSTM} layer with 100 output
neurons. The \gls{LSTM} layer loops through the output from the convolution and
max pool and extracts 100 features from it. Now we have 100 features from each
of the input texts. As earlier we combine the features with the elementwise
absolute difference. On top of that Siamese part of the network we compare the
features extracted with a classic dense network. The network has a single layer
with 500 neurons and activated by the \gls{ReLu} activation function. Lastly we
have a dropout layer with 30\% dropout and the output layer with a softmax as
before. We have shown the network in Figure \ref{fig:network5}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{./pictures/experiments/network5.png}
    \caption{Illustrate the structure of our fifth Siamese Neural Network.
        Weights are shared by the embedding layers, the convolutional layer and
        the LSTM layer. This Figure will be replaced by the new format when we
        decide what that format is.}
    \label{fig:network5}
\end{figure}

We have no actual results for the network. The training time is very slow. It
has run for 3 epochs and rise about 5\% in accuracy each epoch. When we last
looked at it it was at 71\% validation accuracy.


TODO:
\begin{itemize}
    \item Describe Backpropagration
    \item Describe Optimizers
    \item Describe Convolutions
    \item Describe Dropout
    \item Describe RNN
    \item Use squared distance as merge function to get larger differences
        between close and far apart.
    \item Try to not use dense network but a distance metric.
\end{itemize}
