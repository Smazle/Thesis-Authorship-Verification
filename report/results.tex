\section{Results} \label{sec:results}

In this section we go through the results we obtained on the test dataset
\gls{D}. We report the number of \glspl{TP}, \glspl{TN}, \glspl{FP}, \glspl{FN},
accuracy and accusation error. The methods we evaluate on the test dataset are
the \gls{conv-char-NN}, the \gls{rec-sent-NN}, the \gls{conv-char-word-NN}
and the two baseline methods. As mentioned in Section \ref{sec:data} the test
dataset consist of texts from 3558 different authors.

Recall that in the real world approximately 4\% of assignments are ghostwritten.
Since that is the case we will report our performance to a dataset containing
4\% negatives. That result can be used to estimate real world performance of our
methods. We will also report our results on a dataset containing 50\% negatives.
Other authorship verification results and other work on MaCom's dataset use 50\%
negatives. We want to compare our results to those other methods. Therefore we
also evaluate all methods on a dataset with 50\% negatives.


\subsection{Baselines}

The first of our implemented baseline methods was the extended delta method.
The hyperparameters that gave the best result for the extended delta method
was $K = 1$ and $p = 1$. The method requires normalizing to mean 0 and unit
variance. We computed the transformation required to normalize the training
data and applied that to the test data. As discussed earlier the delta method
is a classical solution to the authorship verification problem. The version we
implemented is not able to threshold the accusation error. Therefore we just
report the results that maximize accuracy and the accusation error associated
with that result.

The second of our implemented baseline methods was the author specific
\gls{SVM}. The hyperparameters that gave the best result on the author specific
\gls{SVM} was $C = 10$ and $\gamma = 1000$. As with the extended delta method we
have not thresholded this method to achieve below 10\% accusation error. Once
again there was no obvious way to do the thresholding, so we only report the
accuracy and accusation error of the method when maximizing accuracy.


\subsection{Neural Networks}

We retrained our three neural networks using the \gls{B} dataset. The \gls{B}
dataset is much larger than the dataset they were originally trained on. We used
the \gls{C} dataset for early stopping. For the networks we report both the
configurations maximizing accuracy and the configurations maximizing accuracies
while keeping the accusation error under 10\%. The configurations are shown
together with the results. Note that the found parameters that adhere to the
10\% constraint were found on the training data. As with any machine learning
approach this does not ensure the same level of adherence on the test data.

The results for both the baseline methods and the three networks is shown in
Figure \ref{tab:50_results} for the dataset of 50\% negatives and in Figure
\ref{tab:04_results} for the 4\% negative dataset. 

\begin{table}[]
\centering
\textbf{The results of running our methods on a 50\% negative test dataset}\par\medskip
\begin{adjustbox}{center}
\scriptsize
\begin{tabular}{|c|l|c|c|c|c|c|}
\hline
    \textbf{Constrained} & \textbf{Method} & \textbf{Prediction System} & $\mathbf{\theta}$ & \textbf{Accuracy} & \textbf{Accusation Error} & \textbf{Specificity} \\ \hline
    \multirow{5}{*}{Yes} & \gls{SVM} & \multicolumn{5}{c|}{N/A} \\ \cline{2-7}
    & Extended Delta & \multicolumn{5}{c|}{N/A} \\ \cline{2-7}
    & \gls{conv-char-NN} & $P_{lexp_{0.25}}$ & 0.390 & \textbf{0.80780} & \textbf{0.06310} & 0.66028 \\ \cline{2-7}
    & \gls{conv-char-word-NN} & $P_{lexp_{0.25}}$ & 0.433 & 0.80230 & 0.12680 & \textbf{0.70761} \\ \cline{2-7}
    & \gls{rec-sent-NN} & $P_{lexp_{0.25}}$ & 0.033 & 0.65520 & 0.20020 & 0.41408 \\ \hline\hline
    \multirow{5}{*}{No} & SVM & N/A & N/A & 0.71950 & 0.26560 & 0.68944 \\ \cline{2-7}
    & Extended Delta & N/A & N/A & 0.62150 & 0.36780 & 0.58141 \\ \cline{2-7}
    & \gls{conv-char-NN} & $P_{lexp_{0.25}}$ & 0.486 & \textbf{0.86530} & \textbf{0.09870} & 0.82056 \\ \cline{2-7}
    & \gls{conv-char-word-NN} & $P_{lexp_{0.25}}$ & 0.544 & 0.83070 & 0.19330 & \textbf{0.86986} \\ \cline{2-7}
    & \gls{rec-sent-NN} & $P_{lexp_{0.25}}$ & 0.267 & 0.68600 & 0.36320 & 0.86648 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{The results on the 50\% negative dataset using the listed methods.
Constrained refers to whether or not $\theta$ and the prediction system which we
used was chosen based on their accuracy constrained to a 10\% accusation error.
In case Yes, it refers to under the 10\% threshold, and No refers to simply
maximizing accuracy with no regard for accusation error. In both cases
the best results are shown in bold. We have shown the raw \glspl{TP},
\glspl{TN}, \glspl{FP} and \glspl{FN} in Appendix
\ref{subsec:neural-network-50-test-results}.}
\label{tab:50_results}
\end{table}

\begin{table}[]
\centering
\textbf{The results of running our methods on a 4\% negative test dataset}\par\medskip
\begin{adjustbox}{center}
\scriptsize
\begin{tabular}{|c|l|c|c|c|c|c|}
\hline
    \textbf{Constrained} & \textbf{Method} & \textbf{Prediction System} & $\mathbf{\theta}$ & \textbf{Accuracy} & \textbf{Accusation Error} & \textbf{Specificity} \\ \hline
    \multirow{5}{*}{Yes} & \gls{SVM} & \multicolumn{5}{c|}{N/A} \\ \cline{2-7}
    & Extended Delta & \multicolumn{5}{c|}{N/A} \\ \cline{2-7}
    & \gls{conv-char-NN} & $P_{MV}$ & 0.057 & \textbf{0.96110} & \textbf{0.23520} & 0.08497 \\ \cline{2-7}
    & \gls{conv-char-word-NN} & $P_{exp_{0.25}}$ & 0.127 & 0.96040 & 0.51510 & \textbf{0.22222} \\ \cline{2-7}
    & \gls{rec-sent-NN} & $P_{lexp_{0.25}}$ & 0.002 & 0.95260 & 0.71420 & 0.12162 \\ \hline\hline
    \multirow{5}{*}{No} & \gls{SVM} & N/A & N/A & 0.73720 & 0.90380 & \textbf{0.63226} \\ \cline{2-7}
    & Extended Delta & N/A & N/A & 0.66549 & 0.92770 & 0.62162 \\ \cline{2-7}
    & \gls{conv-char-NN} & $P_{lexp_{0.25}}$ & 0.137 & \textbf{0.96350} & \textbf{0.26310} & 0.18301 \\ \cline{2-7}
    & \gls{conv-char-word-NN} & $P_{lexp_{0.25}}$ & 0.192 & 0.96210 & 0.47770 & 0.32639 \\ \cline{2-7}
    & \gls{rec-sent-NN} & $P_{lexp_{0.25}}$ & 0.002 & 0.95260 & 0.7142 & 0.12162 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{The results on the 4\% negative data set using the listed methods.
Constrained refers to whether or not $\theta$ and the prediction system which we
used was chosen based on their accuracy constrained to a 10\% accusation error.
In case Yes it refers to under the 10\% threshold, and No refers to simply
maximizing accuracy with no regard for accusation error. In both cases
the best results were bolded. We have shown the raw \glspl{TP}, \glspl{TN},
\glspl{FP} and \glspl{FN} results in Appendix
\ref{subsec:neural-network-4-test-results}.}
\label{tab:04_results}
\end{table}
