# Authorship Verification

This is the code produced in connection a masters thesis in computer science.
The goal of the thesis was to verify whether or not a certain text was produced
by a target author, hence authorship verification.
This process is done using a series neural networks of differing generalizing designs, 
with the goal of determining viable deep learning approaches to the authorship 
verification problem.


## Training a network

To train a network you need two datafiles. A training file and a validation
file. The training file is used for training and the validation for early
stopping. If we for example wanted to train a network with the name
*network_name* using the training data file *training_file* and validation
data file *validation_file*, then we would run the command:

```
python -m src.networks.generic_network network_name create-reader training_file\
    validation_file
```

That command will start training the network on the training datafile. The
program will save several files in the directory it is run in. The files are:

 * *reader.p*, which contains information used to map characters to integers.
   If the network is to be used after training that file has to be saved.

 * *weights.\<e\>-\<vl\>.hdf5*, where \<e\> is an epoch number and \<vl\> is the
   validation loss in that epoch. One of these files will be generated at the
   end of each epoch. They contain the weights that have been computed at the
   end of that epoch. The weights can then be loaded by the network later to
   produce a model.

 * *final_model.hdf5*, which is a Keras model that can be used to predict new
   samples. Note that the final model is not necessarily the model with the
   lowest validation loss. Instead it is simply the model that use the current
   weights whenever the training was stopped. That means that in most cases the
   final model can just be deleted after training.

It is possible to have the program output a file containing the history of
training and validation loss and training and validation accuracy in each epoch
of training. If that is needed the option `--history` can be given as in:

```
python -m src.networks.generic_network network_name --history history.csv\
    create-reader training_file validation_file
```

It is also possible for the program to save a graph of the network it is
training for that the `--graph` option has to be given as in:

```
python -m src.networks.generic_network network_name --graph graph.png\
    create-reader training_file validation_file
```

## Continue training

If for some reason the training of a network has to be stopped the training can
be continued later. To continue the training of the networks a file containing
the weights and a *reader.p* file is required. The training can then be
continued with the command:

```
python -m src.networks.generic_network network_name --weights weights.hdf5\
    load-reader ./reader.p
```

## Evaluating trained network

The first step in evaluating a trained network is to generate a Keras model.
The model *final_model.hdf5* is rarely what is needed. As explained earlier it
is not the best weights that are saved but the "last" weights. Instead a model
using the best weights can be generated by finding the weight file corresponding
to the lowest validation loss (maybe by using a history file). When such a
weight file has been identified the model can be generated with:

```
python -m src.networks.generic_network network_name --weights best_weights.hdf5\
    --epochs 0 load-reader ./reader.p
```

The `--epochs 0` option make sure that we train 0 epochs. Instead the command
will load the weights from the weights file and generate a *final_model.hdf5*
using those weights. It is that model that we want to evaluate. To evaluate a
model on a test dataset *testing_file* using the model *final_model.hdf5* and
reader *reader.p* execute the command:

```
python -m src.prediction.prediction final_model.hdf5 reader.p testing_file
```

The stdout of that command will be a csv file with the fields:

 * *weight*, the weight function tried,
 * *threshold*, the threshold tried,
 * *accuracy*, the accuracy obtained,
 * *accusation_error*, the accusation error obtained,
 * *tps*, the number of true positives obtained,
 * *tns*, the number of true negatives obtained,
 * *fps*, the number of false positives obtained,
 * *fns*, the number of false negatives obtained.

From that information the best configuration of weight and threshold can be
easily found. By default 50% negative samples are generated to test on. To
generate some other number of negative samples the option `--negative-chance`
can be set. The negative chance is a number between 0 and 1 which is the
probability for each positive sample generated that a negative sample is also
generated. By default it is 1.0 which gives the 50/50 split between positives
and negatives. If it is set to 0.5 we can expect around 1/3 negatives and 2/3
positives (which is a bit confusing).

## Data format

The format of the data files our programs expect is a variant of a CSV format.
The delimiter between fields is a semicolon (;) and the encoding of the file is
expected to be utf-8. The fields expected are:

 * *ID*, the ID of the author having written the assignment on this line,
 * *Date*, the date the assignment was turned in in the format
   \<dd\>-\<mm\>-\<yyyy\> for example 31-06-2018,
 * *Text*, the text turned in where the authors name is replaced with the
   character sequence $NAME$, newline characters are replaced with $NL$ and
   semi colon characters are replaced with $SC$.

An short example CSV file is:

```
ID;Date;Text
1388192;06-09-2018;This text is from author $NAME$.$NL$
8727388;14-12-2017;This text contains a $SC$.$NL$And a second line.$NL$
```

The ordering of the fields are unfortunately significant as we in multiple
places parse the CSV file simply by splitting each line by the semi colon
character. The fields can therefore not be swapped around!

## Adding a network

In order to add a new network to the solution, you first need to navigate 
to the network folder located in [src/networks/](src/networks/).
From here we can split the creation the creation process into 3 steps.

* Create python file containing script
* Reference in *network_factory.py*
* Create default config file

Step one, creating the network consists of first creating a new python script. 
The file name of this script is the *network_name* parameter previously.
The script contain a single method called *model*, which in turn takes a single argument, *reader*, which
is an object derived from the loaded *reader.p* file mentioned earlier.
This method should return the final keras model produced. Thus the design must be as follows


```python
def model(reader):

    ######################
    ### Network design ###
    ######################

    model = keras.models.Model(inputs=input, outputs=output)
    return model
```

Now that the network design is done, it need to be references to in 
the *network_factory.py* file, which is as the name suggests a factory
producing the different networks available. When looking at the *network_factory.py* 
it should be somewhat obvious what additions would need to be made, when adding a new network.

The last step is adding a config file associated with the newly created network. 
The config files associated with reach network can be found in [src/networks/config/](src/networks/config/).
In here a new file need to be created using the naming filename {network_name}_config.json
This file should simply contained the desired default argument value the network has. This allows
the user to omit certain arguments if they see themselves using the 
same value for a command line argument repeatedly.
An example of such a json file would be 

```json
{
"validation_split":0.95,
"batch_size":8,
"vocabulary_frequency_cutoff":[0.00001],
"batch_normalization":"pad",
"pad":false,
"binary":false,
"channels":["char"],
"sentence_length":null
}
```

It should be noted that the existance of this config file is required, 
and so is the assignment of each of the arguments like in the presented example.

If these instructions were followed, you should be able to run the code just as described earlier.


